{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of seed files collected: 84\n",
      "Data collection of XY simulated to 1% sampling rate, Dataset split: 74/6/4\n",
      "No. of samples: 1950, No. of timesteps: 2560, Chunksize: 64\n",
      "Training shape:  (1950, 2560, 64) (1950, 2560, 64)\n",
      "Validation shape:  (150, 2560, 64) (150, 2560, 64)\n",
      "Test shape:  (97, 2560, 64) (97, 2560, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train = \"Data/readelf/9/train_dataset.npz\"\n",
    "val = \"Data/readelf/9/val_dataset.npz\"\n",
    "test = \"Data/readelf/9/test_dataset.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset = np.load(train)\n",
    "val_dataset = np.load(val)\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = training_dataset['x']\n",
    "y_train = training_dataset['y']\n",
    "\n",
    "x_val = val_dataset['x'][:150]\n",
    "y_val = val_dataset['y'][:150]\n",
    "\n",
    "x_test = test_dataset['x']\n",
    "y_test = test_dataset['y']\n",
    "\n",
    "print(\"No. of seed files collected: 84\")\n",
    "print(\"Data collection of XY simulated to 1% sampling rate, Dataset split: 74/6/4\")\n",
    "samples, timesteps, chunksize = x_train.shape\n",
    "print(\"No. of samples: \" + str(samples) + \", No. of timesteps: \" + str(timesteps) + \", Chunksize: \" + str(chunksize))\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, y_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 2560, 64)          33024     \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1950 samples, validate on 150 samples\n",
      "Epoch 1/200\n",
      "1950/1950 [==============================] - 255s 131ms/step - loss: 0.0992 - val_loss: 0.0772\n",
      "Epoch 2/200\n",
      "1950/1950 [==============================] - 247s 127ms/step - loss: 0.0864 - val_loss: 0.0698\n",
      "Epoch 3/200\n",
      "1950/1950 [==============================] - 238s 122ms/step - loss: 0.0790 - val_loss: 0.0650\n",
      "Epoch 4/200\n",
      "1950/1950 [==============================] - 242s 124ms/step - loss: 0.0741 - val_loss: 0.0617\n",
      "Epoch 5/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0706 - val_loss: 0.0593\n",
      "Epoch 6/200\n",
      "1950/1950 [==============================] - 225s 115ms/step - loss: 0.0681 - val_loss: 0.0575\n",
      "Epoch 7/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0661 - val_loss: 0.0560\n",
      "Epoch 8/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0645 - val_loss: 0.0548\n",
      "Epoch 9/200\n",
      "1950/1950 [==============================] - 236s 121ms/step - loss: 0.0630 - val_loss: 0.0537\n",
      "Epoch 10/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0618 - val_loss: 0.0527\n",
      "Epoch 11/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0606 - val_loss: 0.0518\n",
      "Epoch 12/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0595 - val_loss: 0.0510\n",
      "Epoch 13/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0586 - val_loss: 0.0502\n",
      "Epoch 14/200\n",
      "1950/1950 [==============================] - 222s 114ms/step - loss: 0.0577 - val_loss: 0.0495\n",
      "Epoch 15/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0568 - val_loss: 0.0489\n",
      "Epoch 16/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0561 - val_loss: 0.0483\n",
      "Epoch 17/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0553 - val_loss: 0.0477\n",
      "Epoch 18/200\n",
      "1950/1950 [==============================] - 225s 116ms/step - loss: 0.0547 - val_loss: 0.0472\n",
      "Epoch 19/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0540 - val_loss: 0.0467\n",
      "Epoch 20/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0534 - val_loss: 0.0462\n",
      "Epoch 21/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0529 - val_loss: 0.0458\n",
      "Epoch 22/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0524 - val_loss: 0.0454\n",
      "Epoch 23/200\n",
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0519 - val_loss: 0.0450\n",
      "Epoch 24/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0515 - val_loss: 0.0446\n",
      "Epoch 25/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0510 - val_loss: 0.0443\n",
      "Epoch 26/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0506 - val_loss: 0.0440\n",
      "Epoch 27/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0503 - val_loss: 0.0437\n",
      "Epoch 28/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0499 - val_loss: 0.0434\n",
      "Epoch 29/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0496 - val_loss: 0.0431\n",
      "Epoch 30/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0493 - val_loss: 0.0429\n",
      "Epoch 31/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0490 - val_loss: 0.0426\n",
      "Epoch 32/200\n",
      "1950/1950 [==============================] - 231s 119ms/step - loss: 0.0487 - val_loss: 0.0424\n",
      "Epoch 33/200\n",
      "1950/1950 [==============================] - 225s 115ms/step - loss: 0.0485 - val_loss: 0.0422\n",
      "Epoch 34/200\n",
      "1950/1950 [==============================] - 225s 115ms/step - loss: 0.0482 - val_loss: 0.0420\n",
      "Epoch 35/200\n",
      "1950/1950 [==============================] - 222s 114ms/step - loss: 0.0480 - val_loss: 0.0418\n",
      "Epoch 36/200\n",
      "1950/1950 [==============================] - 220s 113ms/step - loss: 0.0477 - val_loss: 0.0416\n",
      "Epoch 37/200\n",
      "1950/1950 [==============================] - 225s 116ms/step - loss: 0.0475 - val_loss: 0.0414\n",
      "Epoch 38/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0473 - val_loss: 0.0413\n",
      "Epoch 39/200\n",
      "1950/1950 [==============================] - 223s 114ms/step - loss: 0.0471 - val_loss: 0.0411\n",
      "Epoch 40/200\n",
      "1950/1950 [==============================] - 222s 114ms/step - loss: 0.0470 - val_loss: 0.0409\n",
      "Epoch 41/200\n",
      "1950/1950 [==============================] - 223s 114ms/step - loss: 0.0468 - val_loss: 0.0408\n",
      "Epoch 42/200\n",
      "1950/1950 [==============================] - 222s 114ms/step - loss: 0.0466 - val_loss: 0.0407\n",
      "Epoch 43/200\n",
      "1950/1950 [==============================] - 223s 114ms/step - loss: 0.0465 - val_loss: 0.0405\n",
      "Epoch 44/200\n",
      "1950/1950 [==============================] - 223s 114ms/step - loss: 0.0463 - val_loss: 0.0404\n",
      "Epoch 45/200\n",
      "1950/1950 [==============================] - 223s 114ms/step - loss: 0.0462 - val_loss: 0.0403\n",
      "Epoch 46/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0460 - val_loss: 0.0402\n",
      "Epoch 47/200\n",
      "1950/1950 [==============================] - 225s 116ms/step - loss: 0.0459 - val_loss: 0.0401\n",
      "Epoch 48/200\n",
      "1950/1950 [==============================] - 225s 115ms/step - loss: 0.0458 - val_loss: 0.0399\n",
      "Epoch 49/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0457 - val_loss: 0.0398\n",
      "Epoch 50/200\n",
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0455 - val_loss: 0.0397\n",
      "Epoch 51/200\n",
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0454 - val_loss: 0.0396\n",
      "Epoch 52/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0453 - val_loss: 0.0396\n",
      "Epoch 53/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0452 - val_loss: 0.0395\n",
      "Epoch 54/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0451 - val_loss: 0.0394\n",
      "Epoch 55/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0450 - val_loss: 0.0393\n",
      "Epoch 56/200\n",
      "1950/1950 [==============================] - 225s 116ms/step - loss: 0.0449 - val_loss: 0.0392\n",
      "Epoch 57/200\n",
      "1950/1950 [==============================] - 223s 114ms/step - loss: 0.0448 - val_loss: 0.0391\n",
      "Epoch 58/200\n",
      "1950/1950 [==============================] - 224s 115ms/step - loss: 0.0448 - val_loss: 0.0391\n",
      "Epoch 59/200\n",
      "1950/1950 [==============================] - 225s 116ms/step - loss: 0.0447 - val_loss: 0.0390\n",
      "Epoch 60/200\n",
      "1950/1950 [==============================] - 223s 114ms/step - loss: 0.0446 - val_loss: 0.0389\n",
      "Epoch 61/200\n",
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0445 - val_loss: 0.0389\n",
      "Epoch 62/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0445 - val_loss: 0.0388\n",
      "Epoch 63/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0444 - val_loss: 0.0387\n",
      "Epoch 64/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0443 - val_loss: 0.0387\n",
      "Epoch 65/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0443 - val_loss: 0.0386\n",
      "Epoch 66/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0442 - val_loss: 0.0386\n",
      "Epoch 67/200\n",
      "1950/1950 [==============================] - 233s 119ms/step - loss: 0.0441 - val_loss: 0.0385\n",
      "Epoch 68/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0441 - val_loss: 0.0385\n",
      "Epoch 69/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0440 - val_loss: 0.0384\n",
      "Epoch 70/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0440 - val_loss: 0.0384\n",
      "Epoch 71/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0439 - val_loss: 0.0383\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0439 - val_loss: 0.0383\n",
      "Epoch 73/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0438 - val_loss: 0.0383\n",
      "Epoch 74/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0438 - val_loss: 0.0382\n",
      "Epoch 75/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0437 - val_loss: 0.0382\n",
      "Epoch 76/200\n",
      "1950/1950 [==============================] - 233s 120ms/step - loss: 0.0437 - val_loss: 0.0381\n",
      "Epoch 77/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0437 - val_loss: 0.0381\n",
      "Epoch 78/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0436 - val_loss: 0.0381\n",
      "Epoch 79/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0436 - val_loss: 0.0380\n",
      "Epoch 80/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0436 - val_loss: 0.0380\n",
      "Epoch 81/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0435 - val_loss: 0.0380\n",
      "Epoch 82/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0435 - val_loss: 0.0380\n",
      "Epoch 83/200\n",
      "1950/1950 [==============================] - 222s 114ms/step - loss: 0.0435 - val_loss: 0.0379\n",
      "Epoch 84/200\n",
      "1950/1950 [==============================] - 224s 115ms/step - loss: 0.0434 - val_loss: 0.0379\n",
      "Epoch 85/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0434 - val_loss: 0.0379\n",
      "Epoch 86/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0434 - val_loss: 0.0379\n",
      "Epoch 87/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0434 - val_loss: 0.0379\n",
      "Epoch 88/200\n",
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0433 - val_loss: 0.0378\n",
      "Epoch 89/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0433 - val_loss: 0.0378\n",
      "Epoch 90/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0433 - val_loss: 0.0378\n",
      "Epoch 91/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0433 - val_loss: 0.0378\n",
      "Epoch 92/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0433 - val_loss: 0.0378\n",
      "Epoch 93/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0433 - val_loss: 0.0378\n",
      "Epoch 94/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0432 - val_loss: 0.0377\n",
      "Epoch 95/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0432 - val_loss: 0.0377\n",
      "Epoch 96/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0432 - val_loss: 0.0377\n",
      "Epoch 97/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0432 - val_loss: 0.0377\n",
      "Epoch 98/200\n",
      "1950/1950 [==============================] - 219s 112ms/step - loss: 0.0432 - val_loss: 0.0377\n",
      "Epoch 99/200\n",
      "1950/1950 [==============================] - 221s 113ms/step - loss: 0.0432 - val_loss: 0.0377\n",
      "Epoch 100/200\n",
      "1950/1950 [==============================] - 219s 112ms/step - loss: 0.0432 - val_loss: 0.0377\n",
      "Epoch 101/200\n",
      "1950/1950 [==============================] - 233s 120ms/step - loss: 0.0432 - val_loss: 0.0377\n",
      "Epoch 102/200\n",
      "1950/1950 [==============================] - 223s 115ms/step - loss: 0.0431 - val_loss: 0.0377\n",
      "Epoch 103/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0431 - val_loss: 0.0377\n",
      "Epoch 104/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 105/200\n",
      "1950/1950 [==============================] - 223s 115ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 106/200\n",
      "1950/1950 [==============================] - 222s 114ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 107/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 108/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 109/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 110/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 111/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 112/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 113/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 114/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 115/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 116/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 117/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 118/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0431 - val_loss: 0.0376\n",
      "Epoch 119/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 120/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 121/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 122/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 123/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 124/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 125/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 126/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 127/200\n",
      "1950/1950 [==============================] - 224s 115ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 128/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 129/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 130/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0376\n",
      "Epoch 131/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 132/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 133/200\n",
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 134/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 135/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 136/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 137/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 138/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 139/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 140/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 141/200\n",
      "1950/1950 [==============================] - 225s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 142/200\n",
      "1950/1950 [==============================] - 231s 119ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 143/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 144/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 145/200\n",
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 146/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 148/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 149/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 150/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 151/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 152/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 153/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 154/200\n",
      "1950/1950 [==============================] - 227s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 155/200\n",
      "1950/1950 [==============================] - 234s 120ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 156/200\n",
      "1950/1950 [==============================] - 231s 119ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 157/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 158/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 159/200\n",
      "1950/1950 [==============================] - 229s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 160/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 161/200\n",
      "1950/1950 [==============================] - 218s 112ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 162/200\n",
      "1950/1950 [==============================] - 217s 112ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 163/200\n",
      "1950/1950 [==============================] - 224s 115ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 164/200\n",
      "1950/1950 [==============================] - 222s 114ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 165/200\n",
      "1950/1950 [==============================] - 221s 113ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 166/200\n",
      "1950/1950 [==============================] - 217s 111ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 167/200\n",
      "1950/1950 [==============================] - 218s 112ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 168/200\n",
      "1950/1950 [==============================] - 221s 113ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 169/200\n",
      "1950/1950 [==============================] - 221s 113ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 170/200\n",
      "1950/1950 [==============================] - 225s 115ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 171/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 172/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 173/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 174/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 175/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 176/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 177/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 178/200\n",
      "1950/1950 [==============================] - 231s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 179/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 180/200\n",
      "1950/1950 [==============================] - 232s 119ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 181/200\n",
      "1950/1950 [==============================] - 227s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 182/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 183/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 184/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 185/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 186/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 187/200\n",
      "1950/1950 [==============================] - 226s 116ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 188/200\n",
      "1950/1950 [==============================] - 225s 115ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 189/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 190/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 191/200\n",
      "1950/1950 [==============================] - 229s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 192/200\n",
      "1950/1950 [==============================] - 230s 118ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 193/200\n",
      "1950/1950 [==============================] - 228s 117ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 194/200\n",
      "1950/1950 [==============================] - 233s 120ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 195/200\n",
      "1950/1950 [==============================] - 225s 115ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 196/200\n",
      "1950/1950 [==============================] - 225s 115ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 197/200\n",
      "1950/1950 [==============================] - 222s 114ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 198/200\n",
      "1950/1950 [==============================] - 219s 112ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 199/200\n",
      "1950/1950 [==============================] - 216s 111ms/step - loss: 0.0430 - val_loss: 0.0375\n",
      "Epoch 200/200\n",
      "1950/1950 [==============================] - 221s 113ms/step - loss: 0.0430 - val_loss: 0.0375\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(2560, 64), return_sequences=True))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error')\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200, #change accordingly\n",
    "                    batch_size=30, #change accordingly\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"/home/isa/FYPJ/Model/model9.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7dcbaaef98>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkr6ISBFR0hcRKSJK+iIiRURJX0SkiPx/YsOi2CJJICIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7fa617d198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 4s 37ms/step\n",
      "Test loss: 0.04623737040791986\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 944ms/step\n"
     ]
    }
   ],
   "source": [
    "x_predict = test_dataset['x'][:1]\n",
    "prediction = model.predict_classes(x_predict, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30 31  5 ... 34 34 34]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.01395514e-07  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -4.97720691e-07  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.33631976e-07  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -8.48276215e-07  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  2.21634266e-07  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "[ 1.5806755e-06 -1.2755743e-06 -2.6238733e-06  4.9237440e-07\n",
      "  7.9534840e-07  2.4559718e-06 -8.4081597e-07 -2.8156348e-06\n",
      " -2.2088691e-06 -1.8270891e-08 -1.8640984e-07 -1.1909603e-06\n",
      "  3.1356549e-06 -1.1045672e-06 -2.0222612e-06  1.9664569e-06\n",
      "  2.1283638e-07 -9.2518962e-07 -2.1609145e-07  3.7312827e-07\n",
      "  1.1239966e-06 -9.8672857e-08  2.0976745e-06  1.1569721e-06\n",
      "  7.9773585e-08 -2.6624643e-06 -1.2180819e-06  1.1845248e-06\n",
      " -1.6271022e-06  7.9998728e-07  2.9710561e-06  4.8727129e-06\n",
      "  2.5114608e-07 -1.2805071e-06  3.4062664e-06  2.0195002e-06\n",
      "  7.1843226e-07  1.6589120e-10 -1.4476575e-06  2.6159198e-06\n",
      " -1.0614381e-06  1.2866335e-06 -1.0178096e-06  2.5971951e-06\n",
      " -1.8427826e-07 -5.8569645e-07 -2.0092282e-06 -1.9800844e-07\n",
      "  7.2598827e-10  1.3428352e-06  7.5121920e-07  8.4621547e-07\n",
      "  2.4026497e-06 -3.9099245e-06  1.1709735e-08 -9.7965074e-07\n",
      " -9.6295560e-07  1.8382866e-07 -1.0731808e-06 -1.3750563e-06\n",
      " -3.6752763e-07 -1.4165732e-06 -5.2504713e-08  5.0120699e-09]\n",
      "[-3.43677783e-07  2.30230216e-06 -1.26296900e-05  1.12907555e-05\n",
      " -2.81840562e-06  5.46901683e-05 -1.85741396e-06  6.01461124e-06\n",
      " -7.68564678e-06  3.49612037e-07 -1.09903649e-05 -8.23837036e-06\n",
      " -7.11416260e-06 -2.23912821e-06  1.84689929e-06  8.88623708e-06\n",
      " -5.35759500e-06  2.34339927e-06  7.48404273e-06  8.99835686e-06\n",
      "  2.83388886e-06 -1.77516927e-06  3.70432213e-06  1.94477047e-06\n",
      " -9.55525047e-06  4.37414292e-06  2.18512650e-06  4.38539701e-06\n",
      "  3.27028761e-06  7.39651432e-06 -3.37457755e-06  3.54592794e-06\n",
      "  9.26721987e-06  2.48070082e-05  1.97584814e-06  1.17221134e-06\n",
      " -1.10431927e-06  5.30630450e-06  4.27756777e-06 -4.86134968e-06\n",
      " -2.68329950e-06 -2.23390839e-06 -4.21048526e-06  3.12108637e-06\n",
      " -1.14785905e-06  4.12800273e-06  4.89849663e-06 -1.35701112e-05\n",
      " -7.06394701e-07  1.29837017e-05  6.42700024e-06 -5.76072853e-06\n",
      " -3.01154978e-06 -3.39822077e-06 -4.60361161e-06  5.38327413e-06\n",
      "  1.89819787e-06  4.37933380e-07 -4.95822133e-06  5.91180253e-07\n",
      " -7.27514680e-06  1.09141467e-06 -1.12465268e-05 -1.67735095e-06]\n",
      "[ 1.1280927e-06  4.3459600e-06 -6.8212448e-06  1.2520780e-05\n",
      " -5.9985277e-06  4.9984716e-05  3.7853633e-06  1.2073689e-05\n",
      " -9.6748645e-06 -5.6881540e-06 -9.5142395e-06  7.3907711e-07\n",
      " -8.0849059e-06 -4.9964106e-06 -3.3210183e-06  3.6074030e-06\n",
      " -3.2026264e-06  3.0466656e-06  9.0248450e-06  4.3677373e-06\n",
      " -7.2027160e-07 -4.2539482e-06 -5.5432729e-08 -3.4298971e-08\n",
      "  4.1318168e-07  9.6129190e-07  2.2889571e-07  5.5544315e-06\n",
      " -4.4508092e-06 -5.8129026e-06 -9.6713620e-07  7.5456624e-06\n",
      "  9.0618632e-06  1.4570767e-05 -3.2482139e-06  5.9998060e-06\n",
      " -2.3681491e-06  8.2488195e-06  6.4034011e-06 -8.0937862e-06\n",
      "  3.4610496e-06 -1.8397430e-06 -6.7670317e-06  6.8804211e-06\n",
      "  3.4236230e-06 -5.7012485e-06  5.5552327e-06 -1.1251022e-05\n",
      " -2.3529299e-06  6.5350678e-06  7.9466063e-06 -8.6935497e-06\n",
      "  1.0573668e-06  8.8993011e-07 -3.6442532e-06  8.9173718e-06\n",
      "  7.5041962e-06 -1.1664149e-06 -3.6667313e-07  3.9729098e-06\n",
      "  1.1606450e-06 -6.4984744e-07 -8.4301482e-06 -7.2520702e-06]\n",
      "[ 4.1394360e-06  5.5998225e-06 -5.9264912e-06  1.6229653e-05\n",
      " -8.3880186e-06  5.2354462e-05  4.6189471e-06  1.1365057e-05\n",
      " -1.5066208e-05 -8.5522770e-06 -9.6309359e-06  2.6397640e-06\n",
      " -7.4251811e-06 -4.3353739e-06 -1.0126585e-05  3.9052848e-06\n",
      "  8.7384177e-07  1.2558120e-06  6.4724472e-06  2.6459179e-06\n",
      " -3.4040690e-06 -5.0414537e-06  2.4097101e-06  3.1378877e-06\n",
      "  1.6020807e-06 -2.3213793e-06  1.4451118e-06  7.6194046e-06\n",
      " -9.4751476e-06 -4.0160880e-06  1.6929334e-07  6.7768196e-06\n",
      "  1.0936332e-05  1.3845598e-05 -1.3544621e-06  9.8026121e-06\n",
      " -2.3379264e-06  9.2160299e-06  6.7048973e-06 -6.8634508e-06\n",
      "  4.0779978e-06 -1.3350264e-06 -7.9033580e-06  8.2668357e-06\n",
      "  2.9046696e-06 -8.7214003e-06  5.5642349e-06 -7.3072019e-06\n",
      " -2.1744183e-06  9.3321814e-06  6.8329691e-06 -1.0759074e-05\n",
      "  3.5662467e-06  1.5483610e-06 -5.1534566e-06  1.0160799e-05\n",
      "  7.0777496e-06 -1.9450501e-06 -6.3695137e-07  3.1029258e-06\n",
      "  3.9123865e-06 -3.7361685e-06 -7.7677432e-06 -7.2695875e-06]\n",
      "[ 5.3218905e-06  4.6980099e-06 -5.3171379e-06  4.2290417e-06\n",
      " -1.1466618e-05  2.9559873e-05  1.3019330e-05  1.0094678e-05\n",
      " -9.3593999e-06 -5.3804115e-06 -5.8493119e-06  3.2918410e-06\n",
      " -1.2360985e-05 -2.2169289e-07 -7.4992204e-06  4.3575205e-06\n",
      "  4.9708115e-07  3.0023639e-06  1.1239886e-05  3.4606528e-06\n",
      " -9.5886562e-06 -5.7381221e-06 -2.6331111e-07 -5.2151711e-07\n",
      " -1.0222848e-06 -3.8724475e-06 -1.8666951e-06  2.4685501e-06\n",
      " -1.1117167e-05 -8.7969656e-06  3.9792453e-06  3.6236863e-06\n",
      "  7.9490919e-06  8.8074539e-06 -1.8712926e-06  9.9253430e-06\n",
      "  3.7648397e-06  3.6120023e-06  8.5789434e-06 -1.5414925e-06\n",
      " -2.4388638e-07 -4.6673845e-06 -3.3011686e-06  5.4828879e-06\n",
      "  2.3321059e-06 -1.1262779e-05  7.9820920e-06 -2.2222914e-06\n",
      " -1.3061859e-06  8.4065341e-06  9.0146759e-06 -7.0977308e-06\n",
      "  4.0317145e-06  5.7016484e-07 -1.4724721e-06  6.5645249e-06\n",
      "  5.7330330e-06 -8.7790595e-06 -3.9299607e-06  3.9076826e-06\n",
      "  8.7281387e-06 -5.7153920e-06 -3.6377655e-06 -5.1558027e-06]\n",
      "[-3.4589391e-06  6.0477441e-06  2.8952286e-06  1.6619746e-05\n",
      "  3.9808034e-05 -3.0524989e-05 -1.6047168e-05 -1.4285116e-05\n",
      " -9.6116164e-06  1.8466583e-06  6.4409969e-06 -2.1739935e-03\n",
      " -8.6120999e-06  2.2028772e-04 -1.7022412e-05 -2.5637419e-06\n",
      " -1.9586745e-05 -1.4164370e-05 -6.7842075e-06  7.2140856e-06\n",
      "  3.6018199e-07 -1.3587834e-05  1.7021823e-05 -3.7616971e-06\n",
      "  6.4617284e-06 -6.4464289e-06  3.3534300e-06  1.6155264e-04\n",
      " -2.2560922e-05 -1.5016293e-05  9.7969796e-06  1.8429384e-05\n",
      "  1.4860977e-05  1.6760991e-05 -8.9516661e-06 -3.3608251e-06\n",
      "  2.7532412e-06 -1.2919066e-06  2.8288835e-03 -7.8269904e-06\n",
      "  1.0114838e-05 -1.6945485e-05 -1.6569415e-05 -4.2429200e-04\n",
      "  1.7323760e-05 -1.6498123e-05  6.7409765e-06 -3.8499384e-06\n",
      "  1.4994488e-06 -2.6500982e-03  2.0781754e-05 -1.5982669e-05\n",
      "  6.0077123e-06 -6.0093430e-06 -3.5201062e-06  5.0371905e-06\n",
      "  1.7353201e-05 -6.1534079e-06  1.8742574e-05  2.5511531e-06\n",
      " -1.4684592e-05  3.3887334e-06 -9.9046565e-06 -1.2695272e-05]\n",
      "[-2.20456150e-06  6.13994473e-07  3.59906562e-06 -6.57375131e-06\n",
      " -5.42633188e-07 -5.69624490e-06  6.34125342e-07  1.05282140e-06\n",
      " -8.68016627e-07  1.40830537e-06  1.58504497e-06 -4.19099088e-04\n",
      "  3.13138298e-06  2.89333620e-05 -1.29149794e-06  3.24574080e-06\n",
      " -1.99880451e-05 -8.88783416e-06  7.04592503e-06 -8.14019404e-06\n",
      " -9.88251304e-06 -1.96481778e-06  3.88858916e-06 -1.84244175e-06\n",
      "  1.65008478e-05  7.79621041e-06 -7.83581527e-07  2.97972729e-05\n",
      " -2.40143640e-06 -5.07646801e-05  6.30798468e-07  1.04305436e-05\n",
      "  1.85734243e-05  1.65386029e-06 -5.65533855e-06  1.67571295e-06\n",
      "  1.05377476e-05  9.56016606e-07  1.80571384e-04 -1.51624533e-06\n",
      "  5.05399566e-06 -6.53903908e-06  2.83632858e-06 -1.12689944e-04\n",
      " -1.12199160e-07 -2.18521132e-06  3.42955423e-06  7.16379475e-07\n",
      "  2.21617984e-06 -7.65395118e-04  9.76568754e-06  3.15836064e-06\n",
      "  1.82027834e-05 -1.21732251e-06  4.08398137e-06 -8.41871781e-07\n",
      " -4.62516664e-06 -1.43017796e-05 -2.15255091e-06 -3.13249927e-07\n",
      " -4.53984285e-06  3.34760466e-06 -8.51729874e-06 -2.46321520e-06]\n",
      "[-4.65341736e-06  1.96549877e-06  4.86527870e-06 -1.74964243e-05\n",
      "  2.57953411e-06 -6.15721046e-06  5.58271950e-06  2.07546855e-05\n",
      "  1.84216887e-07  2.17806405e-06  1.50424896e-07 -4.82542877e-04\n",
      "  1.10797168e-06  6.80510202e-05  9.83418317e-07  1.38287569e-05\n",
      " -2.20256625e-05 -1.40603443e-05  1.33228823e-05 -1.33230424e-05\n",
      " -1.58678249e-05 -4.20395918e-06 -2.29250668e-06 -4.97260407e-06\n",
      "  1.62117867e-05  1.16906649e-05 -9.71478926e-07  4.70993691e-05\n",
      "  7.30088141e-06 -6.19467537e-05 -4.23505185e-07  1.03265202e-05\n",
      "  2.40769750e-05 -1.86054876e-07 -1.51339573e-05  3.81019413e-06\n",
      "  1.52785833e-05  5.69916119e-06  8.62954941e-04  4.93841185e-07\n",
      "  1.08934119e-05 -7.87479439e-06  6.11568976e-06 -1.39153344e-04\n",
      " -2.63521656e-06 -4.59331113e-06  1.74662819e-05 -2.32284492e-06\n",
      "  5.62063360e-06 -7.95155996e-04  9.45117972e-06  1.92176662e-06\n",
      "  2.95759910e-05 -8.05661784e-07  3.94301281e-07  3.62920991e-06\n",
      " -1.83155225e-06 -2.41290672e-05 -1.40627435e-05  3.01952332e-06\n",
      " -5.62994364e-06  5.63650110e-06 -8.38243068e-06 -8.02920476e-06]\n",
      "[-1.17746849e-05 -8.27884378e-07  9.59289878e-07 -2.45392166e-05\n",
      " -5.32920376e-06 -1.56272236e-07  1.78447754e-05  3.82789112e-05\n",
      "  7.58720216e-06  1.56653425e-06 -5.85467978e-06 -4.19624994e-04\n",
      "  2.57041775e-05  4.77504800e-05  6.03941135e-06  1.73719236e-05\n",
      " -1.43872376e-05 -1.73451408e-05  2.20507300e-05 -2.20232178e-05\n",
      " -1.05937897e-05  3.85973726e-06 -9.03338048e-07 -7.23803441e-06\n",
      "  1.35317332e-05  1.99095248e-05  2.51276447e-06  5.28753008e-05\n",
      "  1.91834642e-05 -7.51540283e-05  4.37755602e-07  6.19217280e-06\n",
      "  1.92767784e-05  2.20431434e-06 -1.88897102e-05  1.05145537e-05\n",
      "  2.46154050e-05  4.72944294e-06  8.81762011e-04  2.06869845e-05\n",
      "  8.44546048e-06 -9.86150098e-06  1.02342792e-05 -9.14041739e-05\n",
      " -1.35512000e-05  3.98846851e-06  1.70910371e-05  3.43383454e-06\n",
      "  1.03749835e-05 -6.68846886e-04  7.61873889e-06  3.03830211e-06\n",
      "  4.93100670e-05 -3.51745825e-07 -2.54133442e-06  1.14818813e-05\n",
      " -1.77599959e-05 -2.46673844e-05 -3.03912093e-05  1.46015873e-06\n",
      " -1.17897243e-05  8.43431008e-06  2.51465963e-06 -1.47784149e-05]\n",
      "[-1.09608409e-05 -1.91957224e-06 -2.19495791e-06 -1.94607837e-05\n",
      " -1.06857315e-05  4.12874579e-06  2.03559102e-05  4.35297443e-05\n",
      "  6.38518668e-06  8.42301347e-07 -1.09037983e-05 -2.35410436e-04\n",
      "  3.63700528e-05  1.22201091e-05  5.18999332e-06  1.32977593e-05\n",
      " -1.01235764e-05 -1.38716578e-05  2.37564200e-05 -2.29826892e-05\n",
      " -3.88206308e-06  6.64916479e-06  6.96604332e-07 -8.28749762e-06\n",
      "  8.87337228e-06  2.16416829e-05  6.41885208e-06  4.23174533e-05\n",
      "  1.56850238e-05 -7.43136843e-05  2.13135445e-06  2.75627099e-06\n",
      "  1.40927432e-05  3.68219344e-06 -1.91352501e-05  1.66012851e-05\n",
      "  1.98405633e-05  4.74937406e-06  4.94648644e-04  2.64046266e-05\n",
      "  6.63184574e-06 -4.79989330e-06  7.37659684e-06 -3.30529838e-05\n",
      " -1.49591333e-05  6.32509636e-06  1.33664835e-05  5.33030970e-06\n",
      "  1.10561705e-05 -3.89092369e-04  6.83533199e-06  5.22657160e-07\n",
      "  4.73066211e-05  1.90686171e-06 -3.98753127e-06  1.19620227e-05\n",
      " -1.84442106e-05 -1.80238949e-05 -3.21617226e-05  4.08185269e-06\n",
      " -1.01647101e-05  8.09570793e-06  6.40068038e-06 -1.91219406e-05]\n",
      "[-7.31983619e-06 -8.84499173e-07 -4.08129017e-06 -1.32101641e-05\n",
      " -1.15759385e-05  5.28599458e-06  1.88448230e-05  4.13840826e-05\n",
      "  2.06638151e-06 -2.38482642e-07 -1.16392839e-05 -1.37029609e-04\n",
      "  3.34872166e-05 -2.62048502e-06  7.05957859e-07  9.41767303e-06\n",
      " -7.29132489e-06 -1.03574976e-05  2.23506740e-05 -2.03088348e-05\n",
      " -2.57905890e-06  5.92477863e-06  2.38484859e-06 -7.00298142e-06\n",
      "  6.95204153e-06  1.80095212e-05  8.66230675e-06  3.49313304e-05\n",
      "  9.02070133e-06 -6.34447933e-05  3.55593602e-06  9.33152705e-07\n",
      "  1.14795912e-05  2.86644467e-06 -1.88608756e-05  1.98870057e-05\n",
      "  1.55909638e-05  4.42418059e-06  3.19875660e-04  2.30467795e-05\n",
      "  5.86079659e-06 -1.90966057e-06  3.78847017e-06 -8.50386550e-06\n",
      " -1.24606586e-05  4.91799756e-06  1.12601247e-05  4.17447609e-06\n",
      "  1.03052425e-05 -2.48740282e-04  5.92178185e-06 -2.75997627e-06\n",
      "  3.85917301e-05  3.41831856e-06 -4.88152727e-06  1.11316922e-05\n",
      " -1.28569927e-05 -1.28664606e-05 -2.66770330e-05  6.97812493e-06\n",
      " -5.87199929e-06  5.73252919e-06  4.62632534e-06 -2.03291784e-05]\n",
      "[ 7.88645991e-07 -2.84385386e-08 -1.89444540e-06 -1.78820983e-06\n",
      " -5.34095398e-06  8.60574914e-07  2.05284941e-05  1.88078466e-05\n",
      " -1.99277270e-06 -4.63380184e-06 -5.82288021e-06 -6.27853733e-05\n",
      "  7.85256361e-06 -3.19059563e-06 -6.32598085e-06  6.67750646e-06\n",
      " -5.78035451e-06 -7.09550977e-06  1.26102814e-05 -1.16854653e-05\n",
      " -4.48043238e-06  2.70290752e-06  3.58537932e-06 -2.85792703e-06\n",
      "  5.68886617e-06  6.50981474e-06  4.29991996e-06  1.05920863e-05\n",
      "  2.80906374e-06 -2.94793335e-05  1.36492076e-06 -2.82670067e-06\n",
      "  9.34643595e-06  2.90542471e-06 -1.03082675e-05  1.47166256e-05\n",
      "  5.82950997e-06  2.44188891e-06  1.28515065e-04  9.04537228e-06\n",
      "  7.07281231e-07 -2.13855310e-06  7.43630551e-07 -6.84733195e-08\n",
      " -8.07982087e-06 -1.11038230e-06  7.68637165e-06  2.66671691e-06\n",
      "  4.24956988e-06 -1.09756416e-04 -1.34585179e-07 -5.74228397e-06\n",
      "  2.17109446e-05 -1.85763372e-06 -2.35621178e-06  8.93025117e-06\n",
      " -6.79422146e-06 -9.14668908e-06 -1.28005140e-05  4.85653527e-06\n",
      "  2.14059719e-06  9.25744246e-07  5.08892390e-06 -1.45226531e-05]\n",
      "[ 4.9357182e-06  4.5319567e-07 -5.8613347e-07  3.0955532e-06\n",
      " -4.4300068e-06 -1.5779428e-06  2.3781962e-05  1.4836264e-05\n",
      " -5.0097196e-06 -6.8154832e-06 -3.5455125e-06 -3.5069355e-05\n",
      " -2.3891250e-06 -2.6050973e-06 -1.2432441e-05  6.5905820e-06\n",
      " -5.0507192e-06 -6.9029102e-06  7.7644272e-06 -7.3210272e-06\n",
      " -6.3682050e-06  7.3977600e-07  4.2060269e-06 -1.3773895e-06\n",
      "  5.8316909e-06  2.0444431e-06  2.5633351e-06  7.6601218e-06\n",
      " -1.5746929e-06 -2.3108563e-05 -3.9563883e-08 -4.2130191e-06\n",
      "  9.7144211e-06  3.3931965e-06 -8.9962514e-06  1.5472137e-05\n",
      "  2.6638202e-06  2.5739828e-06  7.8115125e-05  4.0895870e-06\n",
      " -1.6532088e-06 -1.8729563e-06 -9.3059361e-07  1.3202439e-06\n",
      " -5.3725098e-06 -4.6429523e-06  8.0364944e-06  1.9640377e-06\n",
      "  2.0886316e-06 -6.4554035e-05 -2.8533145e-06 -8.5192805e-06\n",
      "  1.4582345e-05 -5.1190382e-06 -1.8354089e-06  9.6652902e-06\n",
      " -2.2647198e-06 -8.9558544e-06 -7.1245568e-06  3.8660432e-06\n",
      "  6.2713284e-06 -1.2598617e-06  5.1029429e-06 -1.3995359e-05]\n",
      "[ 6.8947929e-06 -6.0744918e-07 -1.1967512e-06  3.6317369e-06\n",
      " -4.3078376e-06 -5.5087327e-07  2.5976453e-05  2.3575951e-05\n",
      " -5.3603158e-06 -7.5592825e-06 -4.9946343e-06 -2.5262691e-05\n",
      " -2.4232523e-07 -3.9989686e-06 -1.0305826e-05  9.3591598e-06\n",
      " -1.3732423e-06 -8.4642224e-06  5.8664068e-06 -6.2428703e-06\n",
      " -5.9100125e-06  6.9504182e-08  5.1656202e-06 -3.0530559e-06\n",
      "  6.7743745e-06  3.9230642e-07  4.1960229e-06  1.2241459e-05\n",
      " -1.6813692e-06 -2.6229614e-05  1.1671117e-06 -1.5398832e-06\n",
      "  1.2017860e-05  6.7389874e-06 -1.0439498e-05  2.3253317e-05\n",
      "  3.3013218e-06  4.8956476e-06  7.5755175e-05  5.2849896e-06\n",
      " -2.1570520e-06  4.0442199e-08 -1.8432470e-06  3.0009915e-06\n",
      " -5.9995459e-06 -3.9722481e-06  8.1400412e-06  1.8756604e-06\n",
      "  1.0763492e-06 -7.2631257e-05 -3.2654118e-06 -1.0813267e-05\n",
      "  1.6040436e-05 -3.4637892e-06 -1.0160855e-06  9.3195704e-06\n",
      " -1.4703628e-06 -1.1098270e-05 -9.3826347e-06  1.4953505e-06\n",
      "  5.9464815e-06 -3.3400886e-06  4.5784072e-06 -1.7812214e-05]\n",
      "[ 8.0648342e-06  4.8011930e-07  5.3063837e-07  3.0919678e-06\n",
      " -4.7720749e-07 -7.7327451e-07  1.8798735e-05  2.2836175e-05\n",
      " -7.1414470e-06 -7.3778174e-06 -5.4963762e-06 -8.8494726e-06\n",
      " -4.6084051e-06 -4.3988211e-06 -5.9226445e-06  7.5295961e-06\n",
      " -4.7338276e-06 -5.3208273e-06  5.1514517e-06 -4.2657498e-06\n",
      " -8.6574473e-06 -3.9523916e-06  1.8560868e-06 -2.5343693e-06\n",
      "  5.2345176e-06  1.7109750e-07  1.6769912e-06  6.8694285e-06\n",
      " -2.8913835e-06 -1.9407566e-05 -3.2491576e-07 -1.5945392e-06\n",
      "  1.0875656e-05  3.2486589e-06 -1.1787403e-05  1.7010922e-05\n",
      "  3.0465039e-06  8.1422595e-06  4.4576966e-05 -1.1969519e-06\n",
      "  3.7537650e-06  9.7010195e-07 -1.1887560e-06  2.6666544e-06\n",
      " -3.5496525e-06 -8.0292266e-06  1.0269451e-05  1.9858401e-06\n",
      "  2.0079126e-06 -3.5075049e-05 -3.1079568e-07 -9.2208966e-06\n",
      "  1.1924171e-05 -2.2800627e-06 -2.6569373e-06  1.0680164e-05\n",
      "  5.1377992e-06 -1.0144435e-05 -5.8646006e-06  3.1919312e-06\n",
      "  5.9718404e-06 -1.8386324e-06  4.5232548e-07 -1.4210498e-05]\n",
      "[-0.  0. -0.  0. -0. -0.  0.  0.  0.  0.  0.  0. -0.  0. -0. -0. -0.  0.\n",
      "  0.  0.  0. -0.  0.  0.  0.  0.  0.  0. -0.  0.  0.  0.  0. -0. -0.  0.\n",
      " -0. -0.  0. -0. -0.  0. -0. -0. -0. -0.  0.  0. -0.  0.  0. -0. -0.  0.\n",
      " -0.  0.  0. -0.  0.  0.  0.  0.  0.  0.]\n",
      "[-0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00 -0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00 -0.0000000e+00 -0.0000000e+00\n",
      " -3.3995173e-09  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00 -0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00 -0.0000000e+00 -0.0000000e+00\n",
      " -0.0000000e+00 -0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_predict)\n",
    "print(predictions[0][0]) # 1\n",
    "print(predictions[0][1]) # 2 (1st row)\n",
    "print(predictions[0][2]) # 1\n",
    "print(predictions[0][3]) # 2 (2nd row)\n",
    "print(predictions[0][4]) # 1\n",
    "print(predictions[0][5]) # 2 (3rd row)\n",
    "print(predictions[0][6]) # 1\n",
    "print(predictions[0][7]) # 2 (4th row)\n",
    "print(predictions[0][8]) # 1\n",
    "print(predictions[0][9]) # 2 (5th row)\n",
    "print(predictions[0][10])# 1\n",
    "print(predictions[0][11])# 2 (6th row)\n",
    "print(predictions[0][12])# 1\n",
    "print(predictions[0][13])# 2 (7th row)\n",
    "print(predictions[0][14])# 1\n",
    "print(predictions[0][15])# 2 (8th row)\n",
    "print(predictions[0][788])# 1\n",
    "print(predictions[0][789])# 2 (OFFSET 0xC50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
