{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of seed files collected: 68\n",
      "Data collection of XY simulated to 1% sampling rate, Dataset split: 60/4/4\n",
      "No. of samples: 1148, No. of timesteps: 3840, Chunksize: 64\n",
      "Training shape:  (1148, 3840, 64) (1148, 3840, 64)\n",
      "Validation shape:  (82, 3840, 64) (82, 3840, 64)\n",
      "Test shape:  (104, 3840, 64) (104, 3840, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train = \"Data/readelf/8/train_dataset8.npz\"\n",
    "val = \"Data/readelf/8/val_dataset8.npz\"\n",
    "test = \"Data/readelf/8/test_dataset8.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset = np.load(train)\n",
    "val_dataset = np.load(val)\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = training_dataset['x'][:1148]\n",
    "y_train = training_dataset['y'][:1148]\n",
    "\n",
    "x_val = val_dataset['x']\n",
    "y_val = val_dataset['y']\n",
    "\n",
    "x_test = test_dataset['x']\n",
    "y_test = test_dataset['y']\n",
    "\n",
    "print(\"No. of seed files collected: 68\")\n",
    "print(\"Data collection of XY simulated to 1% sampling rate, Dataset split: 60/4/4\")\n",
    "samples, timesteps, chunksize = x_train.shape\n",
    "print(\"No. of samples: \" + str(samples) + \", No. of timesteps: \" + str(timesteps) + \", Chunksize: \" + str(chunksize))\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, x_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 3840, 64)          33024     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3840, 64)          0         \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1148 samples, validate on 82 samples\n",
      "Epoch 1/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.4972 - val_loss: 0.4977\n",
      "Epoch 2/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.4940 - val_loss: 0.4958\n",
      "Epoch 3/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.4904 - val_loss: 0.4936\n",
      "Epoch 4/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.4859 - val_loss: 0.4907\n",
      "Epoch 5/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.4802 - val_loss: 0.4869\n",
      "Epoch 6/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.4727 - val_loss: 0.4814\n",
      "Epoch 7/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.4624 - val_loss: 0.4715\n",
      "Epoch 8/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.4353 - val_loss: 0.4009\n",
      "Epoch 9/200\n",
      "1148/1148 [==============================] - 148s 129ms/step - loss: 0.3836 - val_loss: 0.3724\n",
      "Epoch 10/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.3664 - val_loss: 0.3589\n",
      "Epoch 11/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.3554 - val_loss: 0.3490\n",
      "Epoch 12/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.3468 - val_loss: 0.3409\n",
      "Epoch 13/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.3394 - val_loss: 0.3336\n",
      "Epoch 14/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.3326 - val_loss: 0.3269\n",
      "Epoch 15/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.3264 - val_loss: 0.3203\n",
      "Epoch 16/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.3206 - val_loss: 0.3140\n",
      "Epoch 17/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.3151 - val_loss: 0.3081\n",
      "Epoch 18/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.3104 - val_loss: 0.3029\n",
      "Epoch 19/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.3063 - val_loss: 0.2986\n",
      "Epoch 20/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.3032 - val_loss: 0.2956\n",
      "Epoch 21/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.3008 - val_loss: 0.2930\n",
      "Epoch 22/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.2988 - val_loss: 0.2910\n",
      "Epoch 23/200\n",
      "1148/1148 [==============================] - 150s 130ms/step - loss: 0.2974 - val_loss: 0.2897\n",
      "Epoch 24/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2964 - val_loss: 0.2887\n",
      "Epoch 25/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2957 - val_loss: 0.2880\n",
      "Epoch 26/200\n",
      "1148/1148 [==============================] - 148s 129ms/step - loss: 0.2951 - val_loss: 0.2874\n",
      "Epoch 27/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2946 - val_loss: 0.2869\n",
      "Epoch 28/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2943 - val_loss: 0.2866\n",
      "Epoch 29/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2941 - val_loss: 0.2863\n",
      "Epoch 30/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2939 - val_loss: 0.2861\n",
      "Epoch 31/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2937 - val_loss: 0.2860\n",
      "Epoch 32/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2936 - val_loss: 0.2859\n",
      "Epoch 33/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2936 - val_loss: 0.2859\n",
      "Epoch 34/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2935 - val_loss: 0.2858\n",
      "Epoch 35/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2935 - val_loss: 0.2858\n",
      "Epoch 36/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2935 - val_loss: 0.2858\n",
      "Epoch 37/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2934 - val_loss: 0.2857\n",
      "Epoch 38/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2934 - val_loss: 0.2857\n",
      "Epoch 39/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2934 - val_loss: 0.2857\n",
      "Epoch 40/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2934 - val_loss: 0.2857\n",
      "Epoch 41/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2934 - val_loss: 0.2857\n",
      "Epoch 42/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2934 - val_loss: 0.2857\n",
      "Epoch 43/200\n",
      "1148/1148 [==============================] - 150s 130ms/step - loss: 0.2934 - val_loss: 0.2856\n",
      "Epoch 44/200\n",
      "1148/1148 [==============================] - 148s 129ms/step - loss: 0.2933 - val_loss: 0.2856\n",
      "Epoch 45/200\n",
      "1148/1148 [==============================] - 150s 130ms/step - loss: 0.2933 - val_loss: 0.2856\n",
      "Epoch 46/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2933 - val_loss: 0.2856\n",
      "Epoch 47/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2933 - val_loss: 0.2856\n",
      "Epoch 48/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2933 - val_loss: 0.2856\n",
      "Epoch 49/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2933 - val_loss: 0.2855\n",
      "Epoch 50/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2932 - val_loss: 0.2855\n",
      "Epoch 51/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2932 - val_loss: 0.2854\n",
      "Epoch 52/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2931 - val_loss: 0.2853\n",
      "Epoch 53/200\n",
      "1148/1148 [==============================] - 147s 128ms/step - loss: 0.2929 - val_loss: 0.2851\n",
      "Epoch 54/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2927 - val_loss: 0.2848\n",
      "Epoch 55/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2925 - val_loss: 0.2845\n",
      "Epoch 56/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2922 - val_loss: 0.2842\n",
      "Epoch 57/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2918 - val_loss: 0.2839\n",
      "Epoch 58/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2916 - val_loss: 0.2836\n",
      "Epoch 59/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2913 - val_loss: 0.2834\n",
      "Epoch 60/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2912 - val_loss: 0.2833\n",
      "Epoch 61/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2910 - val_loss: 0.2831\n",
      "Epoch 62/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2909 - val_loss: 0.2830\n",
      "Epoch 63/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2907 - val_loss: 0.2829\n",
      "Epoch 64/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2906 - val_loss: 0.2827\n",
      "Epoch 65/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2905 - val_loss: 0.2826\n",
      "Epoch 66/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2904 - val_loss: 0.2825\n",
      "Epoch 67/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2903 - val_loss: 0.2824\n",
      "Epoch 68/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2902 - val_loss: 0.2823\n",
      "Epoch 69/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2901 - val_loss: 0.2823\n",
      "Epoch 70/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2901 - val_loss: 0.2822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2900 - val_loss: 0.2821\n",
      "Epoch 72/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2900 - val_loss: 0.2821\n",
      "Epoch 73/200\n",
      "1148/1148 [==============================] - 148s 129ms/step - loss: 0.2900 - val_loss: 0.2821\n",
      "Epoch 74/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 75/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 76/200\n",
      "1148/1148 [==============================] - 148s 129ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 77/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 78/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 79/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 80/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 81/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 82/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 83/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 84/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 85/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 86/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 87/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 88/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 89/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 90/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 91/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 92/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 93/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 94/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 95/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 96/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 97/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 98/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 99/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 100/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 101/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 102/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 103/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 104/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 105/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 106/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 107/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 108/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 109/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 110/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 111/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 112/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 113/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 114/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 115/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2899 - val_loss: 0.2820\n",
      "Epoch 116/200\n",
      "1148/1148 [==============================] - 150s 130ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 117/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 118/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 119/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 120/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 121/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 122/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 123/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 124/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 125/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 126/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 127/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 128/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 129/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 130/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 131/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 132/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 133/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 134/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 135/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 136/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 137/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 138/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 139/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 140/200\n",
      "1148/1148 [==============================] - 150s 130ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 141/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 142/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 143/200\n",
      "1148/1148 [==============================] - 150s 130ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 144/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 145/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 147/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 148/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 149/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 150/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 151/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 152/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 153/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 154/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 155/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 156/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 157/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 158/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 159/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 160/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 161/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 162/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2820\n",
      "Epoch 163/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 164/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 165/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 166/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 167/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 168/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 169/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 170/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 171/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 172/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 173/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 174/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 175/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 176/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 177/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 178/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 179/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 180/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 181/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 182/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 183/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 184/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 185/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 186/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 187/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 188/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 189/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 190/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 191/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 192/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 193/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 194/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2898 - val_loss: 0.2819\n",
      "Epoch 195/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2897 - val_loss: 0.2819\n",
      "Epoch 196/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2897 - val_loss: 0.2819\n",
      "Epoch 197/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.2897 - val_loss: 0.2819\n",
      "Epoch 198/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2897 - val_loss: 0.2819\n",
      "Epoch 199/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.2897 - val_loss: 0.2819\n",
      "Epoch 200/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.2897 - val_loss: 0.2819\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(3840, 64), return_sequences=True))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error')\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=41,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6355ffa4a8>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqKwZ5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65298fd1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 4s 42ms/step\n",
      "Test loss: 0.28574026089448196\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/home/isa/FYPJ/Model/model8_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46026632 0.44264427 0.39014342 0.40442243 0.41143885 0.43361333\n",
      " 0.4200494  0.42916384 0.4222737  0.38678864 0.43080163 0.411844\n",
      " 0.41896376 0.43146744 0.4155897  0.46425533 0.40230793 0.41974735\n",
      " 0.41565102 0.45346117 0.40833646 0.43215743 0.43942034 0.48479968\n",
      " 0.4393999  0.41659117 0.50681406 0.44388688 0.43448523 0.40944767\n",
      " 0.42888308 0.44309515 0.39096662 0.46406403 0.40209875 0.42784292\n",
      " 0.40910843 0.46105662 0.43134883 0.43771157 0.487213   0.48481238\n",
      " 0.41641414 0.44520605 0.42258924 0.4034257  0.4290756  0.40982604\n",
      " 0.40389735 0.40516776 0.41301137 0.37367314 0.39858127 0.4099515\n",
      " 0.39445227 0.41254142 0.41597903 0.4652351  0.40414697 0.42036358\n",
      " 0.48156136 0.5609727  0.4218799  0.40377125]\n",
      "[0.4369539  0.41306028 0.37526998 0.3888559  0.4049546  0.4138789\n",
      " 0.40142173 0.4146559  0.39161655 0.38248056 0.4007835  0.4029619\n",
      " 0.39721724 0.396877   0.39231235 0.43475905 0.38268292 0.40121138\n",
      " 0.40590516 0.41760722 0.39531553 0.411126   0.4235892  0.44245404\n",
      " 0.43396956 0.39624345 0.5062901  0.4044484  0.42386666 0.3908761\n",
      " 0.39873978 0.4206285  0.3933924  0.44933087 0.39397788 0.40268412\n",
      " 0.3957702  0.42112494 0.4081692  0.41228658 0.45246604 0.45160577\n",
      " 0.38384336 0.4129193  0.4207444  0.37884495 0.40578368 0.38719258\n",
      " 0.3980633  0.40280533 0.391906   0.39161828 0.40173334 0.39827088\n",
      " 0.3909598  0.38411906 0.38628462 0.42061332 0.39606917 0.39506903\n",
      " 0.44330212 0.5195636  0.40973166 0.39460915]\n",
      "[0.38639915 0.3626164  0.3290194  0.3391007  0.3519417  0.3635382\n",
      " 0.35007212 0.34757793 0.3560877  0.3305225  0.36455834 0.35753873\n",
      " 0.35499144 0.34021652 0.3418452  0.39092508 0.3536471  0.35401434\n",
      " 0.3627339  0.3691618  0.367981   0.35339728 0.38178635 0.37473097\n",
      " 0.38889495 0.36068568 0.5037191  0.37048593 0.37301654 0.33139616\n",
      " 0.36064753 0.3878494  0.36908278 0.44557515 0.34616035 0.37429535\n",
      " 0.32753482 0.34417185 0.35952738 0.36151332 0.39434293 0.38657445\n",
      " 0.34247896 0.35563177 0.35570735 0.34078896 0.34443402 0.3391318\n",
      " 0.35274094 0.35092354 0.35055766 0.33628526 0.375518   0.36441332\n",
      " 0.3284017  0.33656788 0.32655412 0.36774886 0.35140926 0.3453081\n",
      " 0.37288883 0.48527604 0.3803259  0.34658358]\n",
      "[0.3426027  0.31862494 0.29141006 0.307726   0.3066938  0.30996826\n",
      " 0.300622   0.32391384 0.30989358 0.2907091  0.31650817 0.30404326\n",
      " 0.30741647 0.2942564  0.3077379  0.32308137 0.3133924  0.30853373\n",
      " 0.32525206 0.3211002  0.30499876 0.30743626 0.32535914 0.31984103\n",
      " 0.3388248  0.31671095 0.502402   0.29892027 0.32196805 0.30213782\n",
      " 0.30921394 0.34015888 0.31046766 0.3584839  0.30791613 0.318325\n",
      " 0.2948411  0.31181404 0.30555996 0.32464746 0.3351585  0.326909\n",
      " 0.2930185  0.32469207 0.3186161  0.2969748  0.31397656 0.31266382\n",
      " 0.3067968  0.3207166  0.31458512 0.30388993 0.31869248 0.3266323\n",
      " 0.2974944  0.30190694 0.30204418 0.3183212  0.30446973 0.3053465\n",
      " 0.32411474 0.40439674 0.31444773 0.30888742]\n",
      "[0.30581117 0.28675106 0.27980945 0.2742745  0.29139236 0.28337714\n",
      " 0.28538242 0.28567326 0.27914643 0.27232662 0.27604362 0.28498176\n",
      " 0.2788224  0.27647758 0.28545064 0.29195324 0.27764666 0.2843676\n",
      " 0.28782177 0.28987655 0.28113168 0.28542656 0.30075428 0.29134536\n",
      " 0.30538833 0.28512922 0.5018429  0.28583267 0.29155213 0.27847323\n",
      " 0.2883538  0.2891403  0.29010352 0.31798914 0.2835274  0.28051886\n",
      " 0.2865875  0.28456235 0.2796913  0.29005164 0.29804924 0.2961933\n",
      " 0.28495622 0.2920436  0.28848    0.27339673 0.29002008 0.28291354\n",
      " 0.28367975 0.29574504 0.2826456  0.27992707 0.2815213  0.28551048\n",
      " 0.2890383  0.28205106 0.28100464 0.28110403 0.27657586 0.28418487\n",
      " 0.28819245 0.3286999  0.2889009  0.2838684 ]\n",
      "[0.27850735 0.27240655 0.26976204 0.2705586  0.27311546 0.27159515\n",
      " 0.27111372 0.27452713 0.27031538 0.27003232 0.2717801  0.2724594\n",
      " 0.2714279  0.26942512 0.27046925 0.2759067  0.27035952 0.2706266\n",
      " 0.27489278 0.27314976 0.2700993  0.27216896 0.28108317 0.27210423\n",
      " 0.27934393 0.27091438 0.50075376 0.2698675  0.27206388 0.27083603\n",
      " 0.2720705  0.2721294  0.27259094 0.2849634  0.2705618  0.27049834\n",
      " 0.269571   0.26978612 0.27257314 0.27122065 0.27466235 0.2745261\n",
      " 0.26969385 0.2741178  0.272691   0.26947677 0.2704469  0.27099004\n",
      " 0.2718223  0.2736064  0.26997647 0.2725368  0.27309915 0.27242172\n",
      " 0.27163023 0.27196172 0.27093422 0.2717457  0.27153146 0.271556\n",
      " 0.2736438  0.28776783 0.2736596  0.27050042]\n",
      "[0.27301174 0.2701782  0.2691238  0.26942587 0.27061993 0.26971996\n",
      " 0.26988524 0.26982424 0.26929885 0.2692272  0.2696864  0.27005425\n",
      " 0.26969916 0.26902455 0.26935086 0.27128932 0.26936325 0.26934537\n",
      " 0.27093393 0.2702724  0.26916805 0.27002347 0.2739152  0.26962414\n",
      " 0.27270162 0.26939857 0.50033265 0.26909783 0.2696291  0.26937616\n",
      " 0.26988494 0.26979285 0.26990876 0.2753092  0.26929495 0.26936662\n",
      " 0.26908728 0.26913127 0.27034104 0.26946834 0.27023083 0.2707075\n",
      " 0.26920396 0.27125463 0.27006236 0.26910365 0.2693995  0.26955402\n",
      " 0.26992413 0.27069813 0.2691203  0.2699764  0.27037004 0.27031443\n",
      " 0.26973894 0.2700253  0.26929113 0.26957464 0.26978877 0.26974106\n",
      " 0.27059412 0.2778141  0.2706694  0.26932582]\n",
      "[0.27035743 0.26930216 0.26896665 0.26904872 0.2696283  0.26917085\n",
      " 0.26917455 0.26915863 0.26902512 0.26901865 0.26914945 0.26937577\n",
      " 0.2690982  0.26895437 0.26908442 0.26965106 0.2690352  0.26902854\n",
      " 0.2696769  0.26922804 0.26901227 0.2692975  0.2707378  0.26908627\n",
      " 0.27021796 0.26904565 0.5001676  0.26897034 0.26911068 0.26902714\n",
      " 0.2691958  0.26913664 0.26919135 0.27196822 0.26901022 0.26904875\n",
      " 0.26897132 0.26897678 0.26937914 0.2690599  0.26916468 0.26942125\n",
      " 0.2689996  0.26967838 0.26917157 0.26899296 0.26903418 0.26904938\n",
      " 0.26926598 0.26951727 0.26899168 0.26924062 0.26932845 0.2692133\n",
      " 0.26926872 0.26930857 0.2690359  0.26906762 0.26913804 0.26927122\n",
      " 0.269528   0.2719865  0.26947674 0.2690255 ]\n",
      "[0.26939696 0.26904196 0.2689478  0.26896623 0.26920727 0.2690036\n",
      " 0.26901627 0.26900303 0.26896223 0.26896608 0.26900044 0.2691311\n",
      " 0.26897463 0.2689438  0.2689859  0.2691894  0.26897186 0.26896024\n",
      " 0.2692525  0.2690247  0.26895478 0.26905873 0.26991034 0.2689874\n",
      " 0.2694187  0.26898697 0.50009215 0.26894668 0.26898685 0.2689583\n",
      " 0.26909685 0.26898766 0.26900953 0.2706684  0.2689567  0.26897538\n",
      " 0.26894736 0.26894763 0.26910403 0.26897043 0.2690041  0.26907066\n",
      " 0.26895815 0.269251   0.26899415 0.2689592  0.2689601  0.26896542\n",
      " 0.26904473 0.26913127 0.2689586  0.26902223 0.26905724 0.26904246\n",
      " 0.26904503 0.26909    0.26896468 0.2689678  0.26900062 0.26901883\n",
      " 0.26908138 0.27014568 0.26911515 0.26896134]\n",
      "[0.26911697 0.26897082 0.26894295 0.2689486  0.26906192 0.2689574\n",
      " 0.2689785  0.2689701  0.268947   0.26894993 0.26895872 0.2690011\n",
      " 0.26895466 0.26894188 0.26896325 0.2690346  0.26895285 0.26894563\n",
      " 0.26907447 0.26897448 0.26894405 0.26900518 0.26934716 0.26895183\n",
      " 0.26911128 0.26896307 0.5000486  0.26894227 0.26895508 0.26894522\n",
      " 0.26901758 0.26895267 0.2689684  0.26963422 0.26894477 0.2689575\n",
      " 0.2689427  0.26894274 0.2690174  0.2689489  0.268965   0.26898247\n",
      " 0.26894605 0.26908255 0.268957   0.26894778 0.26894578 0.26895294\n",
      " 0.26898742 0.26903343 0.2689461  0.26897052 0.26898587 0.2689789\n",
      " 0.26897505 0.26902038 0.26894772 0.26894936 0.26895967 0.26897576\n",
      " 0.26900116 0.26949993 0.26900083 0.2689462 ]\n",
      "[0.2690025  0.26894984 0.26894173 0.2689433  0.26899475 0.26894554\n",
      " 0.26895657 0.26895592 0.26894286 0.2689438  0.26894593 0.26896247\n",
      " 0.2689468  0.2689415  0.2689502  0.2689736  0.2689451  0.2689423\n",
      " 0.26899627 0.26895282 0.26894206 0.2689755  0.26911533 0.2689438\n",
      " 0.26900297 0.2689543  0.50002587 0.26894155 0.26894647 0.26894224\n",
      " 0.26897252 0.2689439  0.26895344 0.269215   0.26894218 0.26895118\n",
      " 0.26894173 0.26894173 0.2689711  0.2689438  0.26895237 0.2689551\n",
      " 0.2689424  0.26900208 0.26894554 0.26894358 0.26894242 0.26894557\n",
      " 0.2689595  0.26897755 0.26894256 0.26895294 0.268959   0.26895255\n",
      " 0.26895255 0.26898956 0.26894307 0.26894328 0.2689469  0.26895806\n",
      " 0.26896724 0.2691839  0.26896036 0.26894253]\n"
     ]
    }
   ],
   "source": [
    "x_predict = test_dataset['x'][:1]\n",
    "predictions = model.predict(x_predict)\n",
    "print(predictions[0][0]) # 1\n",
    "print(predictions[0][1]) # 2 (1st row)\n",
    "print(predictions[0][2]) # 1\n",
    "print(predictions[0][3]) # 2 (2nd row)\n",
    "print(predictions[0][4]) # 1\n",
    "print(predictions[0][5]) # 2 (3rd row)\n",
    "print(predictions[0][6]) # 1\n",
    "print(predictions[0][7]) # 2 (4th row)\n",
    "print(predictions[0][8]) # 1\n",
    "print(predictions[0][9]) # 2 (5th row)\n",
    "print(predictions[0][10])# 1\n",
    "print(predictions[0][11])# 2 (6th row)\n",
    "print(predictions[0][12])# 1\n",
    "print(predictions[0][13])# 2 (7th row)\n",
    "print(predictions[0][14])# 1\n",
    "print(predictions[0][15])# 2 (8th row)\n",
    "print(predictions[0][788])# 1\n",
    "print(predictions[0][789])# 2 (OFFSET 0xC50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
