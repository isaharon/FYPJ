{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of seed files collected: 68\n",
      "Data collection of XY simulated to 1% sampling rate, Dataset split: 60/4/4\n",
      "No. of samples: 1148, No. of timesteps: 3840, Chunksize: 64\n",
      "Training shape:  (1148, 3840, 64) (1148, 3840, 64)\n",
      "Validation shape:  (82, 3840, 64) (82, 3840, 64)\n",
      "Test shape:  (104, 3840, 64) (104, 3840, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train = \"Data/readelf/8/train_dataset8.npz\"\n",
    "val = \"Data/readelf/8/val_dataset8.npz\"\n",
    "test = \"Data/readelf/8/test_dataset8.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset = np.load(train)\n",
    "val_dataset = np.load(val)\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = training_dataset['x'][:1148]\n",
    "y_train = training_dataset['y'][:1148]\n",
    "\n",
    "x_val = val_dataset['x']\n",
    "y_val = val_dataset['y']\n",
    "\n",
    "x_test = test_dataset['x']\n",
    "y_test = test_dataset['y']\n",
    "\n",
    "print(\"No. of seed files collected: 68\")\n",
    "print(\"Data collection of XY simulated to 1% sampling rate, Dataset split: 60/4/4\")\n",
    "samples, timesteps, chunksize = x_train.shape\n",
    "print(\"No. of samples: \" + str(samples) + \", No. of timesteps: \" + str(timesteps) + \", Chunksize: \" + str(chunksize))\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, x_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 3840, 64)          33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3840, 64)          4160      \n",
      "=================================================================\n",
      "Total params: 37,184\n",
      "Trainable params: 37,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1148 samples, validate on 82 samples\n",
      "Epoch 1/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.4996 - val_loss: 0.4987\n",
      "Epoch 2/200\n",
      "1148/1148 [==============================] - 149s 130ms/step - loss: 0.4962 - val_loss: 0.4966\n",
      "Epoch 3/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.4922 - val_loss: 0.4939\n",
      "Epoch 4/200\n",
      "1148/1148 [==============================] - 159s 139ms/step - loss: 0.4866 - val_loss: 0.4898\n",
      "Epoch 5/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.4782 - val_loss: 0.4836\n",
      "Epoch 6/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.4658 - val_loss: 0.4729\n",
      "Epoch 7/200\n",
      "1148/1148 [==============================] - 148s 129ms/step - loss: 0.4425 - val_loss: 0.4064\n",
      "Epoch 8/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.3652 - val_loss: 0.3380\n",
      "Epoch 9/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.3204 - val_loss: 0.2988\n",
      "Epoch 10/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.2863 - val_loss: 0.2665\n",
      "Epoch 11/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.2571 - val_loss: 0.2382\n",
      "Epoch 12/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.2314 - val_loss: 0.2134\n",
      "Epoch 13/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.2089 - val_loss: 0.1915\n",
      "Epoch 14/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.1891 - val_loss: 0.1719\n",
      "Epoch 15/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.1716 - val_loss: 0.1544\n",
      "Epoch 16/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.1561 - val_loss: 0.1388\n",
      "Epoch 17/200\n",
      "1148/1148 [==============================] - 158s 137ms/step - loss: 0.1427 - val_loss: 0.1255\n",
      "Epoch 18/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.1314 - val_loss: 0.1143\n",
      "Epoch 19/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.1220 - val_loss: 0.1050\n",
      "Epoch 20/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.1142 - val_loss: 0.0972\n",
      "Epoch 21/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.1076 - val_loss: 0.0908\n",
      "Epoch 22/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.1021 - val_loss: 0.0853\n",
      "Epoch 23/200\n",
      "1148/1148 [==============================] - 159s 139ms/step - loss: 0.0973 - val_loss: 0.0805\n",
      "Epoch 24/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.0932 - val_loss: 0.0764\n",
      "Epoch 25/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0895 - val_loss: 0.0728\n",
      "Epoch 26/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0863 - val_loss: 0.0696\n",
      "Epoch 27/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0835 - val_loss: 0.0668\n",
      "Epoch 28/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0810 - val_loss: 0.0643\n",
      "Epoch 29/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0787 - val_loss: 0.0620\n",
      "Epoch 30/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0767 - val_loss: 0.0600\n",
      "Epoch 31/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.0748 - val_loss: 0.0581\n",
      "Epoch 32/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.0730 - val_loss: 0.0563\n",
      "Epoch 33/200\n",
      "1148/1148 [==============================] - 161s 140ms/step - loss: 0.0714 - val_loss: 0.0547\n",
      "Epoch 34/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0699 - val_loss: 0.0532\n",
      "Epoch 35/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0685 - val_loss: 0.0518\n",
      "Epoch 36/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.0672 - val_loss: 0.0505\n",
      "Epoch 37/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.0660 - val_loss: 0.0493\n",
      "Epoch 38/200\n",
      "1148/1148 [==============================] - 159s 139ms/step - loss: 0.0649 - val_loss: 0.0481\n",
      "Epoch 39/200\n",
      "1148/1148 [==============================] - 159s 138ms/step - loss: 0.0638 - val_loss: 0.0471\n",
      "Epoch 40/200\n",
      "1148/1148 [==============================] - 158s 137ms/step - loss: 0.0628 - val_loss: 0.0461\n",
      "Epoch 41/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.0619 - val_loss: 0.0451\n",
      "Epoch 42/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0610 - val_loss: 0.0442\n",
      "Epoch 43/200\n",
      "1148/1148 [==============================] - 159s 139ms/step - loss: 0.0602 - val_loss: 0.0434\n",
      "Epoch 44/200\n",
      "1148/1148 [==============================] - 157s 136ms/step - loss: 0.0594 - val_loss: 0.0426\n",
      "Epoch 45/200\n",
      "1148/1148 [==============================] - 160s 139ms/step - loss: 0.0586 - val_loss: 0.0418\n",
      "Epoch 46/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.0579 - val_loss: 0.0411\n",
      "Epoch 47/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0572 - val_loss: 0.0404\n",
      "Epoch 48/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0566 - val_loss: 0.0397\n",
      "Epoch 49/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0559 - val_loss: 0.0391\n",
      "Epoch 50/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0554 - val_loss: 0.0385\n",
      "Epoch 51/200\n",
      "1148/1148 [==============================] - 157s 136ms/step - loss: 0.0548 - val_loss: 0.0380\n",
      "Epoch 52/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0543 - val_loss: 0.0374\n",
      "Epoch 53/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0538 - val_loss: 0.0369\n",
      "Epoch 54/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0533 - val_loss: 0.0364\n",
      "Epoch 55/200\n",
      "1148/1148 [==============================] - 159s 139ms/step - loss: 0.0528 - val_loss: 0.0359\n",
      "Epoch 56/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0524 - val_loss: 0.0355\n",
      "Epoch 57/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.0519 - val_loss: 0.0350\n",
      "Epoch 58/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0515 - val_loss: 0.0346\n",
      "Epoch 59/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0511 - val_loss: 0.0342\n",
      "Epoch 60/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0507 - val_loss: 0.0338\n",
      "Epoch 61/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0504 - val_loss: 0.0335\n",
      "Epoch 62/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.0500 - val_loss: 0.0331\n",
      "Epoch 63/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0497 - val_loss: 0.0328\n",
      "Epoch 64/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0494 - val_loss: 0.0325\n",
      "Epoch 65/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0491 - val_loss: 0.0321\n",
      "Epoch 66/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0488 - val_loss: 0.0318\n",
      "Epoch 67/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0485 - val_loss: 0.0315\n",
      "Epoch 68/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.0482 - val_loss: 0.0312\n",
      "Epoch 69/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.0479 - val_loss: 0.0310\n",
      "Epoch 70/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0477 - val_loss: 0.0307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200\n",
      "1148/1148 [==============================] - 160s 139ms/step - loss: 0.0474 - val_loss: 0.0305\n",
      "Epoch 72/200\n",
      "1148/1148 [==============================] - 162s 141ms/step - loss: 0.0472 - val_loss: 0.0302\n",
      "Epoch 73/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0469 - val_loss: 0.0300\n",
      "Epoch 74/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0467 - val_loss: 0.0297\n",
      "Epoch 75/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0465 - val_loss: 0.0295\n",
      "Epoch 76/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0463 - val_loss: 0.0293\n",
      "Epoch 77/200\n",
      "1107/1148 [===========================>..] - ETA: 5s - loss: 0.0461 "
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(3840, 64), return_sequences=True))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error')\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=41,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/home/isa/FYPJ/Model/model8_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test[0])\n",
    "print(predictions[0][0])\n",
    "print(predictions[0][1])\n",
    "print(predictions[0][2])\n",
    "print(predictions[0][3])\n",
    "print(predictions[0][4])\n",
    "print(predictions[0][5])\n",
    "print(predictions[0][6])\n",
    "print(predictions[0][7])\n",
    "print(predictions[0][8])\n",
    "print(predictions[0][9])\n",
    "print(predictions[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
