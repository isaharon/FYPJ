{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of seed files collected: 112\n",
      "Data collection of XY simulated to 2% sampling rate, Dataset split: 87/20/5\n",
      "No. of training samples: 5120, No. of timesteps: 2560, Chunksize: 64\n",
      "Training shape:  (5120, 2560, 64) (5120, 2560, 64)\n",
      "Validation shape:  (992, 2560, 64) (992, 2560, 64)\n",
      "Test shape:  (239, 2560, 64) (239, 2560, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train = \"Data/readelf/12/train_dataset.npz\"\n",
    "val = \"Data/readelf/12/val_dataset.npz\"\n",
    "test = \"Data/readelf/12/test_dataset.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset = np.load(train)\n",
    "val_dataset = np.load(val)\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = training_dataset['x'][:5120]\n",
    "y_train = training_dataset['y'][:5120]\n",
    "\n",
    "x_val = val_dataset['x'][:992]\n",
    "y_val = val_dataset['y'][:992]\n",
    "\n",
    "x_test = test_dataset['x']\n",
    "y_test = test_dataset['y']\n",
    "\n",
    "print(\"No. of seed files collected: 112\")\n",
    "print(\"Data collection of XY simulated to 2% sampling rate, Dataset split: 87/20/5\")\n",
    "samples, timesteps, chunksize = x_train.shape\n",
    "print(\"No. of training samples: \" + str(samples) + \", No. of timesteps: \" + str(timesteps) + \", Chunksize: \" + str(chunksize))\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, y_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 2560, 64)          33024     \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5120 samples, validate on 992 samples\n",
      "Epoch 1/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0891 - val_loss: 0.0713\n",
      "Epoch 2/100\n",
      "5120/5120 [==============================] - 628s 123ms/step - loss: 0.0731 - val_loss: 0.0627\n",
      "Epoch 3/100\n",
      "5120/5120 [==============================] - 635s 124ms/step - loss: 0.0663 - val_loss: 0.0579\n",
      "Epoch 4/100\n",
      "5120/5120 [==============================] - 633s 124ms/step - loss: 0.0621 - val_loss: 0.0545\n",
      "Epoch 5/100\n",
      "5120/5120 [==============================] - 624s 122ms/step - loss: 0.0589 - val_loss: 0.0519\n",
      "Epoch 6/100\n",
      "5120/5120 [==============================] - 617s 121ms/step - loss: 0.0565 - val_loss: 0.0497\n",
      "Epoch 7/100\n",
      "5120/5120 [==============================] - 623s 122ms/step - loss: 0.0544 - val_loss: 0.0480\n",
      "Epoch 8/100\n",
      "5120/5120 [==============================] - 622s 121ms/step - loss: 0.0527 - val_loss: 0.0465\n",
      "Epoch 9/100\n",
      "5120/5120 [==============================] - 620s 121ms/step - loss: 0.0513 - val_loss: 0.0452\n",
      "Epoch 10/100\n",
      "5120/5120 [==============================] - 633s 124ms/step - loss: 0.0501 - val_loss: 0.0442\n",
      "Epoch 11/100\n",
      "5120/5120 [==============================] - 641s 125ms/step - loss: 0.0490 - val_loss: 0.0433\n",
      "Epoch 12/100\n",
      "5120/5120 [==============================] - 635s 124ms/step - loss: 0.0482 - val_loss: 0.0425\n",
      "Epoch 13/100\n",
      "5120/5120 [==============================] - 638s 125ms/step - loss: 0.0474 - val_loss: 0.0418\n",
      "Epoch 14/100\n",
      "5120/5120 [==============================] - 638s 125ms/step - loss: 0.0468 - val_loss: 0.0412\n",
      "Epoch 15/100\n",
      "5120/5120 [==============================] - 638s 125ms/step - loss: 0.0462 - val_loss: 0.0407\n",
      "Epoch 16/100\n",
      "5120/5120 [==============================] - 643s 126ms/step - loss: 0.0457 - val_loss: 0.0402\n",
      "Epoch 17/100\n",
      "5120/5120 [==============================] - 630s 123ms/step - loss: 0.0452 - val_loss: 0.0398\n",
      "Epoch 18/100\n",
      "5120/5120 [==============================] - 622s 121ms/step - loss: 0.0448 - val_loss: 0.0394\n",
      "Epoch 19/100\n",
      "5120/5120 [==============================] - 620s 121ms/step - loss: 0.0445 - val_loss: 0.0391\n",
      "Epoch 20/100\n",
      "5120/5120 [==============================] - 617s 120ms/step - loss: 0.0442 - val_loss: 0.0388\n",
      "Epoch 21/100\n",
      "5120/5120 [==============================] - 615s 120ms/step - loss: 0.0439 - val_loss: 0.0385\n",
      "Epoch 22/100\n",
      "5120/5120 [==============================] - 619s 121ms/step - loss: 0.0436 - val_loss: 0.0383\n",
      "Epoch 23/100\n",
      "5120/5120 [==============================] - 625s 122ms/step - loss: 0.0434 - val_loss: 0.0381\n",
      "Epoch 24/100\n",
      "5120/5120 [==============================] - 619s 121ms/step - loss: 0.0432 - val_loss: 0.0379\n",
      "Epoch 25/100\n",
      "5120/5120 [==============================] - 617s 121ms/step - loss: 0.0430 - val_loss: 0.0377\n",
      "Epoch 26/100\n",
      "5120/5120 [==============================] - 619s 121ms/step - loss: 0.0428 - val_loss: 0.0375\n",
      "Epoch 27/100\n",
      "5120/5120 [==============================] - 619s 121ms/step - loss: 0.0426 - val_loss: 0.0374\n",
      "Epoch 28/100\n",
      "5120/5120 [==============================] - 623s 122ms/step - loss: 0.0425 - val_loss: 0.0372\n",
      "Epoch 29/100\n",
      "5120/5120 [==============================] - 617s 120ms/step - loss: 0.0424 - val_loss: 0.0371\n",
      "Epoch 30/100\n",
      "5120/5120 [==============================] - 614s 120ms/step - loss: 0.0422 - val_loss: 0.0370\n",
      "Epoch 31/100\n",
      "5120/5120 [==============================] - 619s 121ms/step - loss: 0.0421 - val_loss: 0.0369\n",
      "Epoch 32/100\n",
      "5120/5120 [==============================] - 620s 121ms/step - loss: 0.0420 - val_loss: 0.0368\n",
      "Epoch 33/100\n",
      "5120/5120 [==============================] - 621s 121ms/step - loss: 0.0420 - val_loss: 0.0367\n",
      "Epoch 34/100\n",
      "5120/5120 [==============================] - 620s 121ms/step - loss: 0.0419 - val_loss: 0.0366\n",
      "Epoch 35/100\n",
      "5120/5120 [==============================] - 617s 121ms/step - loss: 0.0418 - val_loss: 0.0366\n",
      "Epoch 36/100\n",
      "5120/5120 [==============================] - 603s 118ms/step - loss: 0.0418 - val_loss: 0.0365\n",
      "Epoch 37/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0417 - val_loss: 0.0365\n",
      "Epoch 38/100\n",
      "5120/5120 [==============================] - 620s 121ms/step - loss: 0.0416 - val_loss: 0.0364\n",
      "Epoch 39/100\n",
      "5120/5120 [==============================] - 622s 121ms/step - loss: 0.0416 - val_loss: 0.0364\n",
      "Epoch 40/100\n",
      "5120/5120 [==============================] - 610s 119ms/step - loss: 0.0416 - val_loss: 0.0364\n",
      "Epoch 41/100\n",
      "5120/5120 [==============================] - 619s 121ms/step - loss: 0.0415 - val_loss: 0.0363\n",
      "Epoch 42/100\n",
      "5120/5120 [==============================] - 610s 119ms/step - loss: 0.0415 - val_loss: 0.0363\n",
      "Epoch 43/100\n",
      "5120/5120 [==============================] - 603s 118ms/step - loss: 0.0415 - val_loss: 0.0363\n",
      "Epoch 44/100\n",
      "5120/5120 [==============================] - 608s 119ms/step - loss: 0.0415 - val_loss: 0.0363\n",
      "Epoch 45/100\n",
      "5120/5120 [==============================] - 603s 118ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 46/100\n",
      "5120/5120 [==============================] - 617s 121ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 47/100\n",
      "5120/5120 [==============================] - 616s 120ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 48/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 49/100\n",
      "5120/5120 [==============================] - 605s 118ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 50/100\n",
      "5120/5120 [==============================] - 609s 119ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 51/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 52/100\n",
      "5120/5120 [==============================] - 612s 120ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 53/100\n",
      "5120/5120 [==============================] - 614s 120ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 54/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 55/100\n",
      "5120/5120 [==============================] - 615s 120ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 56/100\n",
      "5120/5120 [==============================] - 619s 121ms/step - loss: 0.0414 - val_loss: 0.0362\n",
      "Epoch 57/100\n",
      "5120/5120 [==============================] - 600s 117ms/step - loss: 0.0413 - val_loss: 0.0362\n",
      "Epoch 58/100\n",
      "5120/5120 [==============================] - 597s 117ms/step - loss: 0.0413 - val_loss: 0.0362\n",
      "Epoch 59/100\n",
      "5120/5120 [==============================] - 595s 116ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 60/100\n",
      "5120/5120 [==============================] - 612s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 61/100\n",
      "5120/5120 [==============================] - 616s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 62/100\n",
      "5120/5120 [==============================] - 612s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 63/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 64/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 65/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 66/100\n",
      "5120/5120 [==============================] - 614s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 67/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 68/100\n",
      "5120/5120 [==============================] - 620s 121ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 69/100\n",
      "5120/5120 [==============================] - 612s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 70/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 71/100\n",
      "5120/5120 [==============================] - 612s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120/5120 [==============================] - 615s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 73/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 74/100\n",
      "5120/5120 [==============================] - 610s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 75/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 76/100\n",
      "5120/5120 [==============================] - 616s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 77/100\n",
      "5120/5120 [==============================] - 615s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 78/100\n",
      "5120/5120 [==============================] - 616s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 79/100\n",
      "5120/5120 [==============================] - 616s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 80/100\n",
      "5120/5120 [==============================] - 615s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 81/100\n",
      "5120/5120 [==============================] - 612s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 82/100\n",
      "5120/5120 [==============================] - 611s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 83/100\n",
      "5120/5120 [==============================] - 604s 118ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 84/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 85/100\n",
      "5120/5120 [==============================] - 619s 121ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 86/100\n",
      "5120/5120 [==============================] - 614s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 87/100\n",
      "5120/5120 [==============================] - 609s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 88/100\n",
      "5120/5120 [==============================] - 617s 121ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 89/100\n",
      "5120/5120 [==============================] - 608s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 90/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 91/100\n",
      "5120/5120 [==============================] - 614s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 92/100\n",
      "5120/5120 [==============================] - 608s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 93/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 94/100\n",
      "5120/5120 [==============================] - 612s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 95/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 96/100\n",
      "5120/5120 [==============================] - 614s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 97/100\n",
      "5120/5120 [==============================] - 614s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 98/100\n",
      "5120/5120 [==============================] - 613s 120ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 99/100\n",
      "5120/5120 [==============================] - 618s 121ms/step - loss: 0.0413 - val_loss: 0.0361\n",
      "Epoch 100/100\n",
      "5120/5120 [==============================] - 609s 119ms/step - loss: 0.0413 - val_loss: 0.0361\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(2560, 64), return_sequences=True))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error')\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"/home/isa/FYPJ/Model/model_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f66b5a97ac8>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl0HOWd7vHvr7u1y5ZsSbbxvoIt\nDDhgVpNAICRAEpyZQFiyMBNOCDPh3knInVzmzNzMhFnukMkMWeBOQiBcyCTBxMkkvoQACRBgWIwN\n2HgHb2B5lS1vkq2lpd/9o0p2u9VttW1JLVc/n3P61PZ211sun6dKb71VZe6OiIgUhli+KyAiIgNH\noS8iUkAU+iIiBUShLyJSQBT6IiIFRKEvIlJAFPoiIgVEoS8iUkAU+iIiBSSR7wqkq62t9YkTJ+a7\nGiIiJ5XXX399p7vX9VZu0IX+xIkTWbx4cb6rISJyUjGzd3Mpp+YdEZECklPom9mVZrbGzNaa2Z0Z\nlpeY2bxw+UIzmxjOLzazh8xsmZktNbNL+7T2IiJyTHoNfTOLA/cBVwH1wI1mVp9W7BZgt7tPBe4B\n7g7nfwHA3c8ArgD+1cz014WISJ7kEsDnAWvdfb27twOPAnPTyswFHg7H5wOXm5kRHCSeAXD3HcAe\nYHZfVFxERI5dLqE/BtiUMt0QzstYxt2TwF6gBlgKzDWzhJlNAs4Bxp1opUVE5Pjk0nvHMsxLf/NK\ntjI/AmYAi4F3gZeBZI8VmN0K3Aowfvz4HKokIiLHI5cz/QaOPDsfC2zJVsbMEkAV0OTuSXf/irvP\ncve5QDXwTvoK3P1+d5/t7rPr6nrtZioiIscpl9BfBEwzs0lmVgzcACxIK7MAuDkcvxZ41t3dzMrN\nrALAzK4Aku6+so/qfoQtew7yb0+vYcPOlv74eRGRSOi1ecfdk2Z2O/AUEAd+5O4rzOwuYLG7LwAe\nBH5sZmuBJoIDA8AI4Ckz6wI2A5/tj40A2NXcznefXcvMMVVMqq3or9WIiJzUcroj192fAJ5Im/f1\nlPFW4LoM39sInHZiVcxNRUkcgJb2HpcMREQkFJk+85UlwfGrua0zzzURERm8ohP6pUHot7TpTF9E\nJJvIhH5ZUZyYQXOrQl9EJJvIhL6ZUVGcoFln+iIiWUUm9AEqShJq3hEROYpIhX5laUK9d0REjiJS\noV9RklDvHRGRo4hU6FeWxGlu7ch3NUREBq1IhX5FcYIWnemLiGQVqdCvLFHvHRGRo4lW6OtCrojI\nUUUq9Lu7bLqnP+5fREQgYqFfWZKgo9NpS3bluyoiIoNSpEK/ojh80qba9UVEMopU6FeWFgGoB4+I\nSBbRCv3wmfrqwSMiklmkQr/i0DP1FfoiIplEMvTVpi8iklmkQn+IzvRFRI4qUqGvM30RkaOLZOjr\nTF9EJLNohX6xeu+IiBxNpEI/EY9RVhRX846ISBaRCn3Qi1RERI4mcqFfWaIzfRGRbCIX+no5uohI\ndpEL/cqSBPsV+iIiGUUy9HWmLyKSWU6hb2ZXmtkaM1trZndmWF5iZvPC5QvNbGI4v8jMHjazZWa2\nysz+qm+r35Oad0REsus19M0sDtwHXAXUAzeaWX1asVuA3e4+FbgHuDucfx1Q4u5nAOcAX+w+IPQX\n9d4REckulzP984C17r7e3duBR4G5aWXmAg+H4/OBy83MAAcqzCwBlAHtwL4+qXkWlSVxmts6+nMV\nIiInrVxCfwywKWW6IZyXsYy7J4G9QA3BAaAF2Aq8B3zL3ZtOsM5HVVlSRGtHF8lOvTJRRCRdLqFv\nGealv3k8W5nzgE5gNDAJ+KqZTe6xArNbzWyxmS1ubGzMoUrZVYQvUmlpVxOPiEi6XEK/ARiXMj0W\n2JKtTNiUUwU0ATcBT7p7h7vvAF4CZqevwN3vd/fZ7j67rq7u2LciRaWetCkiklUuob8ImGZmk8ys\nGLgBWJBWZgFwczh+LfCsuztBk85lFqgALgBW903VM9PjlUVEsus19MM2+tuBp4BVwGPuvsLM7jKz\na8JiDwI1ZrYWuAPo7tZ5H1AJLCc4eDzk7m/18TYcobI0CH3doCUi0lMil0Lu/gTwRNq8r6eMtxJ0\nz0z/XnOm+f1JzTsiItlF7o7cimKFvohINpEL/cpDb89S7x0RkXTRC/1SnemLiGQTudDv7qevVyaK\niPQUudAvScQpiptCX0Qkg8iFPuhJmyIi2UQy9CtLEjrTFxHJILqh36rQFxFJF8nQryhJ0NKu0BcR\nSRfZ0Fc/fRGRniIZ+pUlcV3IFRHJIKKhr947IiKZRDL0K3QhV0Qko0iGfmV4ITd4pL+IiHSLZOhX\nlCTocjjYoYu5IiKpIhn6h5+0qSYeEZFUkQ79FnXbFBE5QiRDv/s9ubqYKyJypIiGvh6vLCKSSSRD\nf0hJEaAXqYiIpItk6OtMX0Qks0iG/oihpQBs29ea55qIiAwukQz9ypIEVWVFbN59MN9VEREZVCIZ\n+gBjqsvYvEehLyKSKrqhP6xMZ/oiImmiG/rhmb6evyMiclh0Qj/ZDrvWQVszAGOHldHclmTfQfXg\nERHpFp3Q37oEvnc2vPcKEJzpAzTsOZDPWomIDCrRCf3KkcFw/zYgaNMH1K4vIpIip9A3syvNbI2Z\nrTWzOzMsLzGzeeHyhWY2MZz/aTNbkvLpMrNZfbsJoe7Qb94OpJzpK/RFRA7pNfTNLA7cB1wF1AM3\nmll9WrFbgN3uPhW4B7gbwN1/4u6z3H0W8Flgo7sv6csNOKSoFEqrDoX+8IpiSoti6rYpIpIilzP9\n84C17r7e3duBR4G5aWXmAg+H4/OBy83M0srcCPzsRCrbq8qRh0LfzIIePDrTFxE5JJfQHwNsSplu\nCOdlLOPuSWAvUJNW5nqyhL6Z3Wpmi81scWNjYy71zqxyJOzffrhSw8p1pi8ikiKX0E8/YwdI7/x+\n1DJmdj5wwN2XZ1qBu9/v7rPdfXZdXV0OVcoi5UwfdFeuiEi6XEK/ARiXMj0W2JKtjJklgCqgKWX5\nDfR30w7AkFFB6Ic3ZI0dVkZTSzsH2tVXX0QEcgv9RcA0M5tkZsUEAb4grcwC4OZw/FrgWQ9vhTWz\nGHAdwbWA/lU5AjoOQHtwg1Z3D54tOtsXEQFyCP2wjf524ClgFfCYu68ws7vM7Jqw2INAjZmtBe4A\nUrt1fgBocPf1fVv1DCpHBcOwXb+7r766bYqIBBK5FHL3J4An0uZ9PWW8leBsPtN3/wBccPxVPAaV\nI4Jh83aonXroTF/t+iIigejckQtBmz5Ac3BX7sihpSRipm6bIiKhaIX+oUcxBM078ZgxqqpUZ/oi\nIqFohX7ZMIgV9ey2qTN9EREgaqFv1rOv/jD11RcR6Rat0AcYcmTojx1WzrZ9rbQnu/JYKRGRwSF6\noZ/2KIax1WW4w7a9rXmslIjI4BDN0E9r3gG9TEVEBKIa+gd2QmcHcPiuXF3MFRGJYugPCbtttgRP\n6xxdXUZR3FjX2JLHSomIDA7RC/201yYWJ2KcOnIIK7bszWOlREQGhwiGfvdduTsOzTp99FBWbNlH\n+Aw4EZGCFb3Q727eCR/FADBzTBVNLe1s26cePCJS2KIX+hXdD1078kwfYPnmffmokYjIoBG90E8U\nQ9nwQ236ADNOGYoZatcXkYIXvdCHHn31y4sTTK6t0Jm+iBS8aIZ+2qMYIGjXX6kzfREpcNEM/bRH\nMUDQrr9lbytNLe15qpSISP5FN/RTXpAOMHN0FaB2fREpbNEN/c42aN1zaFa9evCIiEQ09If0vEGr\nuryYscPKdKYvIgUtmqHf/YL0lG6bcPjOXBGRQhXR0A/P9NNCf+boKjbsbGF/a0ceKiUikn/RDP1h\nE8DisPPtI2afPiZo11+1dX8+aiUiknfRDP1ECdRMgcbVR8zu7sGzfLPa9UWkMEUz9AFGzIAdK4+c\nNbSUMdVlLNrYlKdKiYjkV4RDvx6aNkD7ka9JvHBKDa+s30VXlx6zLCKFJ8KhPwNw2LnmiNlzptaw\n50AHK7eqF4+IFJ7ohn7djGC448h2/TlTagF4ae3Oga6RiEje5RT6Znalma0xs7VmdmeG5SVmNi9c\nvtDMJqYsO9PMXjGzFWa2zMxK+676RzF8MsSLM7brTxtRyUvrdg1INUREBpNeQ9/M4sB9wFVAPXCj\nmdWnFbsF2O3uU4F7gLvD7yaA/wBuc/fTgUuBgekkH09A7WmwY1WPRXOm1rJoQxNtyc4BqYqIyGCR\ny5n+ecBad1/v7u3Ao8DctDJzgYfD8fnA5WZmwIeBt9x9KYC773L3gUvaETN6dNsEuGhKDQc7Onnz\nvT0ZviQiEl25hP4YYFPKdEM4L2MZd08Ce4Ea4FTAzewpM3vDzL524lU+BiOmw95N0HrkRdvzJ9cQ\nM3hZ7foiUmByCX3LMC+9v2O2MgngYuDT4fCPzOzyHiswu9XMFpvZ4sbGxhyqlKMRYStU2tl+VVkR\nZ4ytVru+iBScXEK/ARiXMj0W2JKtTNiOXwU0hfOfd/ed7n4AeAI4O30F7n6/u89299l1dXXHvhXZ\njOjuwdOzXf/iqTUs3bSH5rZk361PRGSQyyX0FwHTzGySmRUDNwAL0sosAG4Ox68FnnV3B54CzjSz\n8vBgcAmwkoFSNR6KKjJfzJ1SS7LLeW2DzvZFpHD0GvphG/3tBAG+CnjM3VeY2V1mdk1Y7EGgxszW\nAncAd4bf3Q38G8GBYwnwhrv/pu83I4tYDOpO69FtE+DsCcMoScR44W2164tI4UjkUsjdnyBomkmd\n9/WU8Vbguizf/Q+Cbpv5MaIe3nm6x+zSojjvn1bL0yu28fWP1ROLZbosISISLdG9I7fbiBnQsgNa\nejbjXH3GKWzZ28qSBnXdFJHCUAChPz0YNvZs1/9Q/UiK4sZvl20d4EqJiORHAYR+2G1z+4oei4aW\nFvH+aXU8sWwbwXVnEZFoi37oDzkFho6B917NuPjqM05h856DLG3Qi1VEJPqiH/pmMOEiePclyHA2\nf8WMoInnCTXxiEgBiH7oA0yYA83boWl9j0VV5UXMmVrLb97aqiYeEYm8wgl9CM72M+hu4lmmd+eK\nSMQVRujXToOKOtiYOfQ/XD+SRMz4zVtq4hGRaCuM0D/Urv9yxsXV5cVccmodv3xzMx2dXQNcORGR\ngVMYoQ9BE8/e92DPexkX33T+eBr3t/H7ldsHuGIiIgOnsEIfsp7tX3raCEZXlfLT1zIfFEREoqBw\nQn9EPZRWZb2YG48Z1587nhff2cm7u1oGuHIiIgOjcEI/FoPxF2W9mAtw/bnjiMeMn722KWsZEZGT\nWeGEPsDEOdC0DvZvy7h4VFUpl08fwc8Xb6I9qQu6IhI9hRX6Ey4Khlna9SG4oLurpZ2nVmQ+MIiI\nnMwKK/RHnQUlQ2HdM1mLfGBaHeOGl/HIKxsHrFoiIgOlsEI/noBpV8CaJ6GrM2ORWMy4Zc4kFm3c\nzcL1epWiiERLYYU+wPSPwYGdsGlh1iI3nDee2spi7n1u7QBWTESk/xVe6E+7AuLFsOrxrEVKi+J8\n4f2TefGdnSzZpLdqiUh0FF7olwyByZfC6sczPmq526cvmEB1eRH3PquzfRGJjsILfQiaePa8C9uX\nZy1SWZLg83Mm8ftV21m5Zd8AVk5EpP8UZuifdhVgsPo3Ry1280UTGVKS4N7n3hmYeomI9LPCDP3K\nETD+gqO26wNUlRXx+Ysn8cSybSze2DRAlRMR6T+FGfoA0z8K25fB7o1HLfbFSyYzamgp3/h/K+nq\n0pu1ROTkVtihD72e7ZcXJ7jzquks27yX+W80DEDFRET6T+GG/vDJMOpMeGter0XnzhrN2eOr+eaT\na9jf2jEAlRMR6R+FG/oA7/ssbHsLti49ajEz428/fjo7m9t0w5aInNQKO/TPvA7iJfDGj3steta4\naq49ZywPvriB5XqBuoicpHIKfTO70szWmNlaM7szw/ISM5sXLl9oZhPD+RPN7KCZLQk/3+/b6p+g\nsmEw4+Ow7DHoONhr8b/56AyGVxTz1ceW0pbM/OweEZHBrNfQN7M4cB9wFVAP3Ghm9WnFbgF2u/tU\n4B7g7pRl69x9Vvi5rY/q3XfO/iy07u31gi4EL1C/+5Nnsmb7fu75nfrui8jJJ5cz/fOAte6+3t3b\ngUeBuWll5gIPh+PzgcvNzPqumv1o4gegegK8+UhOxT84fQQ3nDuO+19Yx+vvqu++iJxccgn9MUDq\n+wMbwnkZy7h7EtgL1ITLJpnZm2b2vJm9/wTr2/diseCC7oYXoGlDTl/5m4/VM7q6jDseW8o+9eYR\nkZNILqGf6Yw9/S6lbGW2AuPd/X3AHcBPzWxojxWY3Wpmi81scWNjYw5V6mOzbgKLwZv/kVPxypIE\n375+Fpt3H+Qrjy7RTVsictLIJfQbgHEp02OBLdnKmFkCqAKa3L3N3XcBuPvrwDrg1PQVuPv97j7b\n3WfX1dUd+1acqKoxMO0j8PpD0H4gp6/Mnjicv/14Pc+s3sG3f/92P1dQRKRv5BL6i4BpZjbJzIqB\nG4AFaWUWADeH49cCz7q7m1ldeCEYM5sMTAPW903V+9icv4ADu3I+2wf4zAUTuO6csXz32bU8uXxr\nP1ZORKRv9Br6YRv97cBTwCrgMXdfYWZ3mdk1YbEHgRozW0vQjNPdrfMDwFtmtpTgAu9t7j44r35O\nuBDGXQAvfw86c2unNzP+/hMzOWtcNXc8tlQvXBGRQc/8KC8SyYfZs2f74sWL87PyNb+Fn90Af/xD\nOPNTOX9t+75Wrv3+y+xvTfLYFy/k1JFD+rGSIiI9mdnr7j67t3KFfUduumkfgbrp8F/fPupbtdKN\nHFrKT265gOJ4jM88sJD3duV2XUBEZKAp9FPFYkHb/o4V8M7vjumr42vK+fEt59OW7OLTD77KpiYF\nv4gMPgr9dDOvhaFj4YVvHtPZPsBpo4bw8OfPY++BDq79/sus2ba/nyopInJ8FPrpEsVwydegYRGs\n/PUxf33WuGoeu+1C3OFTP3iF19/d3Q+VFBE5Pgr9TN73GRhRD7//W0i2HfPXp48ayi/+7CKGlRfx\nmQcW8vhb6bc1iIjkh0I/k1gcPvz3wasUFz1wXD8xbng5P7/tIupHD+X2n77J//7tKjp1566I5JlC\nP5upH4Ipl8Hz34QDx3drQd2QEn72hQv4zAXj+cHz6/mTh16jcf+x/+UgItJXFPpH8+F/gLZ98MK/\nHPdPFCdi/MMnzuDuT57Bwg1NXPntF3h6xbY+rKSISO4U+kcz8nQ4+3Ow8Aew5c0T+qnrzx3P4//t\nYkYOLeXWH7/O1+brCZ0iMvAU+r350Degog5+fTsk20/op04dOYRffWkOX/rgFOa/3sBl33qeX725\nmcF2V7SIRJdCvzdl1fCxe2D7cvive07454oTMf7yI9P59ZcuZkx1KV+et4Qbf/gqq7bu64PKiogc\nnUI/F9OvhjOuC9r2t6/ok588Y2wVv/zzOfzjH81k1db9XP3dF/nKvCW6k1dE+pUeuJarll1w33nB\ns/dv+R0kSvrsp/ce6ODfn1/HQy9toMudT80ex22XTGHc8PI+W4eIRJseuNbXKmrgmu/C1qXw5F/1\n6U9XlRdx51XTef4vP8h1s8fx88UNXPqtP3DHvCWs3qZmHxHpOzrTP1ZP/y94+bvwR/fDWdf3yyq2\n7W3lgRfX85OF73Gwo5PzJw3nT+dM5EMzRpKI6zgtIj3leqav0D9WnUl45BrY/AZ84VkYWd9vq9pz\noJ15izbxyCvvsnnPQUYOLeGTZ4/lU7PHMbG2ot/WKyInH4V+f9q/HX7wfiiuDNr3K2r6dXWdXc4z\nq7Yzb9Emnluzgy6H2ROGcc2s0Vx9xinUVvbd9QUROTkp9Pvbe6/CI3Nh5Ey4eQEUD8yZ9/Z9rfzi\njQZ+9eZm3t7eTDxmXDi5ho+cPpIr6kcxqqp0QOohIoOLQn8grHocHvts8JyeG34K8aIBXf3qbftY\nsGQLTy7fxvqdLQCcNbaKS08bwQenj+DMMVXEYjagdRKR/FDoD5TFD8HjX4azboK59wVv3xpg7s7a\nHc08tWIbz6zewZJNe3CHYeVFXDilhoum1HLRlBom1VZgpoOASBTlGvqJgahMpM3+U2jeAX/4p2D6\nmu9BfGD/Wc2MaSOHMG3kEG6/bBpNLe28+E4jL7y9k5fX7eSJZcED3morSzh34jBmTxzOrHHVnD56\nKKVF8QGtq4jkl0K/L1zyNTCD5/4ROg7AH/8weANXngyvKGburDHMnTUGd2fDzhZeXd/E4o1NvLax\nid8uDw4CRXFjxilDOX10FTPHBMPTRg6hrFgHApGoUvNOX3r5Xnj6r+HUK+HaHw3Yxd1jtX1fK0s2\n7eHN9/awdNMeVmzZy77WJBAcuybWVHDayCFMG1nJ1BGVTKmrZFJtBRUlOkcQGazUpp8vi38Ev/lq\n8FjmGx+FqrH5rlGv3J2G3QdZsWUvq7ftZ034ebfpwBFv+6obUsKkmgrGDS9n/PByxg0vY0x1GWOG\nlTFyaClFunFMJG8U+vn0zu9g/uchUQo3/ATGnZfvGh2XtmQn7+46wNodzWzc1cLGnS1s3HmA95oO\nsG1f6xFlYxZcMxhVVcrIoaWMHFpCXWUpdUNKqKkspraymOEVJQwvL2ZIaUK9ikT6mEI/3xrXwE+v\nh32b4Yq/h/O/GLSdRERrRyeb9xxkS/jZvKeVbXsPsm1fG9v3ttLY3EZTS+b3D8RjRnVZEVXlRVSV\nFTG0NBhWliYYUppgSEmCivBTWZKgvDhOeXEwLC2KU1Ycp6woTkkiRmlRnLgOICIK/UHhQBP86s/g\n7Sdh2kfgE/8HKmrzXasB09HZxc7mNnY1t7OrpZ1d4YFgz4EOmg60s/dAB/taO9h7sIN9BzvY35pk\nf2uS9s6uY1pPImaUJGIUJ2KUJOIUJ2IUxY3iRJyiuFEUj5GIhcO4kYgZiViMeDgejxlxC4fhJ5Yy\nbcah5WZB2ZhBLFwW6542OzRuKcPuMkYwJGXajEPLIeV7BN/rLkM4HYyFv5vy/bBEsDClzOHxw2VS\nf+eQDMvTy6R2981WJp1lWslRy2T5nd6LpP3msZ8IDIZTh+ryIibUHN+1QIX+YOEOr/0Qnv6b4IUs\nH/1XmPHxfNdqUGtLdtLS1klLW5LmtiQH2js52N5JS3uS1o5OWjuC6bZkF60dXbQmO2lPdtGe7KIt\n2UlHp4fjXSS7uujo7KKj00l2dpHscjo6na4uJ9kVTCc7nS53OrvCTzje1eV0OXR693gwLdJfPnbm\nKdx709nH9V310x8szOD8W2HCRfCft8G8z8BpH4Wr/yV4Nr/0UJKIU5KIM7wif91es/Ew+LuHXe54\neGDo7HII53W54xxe7qnzwwPHoWUp49D9++Ac/m7waxz6bury4Fsceu2mp5SjR5nD23G4xJG/2WPB\nkaMpv51Wvse/VebvH1mm96PoMR9nj+PAfLTtGEgjhvT/Y1RyCn0zuxL4DhAHHnD3f05bXgI8ApwD\n7AKud/eNKcvHAyuBv3P3b/VN1U8yo2bCrc/BK/fBH/45eCHLB/4HnP9nUKTn5ZwsguYdGByNASLH\nrtc+dmYWB+4DrgLqgRvNLP15wrcAu919KnAPcHfa8nuA3554dU9y8SK4+MvwpVdh4sXw+7+De8+F\nZfOh69jasUVEjkcuHavPA9a6+3p3bwceBeamlZkLPByOzwcut/BKipl9AlgP9M3LZaNg2ES4aR58\nbgGUVcEvbgke1bzy1wp/EelXuYT+GGBTynRDOC9jGXdPAnuBGjOrAP4n8I2jrcDMbjWzxWa2uLGx\nMde6n/wmXwK3vhC8hSvZCo99Dr4/B5bOg2Tm7o4iIicil9DP1HiZftUjW5lvAPe4e/PRVuDu97v7\nbHefXVdXl0OVIiQWC167+KXX4I8fgK5O+M9b4Ttnwov/GnT7FBHpI7lcyG0AxqVMjwW2ZCnTYGYJ\noApoAs4HrjWzbwLVQJeZtbr7vSdc86iJxeHM62DmJ2HdM8EF32fuCi761s+Fc/4EJsyJ1A1eIjLw\ncgn9RcA0M5sEbAZuAG5KK7MAuBl4BbgWeNaDvljv7y5gZn8HNCvwexGLwbQrgs+OVcHz+pc+Cst+\nDsMmwZnXw5mfgpop+a6piJyEem3eCdvobweeAlYBj7n7CjO7y8yuCYs9SNCGvxa4A7izvypcUEbM\ngKu/CV9dDZ/4PlSPg+fvhu+dDfd/EF76DjRtyHctReQkojtyTzZ7Nwdn/St/BVveDOaNOiN4nPOp\nV8Ho9+Xl7V0ikl96DEMh2P1u0M1zzW9h06vgXVBeC5MvhSmXBUPd9StSEBT6heZAE6z9ffBZ9xy0\n7AjmD5sU3Ag2YU7wiOfhk3UxWCSCFPqFzB22r4ANL8C7L8HG/4LWPcGy8loYey6MOTtoCjplFlQW\nWDdZkQjSA9cKmVnwrJ9RM+HCPw/u8m1cDQ2vwabXoGFR8Ljn7tstKkcF1wVGzYS6GTBiOtSeCkVl\ned0MEel7Cv1CEIvByPrgc86fBPNa98G2t2DLEti+HLYtg/XPQVcy/JJB1TionQo104JmoeGTguai\n6nE6IIicpBT6hap0aNDWP/Hiw/M6O2DXuuCvgsbVsGst7HwHNv0E2tNuqq4YAdXjgwvFQ8PPkFEw\n5JRgWDkyeDG8rh+IDCoKfTksXhQ07YyYfuR8d2jZCbs3BPcF7H0P9oSf7SuDdwJ3HOj5e4my4HpB\neW3wxrDyGigbHrxMprQ6HFYd/hRXQsmQ4BOLD8w2ixQYhb70ziwI78q6zC95dw8uFO/fDvu3wv5t\n0LwdWhrDz05o3gE7VsPBpp5/NWSSKAv+Uuj+FJVBUXnwsvmi0mB5oiSYTpRAvDgcLw7G48XBQSxW\nFA7jwXgskTKdCD4WD6bNUsZThmbheCycjh3+dB+cUudZLPwLxw6PWyycNv31I3ml0JcTZwZlw4JP\n+l8JmXR2wME9wYGidV843BscDNr2B5/2ZmhvgbZmSB6EjoPQfgAO7ISO1mBesj14OmmyFZJt4J39\nv619JfUgkHpwOGKctDJkmBcO4RjGj6jIkct71DO93FEL9XJAy7VcL9/NWqQvD6YDeGBOrffUD8FH\n/rFfV6fQl4EXLzr8l0Nf6uoKCOGaAAAEoUlEQVQMwr+zPbggnWyDrg7oTAbDrmTw6UwGB4iuZHAA\n8s7wfYad4fzUoaeMd4Wf7vldwXz8cLnu+d6VMt/D8e7vp05nGk95JyIp388071CX6xzHU3n68nRp\n68xY5IgXKWb5nWMo1+t3sxY6tt884fX12cqOnBw6ut/XqNCX6IjFobgcKM93TUQGLT2kRUSkgCj0\nRUQKiEJfRKSAKPRFRAqIQl9EpIAo9EVECohCX0SkgCj0RUQKyKB7iYqZNQLvnsBP1AI7+6g6J4tC\n3GYozO3WNheOY93uCe7e623ugy70T5SZLc7l7TFRUojbDIW53drmwtFf263mHRGRAqLQFxEpIFEM\n/fvzXYE8KMRthsLcbm1z4eiX7Y5cm76IiGQXxTN9ERHJIjKhb2ZXmtkaM1trZnfmuz79wczGmdlz\nZrbKzFaY2V+E84eb2e/M7J1wOCzfde0PZhY3szfN7PFwepKZLQy3e56ZFee7jn3JzKrNbL6ZrQ73\n+YWFsK/N7Cvh/+/lZvYzMyuN4r42sx+Z2Q4zW54yL+P+tcB3w3x7y8zOPt71RiL0zSwO3AdcBdQD\nN5pZfX5r1S+SwFfdfQZwAfClcDvvBJ5x92nAM+F0FP0FsCpl+m7gnnC7dwO35KVW/ec7wJPuPh04\ni2DbI72vzWwM8N+B2e4+E4gDNxDNff1/gSvT5mXbv1cB08LPrcC/H+9KIxH6wHnAWndf7+7twKPA\n3DzXqc+5+1Z3fyMc308QAmMItvXhsNjDwCfyU8P+Y2ZjgY8CD4TTBlwGzA+LRGq7zWwo8AHgQQB3\nb3f3PRTAviZ4o1+ZmSUIXoO2lQjua3d/AWhKm51t/84FHvHAq0C1mZ1yPOuNSuiPATalTDeE8yLL\nzCYC7wMWAiPdfSsEBwZgRP5q1m++DXwN6Aqna4A97p4Mp6O2zycDjcBDYZPWA2ZWQcT3tbtvBr4F\nvEcQ9nuB14n2vk6Vbf/2WcZFJfQzvbo+st2SzKwS+AXwZXffl+/69Dcz+xiww91fT52doWiU9nkC\nOBv4d3d/H9BCxJpyMgnbsOcCk4DRQAVB00a6KO3rXPTZ//eohH4DMC5leiywJU916VdmVkQQ+D9x\n91+Gs7d3/6kXDnfkq379ZA5wjZltJGi6u4zgzL86bAKA6O3zBqDB3ReG0/MJDgJR39cfAja4e6O7\ndwC/BC4i2vs6Vbb922cZF5XQXwRMC6/wFxNc+FmQ5zr1ubAd+0Fglbv/W8qiBcDN4fjNwK8Hum79\nyd3/yt3HuvtEgn37rLt/GngOuDYsFqntdvdtwCYzOy2cdTmwkojva4JmnQvMrDz8/9693ZHd12my\n7d8FwOfCXjwXAHu7m4GOmbtH4gNcDbwNrAP+Ot/16adtvJjgT7q3gCXh52qC9u1ngHfC4fB817Uf\n/w0uBR4PxycDrwFrgZ8DJfmuXx9v6yxgcbi/fwUMK4R9DXwDWA0sB34MlERxXwM/I7hu0UFwJn9L\ntv1L0LxzX5hvywh6Nx3XenVHrohIAYlK846IiORAoS8iUkAU+iIiBUShLyJSQBT6IiIFRKEvIlJA\nFPoiIgVEoS8iUkD+P5xOtYgPoroyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6b31ac4470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 7s 29ms/step\n",
      "Test loss: 0.04077609654157481\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View some predictions of some chunks\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0. -0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. -0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0. -0.  0.  0.]\n",
      "[-2.60037632e-06 -1.22511096e-06  9.84580083e-07 -7.97806820e-07\n",
      "  3.46744628e-06 -8.91832656e-07  3.40134051e-07  1.42182373e-07\n",
      "  1.78359039e-06  8.16317595e-07 -8.78932894e-07 -1.37469260e-06\n",
      " -1.68187597e-07  9.31696491e-07  6.38510016e-07  2.00424984e-06\n",
      " -2.33852415e-06 -7.92758158e-07  6.65491541e-07  1.39257239e-07\n",
      "  9.74656177e-06  3.04683624e-07  2.22545509e-06 -1.05559877e-06\n",
      "  1.77209097e-06  2.43739282e-06  7.45037767e-07  3.01715045e-06\n",
      " -1.62907850e-06  1.75009507e-06  1.14404941e-06 -3.90147591e-07\n",
      " -5.83277426e-07  1.72423120e-06 -1.78731148e-06  1.60753314e-06\n",
      " -3.89487241e-07  9.20611996e-08  1.30267267e-06 -3.19264086e-06\n",
      "  7.25866158e-08  5.93389927e-07  2.11557477e-07  6.49507754e-08\n",
      "  1.14975091e-06 -2.42094097e-06  1.56255476e-06  1.85093722e-06\n",
      " -3.62675877e-07 -1.57112072e-06  3.10121163e-06 -5.08018729e-07\n",
      "  2.32255999e-07  2.38268044e-07 -1.05537666e-07 -3.66774685e-07\n",
      " -6.94415803e-07  3.12896555e-06  7.93660035e-07 -1.35442394e-06\n",
      " -1.51482766e-06  1.17042043e-06 -1.18215428e-06  6.93330833e-07]\n",
      "[-5.15786951e-06  6.41683846e-06 -2.08670463e-06 -1.43536145e-05\n",
      "  3.07706614e-06 -2.74162585e-06  7.88375291e-06 -1.35627470e-05\n",
      " -1.76944468e-06 -1.18978051e-05 -1.66549205e-06 -8.78741901e-07\n",
      "  6.70425067e-08  9.48063098e-06  6.35427568e-06 -5.71018336e-06\n",
      " -3.34370043e-07  1.06288617e-05  5.11173812e-06 -2.89957075e-06\n",
      "  8.83833491e-06  2.37655877e-05  1.00067064e-05 -1.68795623e-06\n",
      " -1.09020693e-05  2.76529931e-06 -3.88419767e-06  5.59085220e-06\n",
      " -4.93918651e-06  4.54909787e-06  4.58947852e-06 -3.18546427e-06\n",
      " -1.92131210e-06 -3.31623119e-06  4.27775331e-06 -5.93005507e-06\n",
      " -5.63307822e-06  9.78713706e-06  7.25105383e-06  4.50944799e-06\n",
      "  1.05083782e-05  7.96176118e-06 -6.87088288e-07  3.76533512e-06\n",
      " -4.20313199e-06  4.42551027e-06  5.22348785e-07  7.70254793e-09\n",
      "  1.13335163e-05  2.47109256e-06 -1.17383997e-05 -7.02548869e-06\n",
      " -4.97268366e-06  1.64566279e-06  2.12291479e-06  2.69555312e-06\n",
      " -4.87289746e-08  3.03239489e-07 -6.34622575e-06  1.22863084e-05\n",
      "  4.78533138e-06  3.92558286e-06  3.30150237e-06  3.25585961e-06]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0. -0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. -0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0. -0.  0.  0.]\n",
      "[-2.60037632e-06 -1.22511096e-06  9.84580083e-07 -7.97806820e-07\n",
      "  3.46744628e-06 -8.91832656e-07  3.40134051e-07  1.42182373e-07\n",
      "  1.78359039e-06  8.16317595e-07 -8.78932894e-07 -1.37469260e-06\n",
      " -1.68187597e-07  9.31696491e-07  6.38510016e-07  2.00424984e-06\n",
      " -2.33852415e-06 -7.92758158e-07  6.65491541e-07  1.39257239e-07\n",
      "  9.74656177e-06  3.04683624e-07  2.22545509e-06 -1.05559877e-06\n",
      "  1.77209097e-06  2.43739282e-06  7.45037767e-07  3.01715045e-06\n",
      " -1.62907850e-06  1.75009507e-06  1.14404941e-06 -3.90147591e-07\n",
      " -5.83277426e-07  1.72423120e-06 -1.78731148e-06  1.60753314e-06\n",
      " -3.89487241e-07  9.20611996e-08  1.30267267e-06 -3.19264086e-06\n",
      "  7.25866158e-08  5.93389927e-07  2.11557477e-07  6.49507754e-08\n",
      "  1.14975091e-06 -2.42094097e-06  1.56255476e-06  1.85093722e-06\n",
      " -3.62675877e-07 -1.57112072e-06  3.10121163e-06 -5.08018729e-07\n",
      "  2.32255999e-07  2.38268044e-07 -1.05537666e-07 -3.66774685e-07\n",
      " -6.94415803e-07  3.12896555e-06  7.93660035e-07 -1.35442394e-06\n",
      " -1.51482766e-06  1.17042043e-06 -1.18215428e-06  6.93330833e-07]\n",
      "[-5.15786951e-06  6.41683846e-06 -2.08670463e-06 -1.43536145e-05\n",
      "  3.07706614e-06 -2.74162585e-06  7.88375291e-06 -1.35627470e-05\n",
      " -1.76944468e-06 -1.18978051e-05 -1.66549205e-06 -8.78741901e-07\n",
      "  6.70425067e-08  9.48063098e-06  6.35427568e-06 -5.71018336e-06\n",
      " -3.34370043e-07  1.06288617e-05  5.11173812e-06 -2.89957075e-06\n",
      "  8.83833491e-06  2.37655877e-05  1.00067064e-05 -1.68795623e-06\n",
      " -1.09020693e-05  2.76529931e-06 -3.88419767e-06  5.59085220e-06\n",
      " -4.93918651e-06  4.54909787e-06  4.58947852e-06 -3.18546427e-06\n",
      " -1.92131210e-06 -3.31623119e-06  4.27775331e-06 -5.93005507e-06\n",
      " -5.63307822e-06  9.78713706e-06  7.25105383e-06  4.50944799e-06\n",
      "  1.05083782e-05  7.96176118e-06 -6.87088288e-07  3.76533512e-06\n",
      " -4.20313199e-06  4.42551027e-06  5.22348785e-07  7.70254793e-09\n",
      "  1.13335163e-05  2.47109256e-06 -1.17383997e-05 -7.02548869e-06\n",
      " -4.97268366e-06  1.64566279e-06  2.12291479e-06  2.69555312e-06\n",
      " -4.87289746e-08  3.03239489e-07 -6.34622575e-06  1.22863084e-05\n",
      "  4.78533138e-06  3.92558286e-06  3.30150237e-06  3.25585961e-06]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0. -0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0. -0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0. -0.  0.  0.]\n",
      "[-2.60037632e-06 -1.22511096e-06  9.84580083e-07 -7.97806820e-07\n",
      "  3.46744628e-06 -8.91832656e-07  3.40134051e-07  1.42182373e-07\n",
      "  1.78359039e-06  8.16317595e-07 -8.78932894e-07 -1.37469260e-06\n",
      " -1.68187597e-07  9.31696491e-07  6.38510016e-07  2.00424984e-06\n",
      " -2.33852415e-06 -7.92758158e-07  6.65491541e-07  1.39257239e-07\n",
      "  9.74656177e-06  3.04683624e-07  2.22545509e-06 -1.05559877e-06\n",
      "  1.77209097e-06  2.43739282e-06  7.45037767e-07  3.01715045e-06\n",
      " -1.62907850e-06  1.75009507e-06  1.14404941e-06 -3.90147591e-07\n",
      " -5.83277426e-07  1.72423120e-06 -1.78731148e-06  1.60753314e-06\n",
      " -3.89487241e-07  9.20611996e-08  1.30267267e-06 -3.19264086e-06\n",
      "  7.25866158e-08  5.93389927e-07  2.11557477e-07  6.49507754e-08\n",
      "  1.14975091e-06 -2.42094097e-06  1.56255476e-06  1.85093722e-06\n",
      " -3.62675877e-07 -1.57112072e-06  3.10121163e-06 -5.08018729e-07\n",
      "  2.32255999e-07  2.38268044e-07 -1.05537666e-07 -3.66774685e-07\n",
      " -6.94415803e-07  3.12896555e-06  7.93660035e-07 -1.35442394e-06\n",
      " -1.51482766e-06  1.17042043e-06 -1.18215428e-06  6.93330833e-07]\n",
      "[-5.15786951e-06  6.41683846e-06 -2.08670463e-06 -1.43536145e-05\n",
      "  3.07706614e-06 -2.74162585e-06  7.88375291e-06 -1.35627470e-05\n",
      " -1.76944468e-06 -1.18978051e-05 -1.66549205e-06 -8.78741901e-07\n",
      "  6.70425067e-08  9.48063098e-06  6.35427568e-06 -5.71018336e-06\n",
      " -3.34370043e-07  1.06288617e-05  5.11173812e-06 -2.89957075e-06\n",
      "  8.83833491e-06  2.37655877e-05  1.00067064e-05 -1.68795623e-06\n",
      " -1.09020693e-05  2.76529931e-06 -3.88419767e-06  5.59085220e-06\n",
      " -4.93918651e-06  4.54909787e-06  4.58947852e-06 -3.18546427e-06\n",
      " -1.92131210e-06 -3.31623119e-06  4.27775331e-06 -5.93005507e-06\n",
      " -5.63307822e-06  9.78713706e-06  7.25105383e-06  4.50944799e-06\n",
      "  1.05083782e-05  7.96176118e-06 -6.87088288e-07  3.76533512e-06\n",
      " -4.20313199e-06  4.42551027e-06  5.22348785e-07  7.70254793e-09\n",
      "  1.13335163e-05  2.47109256e-06 -1.17383997e-05 -7.02548869e-06\n",
      " -4.97268366e-06  1.64566279e-06  2.12291479e-06  2.69555312e-06\n",
      " -4.87289746e-08  3.03239489e-07 -6.34622575e-06  1.22863084e-05\n",
      "  4.78533138e-06  3.92558286e-06  3.30150237e-06  3.25585961e-06]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(\"View some predictions of some chunks\")\n",
    "print(predictions[0][0])\n",
    "print(predictions[0][1])\n",
    "print(predictions[0][2])\n",
    "print(predictions[1][0])\n",
    "print(predictions[1][1])\n",
    "print(predictions[1][2])\n",
    "print(predictions[13][0])\n",
    "print(predictions[13][1])\n",
    "print(predictions[13][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
