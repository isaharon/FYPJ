{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of dataset collected: 28\n",
      "No limit set for data collection except skip random no of files up to 5\n",
      "Training shape:  (5408, 3840, 64) (5408, 3840, 64)\n",
      "Validation shape:  (896, 3840, 64) (896, 3840, 64)\n",
      "Test shape:  (2278, 3840, 64) (2278, 3840, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train1 = \"Data/readelf/train_dataset1.npz\"\n",
    "train2 = \"Data/readelf/train_dataset2.npz\"\n",
    "val = \"Data/readelf/val_dataset.npz\"\n",
    "test = \"Data/readelf/test_dataset.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset1 = np.load(train1)\n",
    "training_dataset2 = np.load(train2)\n",
    "val_dataset = np.load(val)\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = np.concatenate((training_dataset1['x'], training_dataset2['x']))[:5408]\n",
    "y_train = np.concatenate((training_dataset1['y'], training_dataset2['y']))[:5408]\n",
    "\n",
    "# Swap val and test\n",
    "x_test = val_dataset['x']\n",
    "y_test = val_dataset['y']\n",
    "\n",
    "x_val = test_dataset['x'][:896]\n",
    "y_val = test_dataset['y'][:896]\n",
    "\n",
    "print(\"No. of dataset collected: 28\")\n",
    "print(\"No limit set for data collection except skip random no of files up to 5\")\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, x_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 3840, 64)          33024     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3840, 64)          0         \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5408 samples, validate on 896 samples\n",
      "Epoch 1/50\n",
      "5408/5408 [==============================] - 913s 169ms/step - loss: 0.4895 - mean_absolute_error: 0.4895 - mean_squared_error: 0.2406 - val_loss: 0.4772 - val_mean_absolute_error: 0.4772 - val_mean_squared_error: 0.2296\n",
      "Epoch 2/50\n",
      "5408/5408 [==============================] - 907s 168ms/step - loss: 0.4061 - mean_absolute_error: 0.4061 - mean_squared_error: 0.1703 - val_loss: 0.3518 - val_mean_absolute_error: 0.3518 - val_mean_squared_error: 0.1272\n",
      "Epoch 3/50\n",
      "5408/5408 [==============================] - 888s 164ms/step - loss: 0.3272 - mean_absolute_error: 0.3272 - mean_squared_error: 0.1111 - val_loss: 0.3046 - val_mean_absolute_error: 0.3046 - val_mean_squared_error: 0.0964\n",
      "Epoch 4/50\n",
      "5408/5408 [==============================] - 888s 164ms/step - loss: 0.2912 - mean_absolute_error: 0.2912 - mean_squared_error: 0.0889 - val_loss: 0.2814 - val_mean_absolute_error: 0.2814 - val_mean_squared_error: 0.0826\n",
      "Epoch 5/50\n",
      "5408/5408 [==============================] - 889s 164ms/step - loss: 0.2798 - mean_absolute_error: 0.2798 - mean_squared_error: 0.0821 - val_loss: 0.2774 - val_mean_absolute_error: 0.2774 - val_mean_squared_error: 0.0803\n",
      "Epoch 6/50\n",
      "5408/5408 [==============================] - 850s 157ms/step - loss: 0.2780 - mean_absolute_error: 0.2780 - mean_squared_error: 0.0811 - val_loss: 0.2767 - val_mean_absolute_error: 0.2767 - val_mean_squared_error: 0.0799\n",
      "Epoch 7/50\n",
      "5408/5408 [==============================] - 885s 164ms/step - loss: 0.2777 - mean_absolute_error: 0.2777 - mean_squared_error: 0.0809 - val_loss: 0.2766 - val_mean_absolute_error: 0.2766 - val_mean_squared_error: 0.0798\n",
      "Epoch 8/50\n",
      "5408/5408 [==============================] - 887s 164ms/step - loss: 0.2776 - mean_absolute_error: 0.2776 - mean_squared_error: 0.0809 - val_loss: 0.2766 - val_mean_absolute_error: 0.2766 - val_mean_squared_error: 0.0798\n",
      "Epoch 9/50\n",
      "5408/5408 [==============================] - 890s 164ms/step - loss: 0.2776 - mean_absolute_error: 0.2776 - mean_squared_error: 0.0809 - val_loss: 0.2765 - val_mean_absolute_error: 0.2765 - val_mean_squared_error: 0.0798\n",
      "Epoch 10/50\n",
      "5408/5408 [==============================] - 886s 164ms/step - loss: 0.2776 - mean_absolute_error: 0.2776 - mean_squared_error: 0.0809 - val_loss: 0.2765 - val_mean_absolute_error: 0.2765 - val_mean_squared_error: 0.0798\n",
      "Epoch 11/50\n",
      "5408/5408 [==============================] - 888s 164ms/step - loss: 0.2776 - mean_absolute_error: 0.2776 - mean_squared_error: 0.0808 - val_loss: 0.2765 - val_mean_absolute_error: 0.2765 - val_mean_squared_error: 0.0798\n",
      "Epoch 12/50\n",
      "5408/5408 [==============================] - 887s 164ms/step - loss: 0.2775 - mean_absolute_error: 0.2775 - mean_squared_error: 0.0808 - val_loss: 0.2765 - val_mean_absolute_error: 0.2765 - val_mean_squared_error: 0.0798\n",
      "Epoch 13/50\n",
      "5408/5408 [==============================] - 887s 164ms/step - loss: 0.2775 - mean_absolute_error: 0.2775 - mean_squared_error: 0.0808 - val_loss: 0.2765 - val_mean_absolute_error: 0.2765 - val_mean_squared_error: 0.0798\n",
      "Epoch 14/50\n",
      "5408/5408 [==============================] - 889s 164ms/step - loss: 0.2775 - mean_absolute_error: 0.2775 - mean_squared_error: 0.0808 - val_loss: 0.2765 - val_mean_absolute_error: 0.2765 - val_mean_squared_error: 0.0798\n",
      "Epoch 15/50\n",
      "5408/5408 [==============================] - 887s 164ms/step - loss: 0.2775 - mean_absolute_error: 0.2775 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0798\n",
      "Epoch 16/50\n",
      "5408/5408 [==============================] - 887s 164ms/step - loss: 0.2775 - mean_absolute_error: 0.2775 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 17/50\n",
      "5408/5408 [==============================] - 887s 164ms/step - loss: 0.2775 - mean_absolute_error: 0.2775 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 18/50\n",
      "5408/5408 [==============================] - 888s 164ms/step - loss: 0.2775 - mean_absolute_error: 0.2775 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 19/50\n",
      "5408/5408 [==============================] - 887s 164ms/step - loss: 0.2775 - mean_absolute_error: 0.2775 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 20/50\n",
      "5408/5408 [==============================] - 889s 164ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 21/50\n",
      "5408/5408 [==============================] - 888s 164ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 22/50\n",
      "5408/5408 [==============================] - 873s 161ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 23/50\n",
      "5408/5408 [==============================] - 860s 159ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0808 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 24/50\n",
      "5408/5408 [==============================] - 845s 156ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 25/50\n",
      "5408/5408 [==============================] - 863s 160ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2764 - val_mean_absolute_error: 0.2764 - val_mean_squared_error: 0.0797\n",
      "Epoch 26/50\n",
      "5408/5408 [==============================] - 863s 160ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 27/50\n",
      "5408/5408 [==============================] - 862s 159ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 28/50\n",
      "5408/5408 [==============================] - 888s 164ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 29/50\n",
      "5408/5408 [==============================] - 882s 163ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 30/50\n",
      "5408/5408 [==============================] - 877s 162ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 31/50\n",
      "5408/5408 [==============================] - 877s 162ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 32/50\n",
      "5408/5408 [==============================] - 864s 160ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 33/50\n",
      "5408/5408 [==============================] - 864s 160ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 34/50\n",
      "5408/5408 [==============================] - 874s 162ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0797\n",
      "Epoch 35/50\n",
      "5408/5408 [==============================] - 874s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 36/50\n",
      "5408/5408 [==============================] - 877s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 37/50\n",
      "5408/5408 [==============================] - 872s 161ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 38/50\n",
      "5408/5408 [==============================] - 878s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 39/50\n",
      "5408/5408 [==============================] - 878s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 40/50\n",
      "5408/5408 [==============================] - 876s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 41/50\n",
      "5408/5408 [==============================] - 875s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 42/50\n",
      "5408/5408 [==============================] - 874s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 43/50\n",
      "5408/5408 [==============================] - 875s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 44/50\n",
      "5408/5408 [==============================] - 876s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 45/50\n",
      "5408/5408 [==============================] - 874s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 46/50\n",
      "5408/5408 [==============================] - 877s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 47/50\n",
      "5408/5408 [==============================] - 876s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 48/50\n",
      "5408/5408 [==============================] - 873s 161ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 49/50\n",
      "5408/5408 [==============================] - 874s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763 - val_mean_squared_error: 0.0796\n",
      "Epoch 50/50\n",
      "5408/5408 [==============================] - 874s 162ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - mean_squared_error: 0.0807 - val_loss: 0.2762 - val_mean_absolute_error: 0.2762 - val_mean_squared_error: 0.0796\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(3840, 64), return_sequences=True))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error', metrics=['mae', 'mse'])\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe2087f6a90>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe27dd7b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - 81s 36ms/step\n",
      "Test loss: [0.27787759578259397, 0.2778775856566073, 0.08124468089641448]\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "[0.38990673 0.37668237 0.3913964  0.4857818  0.3863949  0.37736526\n",
      " 0.3473025  0.3856807  0.35738164 0.40645462 0.41414908 0.40107346\n",
      " 0.3650167  0.4225824  0.3874438  0.37155446 0.39143586 0.38099715\n",
      " 0.36927179 0.42088282 0.3864457  0.39732695 0.39057875 0.41258553\n",
      " 0.39450562 0.35611597 0.37574935 0.388465   0.34959912 0.3748214\n",
      " 0.38167927 0.43071124 0.39034867 0.36867958 0.37150905 0.3616166\n",
      " 0.36070707 0.37315515 0.38778064 0.37657124 0.35529864 0.38955605\n",
      " 0.37633222 0.39335924 0.4073108  0.38334128 0.36850992 0.38976312\n",
      " 0.38718602 0.36386034 0.3635818  0.4259726  0.35088277 0.3940256\n",
      " 0.4031572  0.373133   0.380428   0.37143385 0.37012437 0.38359216\n",
      " 0.41877243 0.37435612 0.39977613 0.48885602]\n",
      "[0.35205328 0.34612888 0.34700793 0.42182645 0.34341997 0.33887282\n",
      " 0.3413418  0.33931062 0.34690696 0.3539821  0.34966215 0.34318787\n",
      " 0.34839895 0.35739508 0.3459162  0.33093128 0.3534398  0.3397987\n",
      " 0.3427486  0.36154667 0.3460271  0.35130003 0.34450695 0.35165218\n",
      " 0.3490354  0.34401903 0.34463984 0.34135252 0.35306996 0.34247398\n",
      " 0.35228652 0.33914453 0.3639455  0.3470299  0.33312193 0.34117278\n",
      " 0.3423584  0.35764018 0.34262973 0.34776938 0.34023345 0.35136414\n",
      " 0.35483748 0.3383386  0.34304607 0.34444764 0.33705676 0.34060854\n",
      " 0.33644983 0.33938393 0.34571114 0.3609147  0.34744167 0.35278532\n",
      " 0.33642107 0.3535888  0.34293923 0.3450726  0.3530755  0.34269053\n",
      " 0.3700761  0.33284417 0.35431045 0.4039144 ]\n",
      "[0.28614038 0.2917639  0.2938843  0.32761028 0.28038    0.2850736\n",
      " 0.29328486 0.28793043 0.28455472 0.30029985 0.2936315  0.28988925\n",
      " 0.28305593 0.30006146 0.29500106 0.27512076 0.3004689  0.28410354\n",
      " 0.2851705  0.29761738 0.29125127 0.2882103  0.29102808 0.2947136\n",
      " 0.28783688 0.2871795  0.2935789  0.2766457  0.2835928  0.28869674\n",
      " 0.28987962 0.28718084 0.3035298  0.29145792 0.28557095 0.2800146\n",
      " 0.28892663 0.28324816 0.2932992  0.29012364 0.27524337 0.2962611\n",
      " 0.28336626 0.27698877 0.28103885 0.2920962  0.2811457  0.2813461\n",
      " 0.27821028 0.28093007 0.28301498 0.29406396 0.29402593 0.2951948\n",
      " 0.27611575 0.29171455 0.28214496 0.29055378 0.28436288 0.283963\n",
      " 0.30673486 0.27691153 0.28455475 0.31722555]\n",
      "[0.27217534 0.271124   0.2713712  0.28352526 0.2705554  0.26992485\n",
      " 0.27076787 0.2702618  0.27199888 0.27307862 0.27014813 0.27169392\n",
      " 0.27217337 0.27282405 0.27031562 0.26995704 0.27171716 0.26996437\n",
      " 0.27033046 0.2711449  0.27266076 0.27123082 0.27057996 0.27104142\n",
      " 0.2718105  0.27008536 0.27149007 0.2701331  0.27238733 0.27051234\n",
      " 0.2716813  0.27009472 0.27384615 0.27066004 0.27007842 0.27063864\n",
      " 0.27004543 0.2720527  0.2697292  0.27191126 0.27014044 0.27156997\n",
      " 0.2713422  0.2703468  0.27008244 0.27099636 0.27029875 0.27090576\n",
      " 0.27010906 0.2719706  0.27237898 0.27195558 0.27061388 0.27158037\n",
      " 0.27040428 0.27145314 0.27031484 0.27134758 0.27256638 0.27039596\n",
      " 0.2738694  0.2695963  0.27140722 0.27836916]\n",
      "[0.26972765 0.2693318  0.26954114 0.27246982 0.2692541  0.26910478\n",
      " 0.26936284 0.2692144  0.2696074  0.26994908 0.26912835 0.26951414\n",
      " 0.26981467 0.2697746  0.26916778 0.26910642 0.26953405 0.26910228\n",
      " 0.2692737  0.26929072 0.26990166 0.26941988 0.26922515 0.26943126\n",
      " 0.26957148 0.2691535  0.2695878  0.26912192 0.2699514  0.26925424\n",
      " 0.26959547 0.26912084 0.27016252 0.2692354  0.26912537 0.26941502\n",
      " 0.26910666 0.2698333  0.26905736 0.26974103 0.2691876  0.2697224\n",
      " 0.2693507  0.2692554  0.2691231  0.26936236 0.269195   0.26939502\n",
      " 0.26914915 0.26966518 0.26982564 0.2695815  0.2693033  0.26961493\n",
      " 0.26916653 0.26955613 0.26919982 0.26946384 0.26997373 0.26924536\n",
      " 0.27004144 0.2690374  0.2693134  0.27117932]\n",
      "[0.38990673 0.37668237 0.3913964  0.4857818  0.3863949  0.37736526\n",
      " 0.3473025  0.3856807  0.35738164 0.40645462 0.41414908 0.40107346\n",
      " 0.3650167  0.4225824  0.3874438  0.37155446 0.39143586 0.38099715\n",
      " 0.36927179 0.42088282 0.3864457  0.39732695 0.39057875 0.41258553\n",
      " 0.39450562 0.35611597 0.37574935 0.388465   0.34959912 0.3748214\n",
      " 0.38167927 0.43071124 0.39034867 0.36867958 0.37150905 0.3616166\n",
      " 0.36070707 0.37315515 0.38778064 0.37657124 0.35529864 0.38955605\n",
      " 0.37633222 0.39335924 0.4073108  0.38334128 0.36850992 0.38976312\n",
      " 0.38718602 0.36386034 0.3635818  0.4259726  0.35088277 0.3940256\n",
      " 0.4031572  0.373133   0.380428   0.37143385 0.37012437 0.38359216\n",
      " 0.41877243 0.37435612 0.39977613 0.48885602]\n",
      "[0.35205328 0.34612888 0.34700793 0.42182645 0.34341997 0.33887282\n",
      " 0.3413418  0.33931062 0.34690696 0.3539821  0.34966215 0.34318787\n",
      " 0.34839895 0.35739508 0.3459162  0.33093128 0.3534398  0.3397987\n",
      " 0.3427486  0.36154667 0.3460271  0.35130003 0.34450695 0.35165218\n",
      " 0.3490354  0.34401903 0.34463984 0.34135252 0.35306996 0.34247398\n",
      " 0.35228652 0.33914453 0.3639455  0.3470299  0.33312193 0.34117278\n",
      " 0.3423584  0.35764018 0.34262973 0.34776938 0.34023345 0.35136414\n",
      " 0.35483748 0.3383386  0.34304607 0.34444764 0.33705676 0.34060854\n",
      " 0.33644983 0.33938393 0.34571114 0.3609147  0.34744167 0.35278532\n",
      " 0.33642107 0.3535888  0.34293923 0.3450726  0.3530755  0.34269053\n",
      " 0.3700761  0.33284417 0.35431045 0.4039144 ]\n",
      "[0.28614038 0.2917639  0.2938843  0.32761028 0.28038    0.2850736\n",
      " 0.29328486 0.28793043 0.28455472 0.30029985 0.2936315  0.28988925\n",
      " 0.28305593 0.30006146 0.29500106 0.27512076 0.3004689  0.28410354\n",
      " 0.2851705  0.29761738 0.29125127 0.2882103  0.29102808 0.2947136\n",
      " 0.28783688 0.2871795  0.2935789  0.2766457  0.2835928  0.28869674\n",
      " 0.28987962 0.28718084 0.3035298  0.29145792 0.28557095 0.2800146\n",
      " 0.28892663 0.28324816 0.2932992  0.29012364 0.27524337 0.2962611\n",
      " 0.28336626 0.27698877 0.28103885 0.2920962  0.2811457  0.2813461\n",
      " 0.27821028 0.28093007 0.28301498 0.29406396 0.29402593 0.2951948\n",
      " 0.27611575 0.29171455 0.28214496 0.29055378 0.28436288 0.283963\n",
      " 0.30673486 0.27691153 0.28455475 0.31722555]\n",
      "[0.27217534 0.271124   0.2713712  0.28352526 0.2705554  0.26992485\n",
      " 0.27076787 0.2702618  0.27199888 0.27307862 0.27014813 0.27169392\n",
      " 0.27217337 0.27282405 0.27031562 0.26995704 0.27171716 0.26996437\n",
      " 0.27033046 0.2711449  0.27266076 0.27123082 0.27057996 0.27104142\n",
      " 0.2718105  0.27008536 0.27149007 0.2701331  0.27238733 0.27051234\n",
      " 0.2716813  0.27009472 0.27384615 0.27066004 0.27007842 0.27063864\n",
      " 0.27004543 0.2720527  0.2697292  0.27191126 0.27014044 0.27156997\n",
      " 0.2713422  0.2703468  0.27008244 0.27099636 0.27029875 0.27090576\n",
      " 0.27010906 0.2719706  0.27237898 0.27195558 0.27061388 0.27158037\n",
      " 0.27040428 0.27145314 0.27031484 0.27134758 0.27256638 0.27039596\n",
      " 0.2738694  0.2695963  0.27140722 0.27836916]\n",
      "[0.26972765 0.2693318  0.26954114 0.27246982 0.2692541  0.26910478\n",
      " 0.26936284 0.2692144  0.2696074  0.26994908 0.26912835 0.26951414\n",
      " 0.26981467 0.2697746  0.26916778 0.26910642 0.26953405 0.26910228\n",
      " 0.2692737  0.26929072 0.26990166 0.26941988 0.26922515 0.26943126\n",
      " 0.26957148 0.2691535  0.2695878  0.26912192 0.2699514  0.26925424\n",
      " 0.26959547 0.26912084 0.27016252 0.2692354  0.26912537 0.26941502\n",
      " 0.26910666 0.2698333  0.26905736 0.26974103 0.2691876  0.2697224\n",
      " 0.2693507  0.2692554  0.2691231  0.26936236 0.269195   0.26939502\n",
      " 0.26914915 0.26966518 0.26982564 0.2695815  0.2693033  0.26961493\n",
      " 0.26916653 0.26955613 0.26919982 0.26946384 0.26997373 0.26924536\n",
      " 0.27004144 0.2690374  0.2693134  0.27117932]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(np.argmax(predictions[0][0]))\n",
    "print(predictions[0][0])\n",
    "print(predictions[0][1])\n",
    "print(predictions[0][2])\n",
    "print(predictions[0][3])\n",
    "print(predictions[0][4])\n",
    "print(predictions[1][0])\n",
    "print(predictions[1][1])\n",
    "print(predictions[1][2])\n",
    "print(predictions[1][3])\n",
    "print(predictions[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
