{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train1 = \"Data/readelf/11/train_dataset1.npz\"\n",
    "train2 = \"Data/readelf/11/train_dataset2.npz\"\n",
    "train3 = \"Data/readelf/11/train_dataset3.npz\"\n",
    "train4 = \"Data/readelf/11/train_dataset4.npz\"\n",
    "train5 = \"Data/readelf/11/train_dataset5.npz\"\n",
    "train6 = \"Data/readelf/11/train_dataset6.npz\"\n",
    "train7 = \"Data/readelf/11/train_dataset7.npz\"\n",
    "train8 = \"Data/readelf/11/train_dataset8.npz\"\n",
    "train9 = \"Data/readelf/11/train_dataset9.npz\"\n",
    "train10 = \"Data/readelf/11/train_dataset10.npz\"\n",
    "val1 = \"Data/readelf/11/val_dataset1.npz\"\n",
    "val2 = \"Data/readelf/11/val_dataset2.npz\"\n",
    "test = \"Data/readelf/11/test_dataset.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset1 = np.load(train1)\n",
    "training_dataset2 = np.load(train2)\n",
    "training_dataset3 = np.load(train3)\n",
    "training_dataset4 = np.load(train4)\n",
    "training_dataset5 = np.load(train5)\n",
    "training_dataset6 = np.load(train6)\n",
    "training_dataset7 = np.load(train7)\n",
    "training_dataset8 = np.load(train8)\n",
    "training_dataset9 = np.load(train9)\n",
    "training_dataset10 = np.load(train10)\n",
    "\n",
    "val_dataset1 = np.load(val1)\n",
    "val_dataset2 = np.load(val2)\n",
    "\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = np.concatenate((training_dataset1['x'], training_dataset2['x'],\n",
    "                          training_dataset3['x'], training_dataset4['x'],\n",
    "                          training_dataset5['x'], training_dataset6['x'],\n",
    "                          training_dataset7['x'], training_dataset8['x'],\n",
    "                          training_dataset9['x'], training_dataset10['x']))\n",
    "y_train = np.concatenate((training_dataset1['y'], training_dataset2['y'],\n",
    "                          training_dataset3['y'], training_dataset4['y'],\n",
    "                          training_dataset5['y'], training_dataset6['y'],\n",
    "                          training_dataset7['y'], training_dataset8['y'],\n",
    "                          training_dataset9['y'], training_dataset10['y']))\n",
    "\n",
    "x_val = np.concatenate((val_dataset1['x'], val_dataset2['x']))\n",
    "y_val = np.concatenate((val_dataset1['y'], val_dataset2['y']))\n",
    "\n",
    "x_test = test_dataset['x']\n",
    "y_test = test_dataset['y']\n",
    "\n",
    "print(\"No. of seed files collected: 124\")\n",
    "print(\"Data collection of XY collected by skipping some files(0 to 5), Dataset split: 99/20/5\")\n",
    "samples, timesteps, chunksize = x_train.shape\n",
    "print(\"No. of samples: \" + str(samples) + \", No. of timesteps: \" + str(timesteps) + \", Chunksize: \" + str(chunksize))\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, y_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(2560, 64), return_sequences=True))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error')\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200, #change accordingly\n",
    "                    batch_size=25, #change accordingly\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"/home/isa/FYPJ/Model/model11.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict = test_dataset['x'][:1]\n",
    "predictions = model.predict(x_predict)\n",
    "print(predictions[0][0]) # 1\n",
    "print(predictions[0][1]) # 2 (1st row)\n",
    "print(predictions[0][2]) # 1\n",
    "print(predictions[0][3]) # 2 (2nd row)\n",
    "print(predictions[0][4]) # 1\n",
    "print(predictions[0][5]) # 2 (3rd row)\n",
    "print(predictions[0][6]) # 1\n",
    "print(predictions[0][7]) # 2 (4th row)\n",
    "print(predictions[0][8]) # 1\n",
    "print(predictions[0][9]) # 2 (5th row)\n",
    "print(predictions[0][10])# 1\n",
    "print(predictions[0][11])# 2 (6th row)\n",
    "print(predictions[0][12])# 1\n",
    "print(predictions[0][13])# 2 (7th row)\n",
    "print(predictions[0][14])# 1\n",
    "print(predictions[0][15])# 2 (8th row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
