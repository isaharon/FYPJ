{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of seed files collected: 68\n",
      "Data collection of XY simulated to 1% sampling rate, Dataset split: 60/4/4\n",
      "No. of samples: 1148, No. of timesteps: 3840, Chunksize: 64\n",
      "Training shape:  (1148, 3840, 64) (1148, 3840, 64)\n",
      "Validation shape:  (82, 3840, 64) (82, 3840, 64)\n",
      "Test shape:  (104, 3840, 64) (104, 3840, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train = \"Data/readelf/8/train_dataset8.npz\"\n",
    "val = \"Data/readelf/8/val_dataset8.npz\"\n",
    "test = \"Data/readelf/8/test_dataset8.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset = np.load(train)\n",
    "val_dataset = np.load(val)\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = training_dataset['x'][:1148]\n",
    "y_train = training_dataset['y'][:1148]\n",
    "\n",
    "x_val = val_dataset['x']\n",
    "y_val = val_dataset['y']\n",
    "\n",
    "x_test = test_dataset['x']\n",
    "y_test = test_dataset['y']\n",
    "\n",
    "print(\"No. of seed files collected: 68\")\n",
    "print(\"Data collection of XY simulated to 1% sampling rate, Dataset split: 60/4/4\")\n",
    "samples, timesteps, chunksize = x_train.shape\n",
    "print(\"No. of samples: \" + str(samples) + \", No. of timesteps: \" + str(timesteps) + \", Chunksize: \" + str(chunksize))\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, x_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 3840, 64)          33024     \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1148 samples, validate on 82 samples\n",
      "Epoch 1/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.0864 - val_loss: 0.0448\n",
      "Epoch 2/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0803 - val_loss: 0.0422\n",
      "Epoch 3/200\n",
      "1148/1148 [==============================] - 159s 138ms/step - loss: 0.0759 - val_loss: 0.0403\n",
      "Epoch 4/200\n",
      "1148/1148 [==============================] - 160s 139ms/step - loss: 0.0726 - val_loss: 0.0388\n",
      "Epoch 5/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.0699 - val_loss: 0.0375\n",
      "Epoch 6/200\n",
      "1148/1148 [==============================] - 159s 139ms/step - loss: 0.0678 - val_loss: 0.0365\n",
      "Epoch 7/200\n",
      "1148/1148 [==============================] - 159s 139ms/step - loss: 0.0659 - val_loss: 0.0356\n",
      "Epoch 8/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.0644 - val_loss: 0.0348\n",
      "Epoch 9/200\n",
      "1148/1148 [==============================] - 157s 136ms/step - loss: 0.0630 - val_loss: 0.0342\n",
      "Epoch 10/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0618 - val_loss: 0.0336\n",
      "Epoch 11/200\n",
      "1148/1148 [==============================] - 158s 137ms/step - loss: 0.0608 - val_loss: 0.0330\n",
      "Epoch 12/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0598 - val_loss: 0.0326\n",
      "Epoch 13/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0590 - val_loss: 0.0321\n",
      "Epoch 14/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0582 - val_loss: 0.0317\n",
      "Epoch 15/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0575 - val_loss: 0.0314\n",
      "Epoch 16/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0568 - val_loss: 0.0310\n",
      "Epoch 17/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0562 - val_loss: 0.0307\n",
      "Epoch 18/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0556 - val_loss: 0.0304\n",
      "Epoch 19/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0551 - val_loss: 0.0301\n",
      "Epoch 20/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0545 - val_loss: 0.0299\n",
      "Epoch 21/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0541 - val_loss: 0.0296\n",
      "Epoch 22/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0536 - val_loss: 0.0293\n",
      "Epoch 23/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0531 - val_loss: 0.0291\n",
      "Epoch 24/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0527 - val_loss: 0.0289\n",
      "Epoch 25/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0523 - val_loss: 0.0287\n",
      "Epoch 26/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0519 - val_loss: 0.0284\n",
      "Epoch 27/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0515 - val_loss: 0.0282\n",
      "Epoch 28/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0511 - val_loss: 0.0280\n",
      "Epoch 29/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0507 - val_loss: 0.0278\n",
      "Epoch 30/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0504 - val_loss: 0.0277\n",
      "Epoch 31/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0500 - val_loss: 0.0275\n",
      "Epoch 32/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0497 - val_loss: 0.0273\n",
      "Epoch 33/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0494 - val_loss: 0.0271\n",
      "Epoch 34/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0491 - val_loss: 0.0270\n",
      "Epoch 35/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0488 - val_loss: 0.0268\n",
      "Epoch 36/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0485 - val_loss: 0.0267\n",
      "Epoch 37/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0482 - val_loss: 0.0265\n",
      "Epoch 38/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0480 - val_loss: 0.0264\n",
      "Epoch 39/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0477 - val_loss: 0.0262\n",
      "Epoch 40/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0475 - val_loss: 0.0261\n",
      "Epoch 41/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0472 - val_loss: 0.0260\n",
      "Epoch 42/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0470 - val_loss: 0.0259\n",
      "Epoch 43/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0468 - val_loss: 0.0257\n",
      "Epoch 44/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0465 - val_loss: 0.0256\n",
      "Epoch 45/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0463 - val_loss: 0.0255\n",
      "Epoch 46/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0461 - val_loss: 0.0254\n",
      "Epoch 47/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0459 - val_loss: 0.0253\n",
      "Epoch 48/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0457 - val_loss: 0.0252\n",
      "Epoch 49/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0455 - val_loss: 0.0251\n",
      "Epoch 50/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0454 - val_loss: 0.0250\n",
      "Epoch 51/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0452 - val_loss: 0.0249\n",
      "Epoch 52/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0450 - val_loss: 0.0248\n",
      "Epoch 53/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0448 - val_loss: 0.0247\n",
      "Epoch 54/200\n",
      "1148/1148 [==============================] - 148s 129ms/step - loss: 0.0447 - val_loss: 0.0246\n",
      "Epoch 55/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0445 - val_loss: 0.0245\n",
      "Epoch 56/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0444 - val_loss: 0.0244\n",
      "Epoch 57/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0442 - val_loss: 0.0243\n",
      "Epoch 58/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0441 - val_loss: 0.0243\n",
      "Epoch 59/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.0439 - val_loss: 0.0242\n",
      "Epoch 60/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0438 - val_loss: 0.0241\n",
      "Epoch 61/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.0436 - val_loss: 0.0240\n",
      "Epoch 62/200\n",
      "1148/1148 [==============================] - 150s 131ms/step - loss: 0.0435 - val_loss: 0.0240\n",
      "Epoch 63/200\n",
      "1148/1148 [==============================] - 149s 129ms/step - loss: 0.0434 - val_loss: 0.0239\n",
      "Epoch 64/200\n",
      "1148/1148 [==============================] - 148s 129ms/step - loss: 0.0433 - val_loss: 0.0238\n",
      "Epoch 65/200\n",
      "1148/1148 [==============================] - 147s 128ms/step - loss: 0.0431 - val_loss: 0.0238\n",
      "Epoch 66/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0430 - val_loss: 0.0237\n",
      "Epoch 67/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0429 - val_loss: 0.0236\n",
      "Epoch 68/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0428 - val_loss: 0.0236\n",
      "Epoch 69/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0427 - val_loss: 0.0235\n",
      "Epoch 70/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0426 - val_loss: 0.0234\n",
      "Epoch 71/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0425 - val_loss: 0.0234\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.0424 - val_loss: 0.0233\n",
      "Epoch 73/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0423 - val_loss: 0.0233\n",
      "Epoch 74/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0422 - val_loss: 0.0232\n",
      "Epoch 75/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0421 - val_loss: 0.0232\n",
      "Epoch 76/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.0420 - val_loss: 0.0231\n",
      "Epoch 77/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.0419 - val_loss: 0.0231\n",
      "Epoch 78/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0418 - val_loss: 0.0230\n",
      "Epoch 79/200\n",
      "1148/1148 [==============================] - 151s 132ms/step - loss: 0.0417 - val_loss: 0.0230\n",
      "Epoch 80/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.0417 - val_loss: 0.0229\n",
      "Epoch 81/200\n",
      "1148/1148 [==============================] - 157s 137ms/step - loss: 0.0416 - val_loss: 0.0229\n",
      "Epoch 82/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0415 - val_loss: 0.0228\n",
      "Epoch 83/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0414 - val_loss: 0.0228\n",
      "Epoch 84/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0413 - val_loss: 0.0227\n",
      "Epoch 85/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0413 - val_loss: 0.0227\n",
      "Epoch 86/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0412 - val_loss: 0.0226\n",
      "Epoch 87/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0411 - val_loss: 0.0226\n",
      "Epoch 88/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0411 - val_loss: 0.0226\n",
      "Epoch 89/200\n",
      "1148/1148 [==============================] - 154s 135ms/step - loss: 0.0410 - val_loss: 0.0225\n",
      "Epoch 90/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0409 - val_loss: 0.0225\n",
      "Epoch 91/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0409 - val_loss: 0.0224\n",
      "Epoch 92/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0408 - val_loss: 0.0224\n",
      "Epoch 93/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0407 - val_loss: 0.0224\n",
      "Epoch 94/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0407 - val_loss: 0.0223\n",
      "Epoch 95/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0406 - val_loss: 0.0223\n",
      "Epoch 96/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0405 - val_loss: 0.0222\n",
      "Epoch 97/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0405 - val_loss: 0.0222\n",
      "Epoch 98/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0404 - val_loss: 0.0222\n",
      "Epoch 99/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0404 - val_loss: 0.0221\n",
      "Epoch 100/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0403 - val_loss: 0.0221\n",
      "Epoch 101/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0403 - val_loss: 0.0221\n",
      "Epoch 102/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0402 - val_loss: 0.0220\n",
      "Epoch 103/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0402 - val_loss: 0.0220\n",
      "Epoch 104/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0401 - val_loss: 0.0220\n",
      "Epoch 105/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0401 - val_loss: 0.0220\n",
      "Epoch 106/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0400 - val_loss: 0.0219\n",
      "Epoch 107/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0400 - val_loss: 0.0219\n",
      "Epoch 108/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0399 - val_loss: 0.0219\n",
      "Epoch 109/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0399 - val_loss: 0.0218\n",
      "Epoch 110/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0398 - val_loss: 0.0218\n",
      "Epoch 111/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0398 - val_loss: 0.0218\n",
      "Epoch 112/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0397 - val_loss: 0.0217\n",
      "Epoch 113/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0397 - val_loss: 0.0217\n",
      "Epoch 114/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0397 - val_loss: 0.0217\n",
      "Epoch 115/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0396 - val_loss: 0.0217\n",
      "Epoch 116/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.0396 - val_loss: 0.0216\n",
      "Epoch 117/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0395 - val_loss: 0.0216\n",
      "Epoch 118/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0395 - val_loss: 0.0216\n",
      "Epoch 119/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0395 - val_loss: 0.0216\n",
      "Epoch 120/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0394 - val_loss: 0.0215\n",
      "Epoch 121/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0394 - val_loss: 0.0215\n",
      "Epoch 122/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0393 - val_loss: 0.0215\n",
      "Epoch 123/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0393 - val_loss: 0.0215\n",
      "Epoch 124/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0393 - val_loss: 0.0215\n",
      "Epoch 125/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0392 - val_loss: 0.0214\n",
      "Epoch 126/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0392 - val_loss: 0.0214\n",
      "Epoch 127/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0392 - val_loss: 0.0214\n",
      "Epoch 128/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0391 - val_loss: 0.0214\n",
      "Epoch 129/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0391 - val_loss: 0.0213\n",
      "Epoch 130/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0391 - val_loss: 0.0213\n",
      "Epoch 131/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0390 - val_loss: 0.0213\n",
      "Epoch 132/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0390 - val_loss: 0.0213\n",
      "Epoch 133/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0390 - val_loss: 0.0213\n",
      "Epoch 134/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0390 - val_loss: 0.0212\n",
      "Epoch 135/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0389 - val_loss: 0.0212\n",
      "Epoch 136/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0389 - val_loss: 0.0212\n",
      "Epoch 137/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0389 - val_loss: 0.0212\n",
      "Epoch 138/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0388 - val_loss: 0.0212\n",
      "Epoch 139/200\n",
      "1148/1148 [==============================] - 154s 135ms/step - loss: 0.0388 - val_loss: 0.0211\n",
      "Epoch 140/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0388 - val_loss: 0.0211\n",
      "Epoch 141/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0388 - val_loss: 0.0211\n",
      "Epoch 142/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0387 - val_loss: 0.0211\n",
      "Epoch 143/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0387 - val_loss: 0.0211\n",
      "Epoch 144/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0387 - val_loss: 0.0211\n",
      "Epoch 145/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0387 - val_loss: 0.0210\n",
      "Epoch 146/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0386 - val_loss: 0.0210\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0386 - val_loss: 0.0210\n",
      "Epoch 148/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0386 - val_loss: 0.0210\n",
      "Epoch 149/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0386 - val_loss: 0.0210\n",
      "Epoch 150/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0385 - val_loss: 0.0210\n",
      "Epoch 151/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0385 - val_loss: 0.0210\n",
      "Epoch 152/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0385 - val_loss: 0.0209\n",
      "Epoch 153/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0385 - val_loss: 0.0209\n",
      "Epoch 154/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0385 - val_loss: 0.0209\n",
      "Epoch 155/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0384 - val_loss: 0.0209\n",
      "Epoch 156/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0384 - val_loss: 0.0209\n",
      "Epoch 157/200\n",
      "1148/1148 [==============================] - 156s 136ms/step - loss: 0.0384 - val_loss: 0.0209\n",
      "Epoch 158/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0384 - val_loss: 0.0209\n",
      "Epoch 159/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0384 - val_loss: 0.0208\n",
      "Epoch 160/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0383 - val_loss: 0.0208\n",
      "Epoch 161/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0383 - val_loss: 0.0208\n",
      "Epoch 162/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0383 - val_loss: 0.0208\n",
      "Epoch 163/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0383 - val_loss: 0.0208\n",
      "Epoch 164/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0383 - val_loss: 0.0208\n",
      "Epoch 165/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0382 - val_loss: 0.0208\n",
      "Epoch 166/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0382 - val_loss: 0.0208\n",
      "Epoch 167/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0382 - val_loss: 0.0207\n",
      "Epoch 168/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0382 - val_loss: 0.0207\n",
      "Epoch 169/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0382 - val_loss: 0.0207\n",
      "Epoch 170/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0382 - val_loss: 0.0207\n",
      "Epoch 171/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0382 - val_loss: 0.0207\n",
      "Epoch 172/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0381 - val_loss: 0.0207\n",
      "Epoch 173/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0381 - val_loss: 0.0207\n",
      "Epoch 174/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0381 - val_loss: 0.0207\n",
      "Epoch 175/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0381 - val_loss: 0.0207\n",
      "Epoch 176/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0381 - val_loss: 0.0207\n",
      "Epoch 177/200\n",
      "1148/1148 [==============================] - 154s 135ms/step - loss: 0.0381 - val_loss: 0.0206\n",
      "Epoch 178/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0380 - val_loss: 0.0206\n",
      "Epoch 179/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0380 - val_loss: 0.0206\n",
      "Epoch 180/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0380 - val_loss: 0.0206\n",
      "Epoch 181/200\n",
      "1148/1148 [==============================] - 158s 138ms/step - loss: 0.0380 - val_loss: 0.0206\n",
      "Epoch 182/200\n",
      "1148/1148 [==============================] - 154s 135ms/step - loss: 0.0380 - val_loss: 0.0206\n",
      "Epoch 183/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0380 - val_loss: 0.0206\n",
      "Epoch 184/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0380 - val_loss: 0.0206\n",
      "Epoch 185/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0380 - val_loss: 0.0206\n",
      "Epoch 186/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0379 - val_loss: 0.0206\n",
      "Epoch 187/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0379 - val_loss: 0.0206\n",
      "Epoch 188/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0379 - val_loss: 0.0206\n",
      "Epoch 189/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0379 - val_loss: 0.0206\n",
      "Epoch 190/200\n",
      "1148/1148 [==============================] - 152s 133ms/step - loss: 0.0379 - val_loss: 0.0205\n",
      "Epoch 191/200\n",
      "1148/1148 [==============================] - 152s 132ms/step - loss: 0.0379 - val_loss: 0.0205\n",
      "Epoch 192/200\n",
      "1148/1148 [==============================] - 151s 131ms/step - loss: 0.0379 - val_loss: 0.0205\n",
      "Epoch 193/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0379 - val_loss: 0.0205\n",
      "Epoch 194/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0379 - val_loss: 0.0205\n",
      "Epoch 195/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0379 - val_loss: 0.0205\n",
      "Epoch 196/200\n",
      "1148/1148 [==============================] - 153s 134ms/step - loss: 0.0378 - val_loss: 0.0205\n",
      "Epoch 197/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0378 - val_loss: 0.0205\n",
      "Epoch 198/200\n",
      "1148/1148 [==============================] - 155s 135ms/step - loss: 0.0378 - val_loss: 0.0205\n",
      "Epoch 199/200\n",
      "1148/1148 [==============================] - 154s 134ms/step - loss: 0.0378 - val_loss: 0.0205\n",
      "Epoch 200/200\n",
      "1148/1148 [==============================] - 153s 133ms/step - loss: 0.0378 - val_loss: 0.0205\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(3840, 64), return_sequences=True))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error')\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=41,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8b7f8108d0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8XXWd//HX596bfWubpFu6pCvQ\nFihtKKCsslhwpIooRR2YkQHRwWX4zYzw8/dTfsz8fiOjI8rIqCgqKLKIC1XAKoOArJIu0I22adrS\ntOmWptm3m3x/f5yT9OY2aW7bJCe59/18PM7jnPM935v7ycnN53vu9yxfc84hIiKpIRR0ACIiMnyU\n9EVEUoiSvohIClHSFxFJIUr6IiIpRElfRCSFKOmLiKQQJX0RkRSipC8ikkIiQQcQr6ioyJWWlgYd\nhojIqLJq1aqDzrnigeqNuKRfWlpKeXl50GGIiIwqZrYzkXrq3hERSSFK+iIiKURJX0QkhSjpi4ik\nECV9EZEUoqQvIpJClPRFRFJI0iT9PYdb+OYfNrP9YFPQoYiIjFhJk/QPNbVz3/MVbN5bH3QoIiIj\nVtIk/cLcdABqmtoDjkREZORKmqQ/LsdL+ocalfRFRPqTNEk/IxImLyOiI30RkWNImqQPMC43nUNK\n+iIi/UqupJ+TTk1TW9BhiIiMWEmV9AtzMqhRn76ISL+SLOmre0dE5FiSKul39+k754IORURkREqq\npF+Yk060y1HfEg06FBGRESm5kn7PDVo6mSsi0pekSvrjcjIA1K8vItKPpEr6hTl6FIOIyLEkV9Lv\n7t7RZZsiIn1KqqTf8/wd9emLiPQpqZJ+RiRMrp6/IyLSr4SSvpktNbPNZlZhZnf0sT3DzB73t79h\nZqV+eZqZPWRm68xsk5ndObjhH22cbtASEenXgEnfzMLA/cCVwDzgejObF1ftJqDWOTcbuBe4xy//\nKJDhnDsdWAx8urtBGCqFuenq0xcR6UciR/pLgArnXKVzrh14DFgWV2cZ8JC//CRwqZkZ4IAcM4sA\nWUA7MKRDWxXmpHOwUX36IiJ9SSTplwC7Ytar/LI+6zjnokAdUIjXADQB1cC7wDecc4fi38DMbjGz\ncjMrP3DgwHH/ErEmFmSyt771pH6GiEiySiTpWx9l8Q+36a/OEqATmAzMAP6Hmc08qqJzDzjnypxz\nZcXFxQmE1L/JY7I43NxBc7sexSAiEi+RpF8FTI1ZnwLs6a+O35VTABwCPg783jnX4ZzbD7wClJ1s\n0MdSMiYLgD2HdbQvIhIvkaT/JjDHzGaYWTqwHFgRV2cFcKO/fC3wvPMedfku8D7z5ADnAu8MTuh9\nm9yT9FuG8m1EREalAZO+30d/G7AS2AQ84ZzbYGZ3m9nVfrUHgUIzqwBuB7ov67wfyAXW4zUeP3bO\nvT3Iv0MvkwoyASV9EZG+RBKp5Jx7BngmruwrMcuteJdnxr+usa/yoTQhP5OQKemLiPQlqe7IBUgL\nh5iQn8lu9emLiBwl6ZI+eP361XU60hcRiZe0SV/dOyIiR0vSpJ/JnrpWuro0Vq6ISKykTPolY7Jo\nj3bpaZsiInGSMulPLtC1+iIifUnOpO/foLVbSV9EpJekTPrTCrMB2H6wKeBIRERGlqRM+rkZESbm\nZ1J5QElfRCRWUiZ9gJnFOWw70Bh0GCIiI0pSJ/3KA414z30TERFI4qQ/qziX+tYoBzV0oohIj6RN\n+jOLcwGoVBePiEiP5E36RTkAVOoKHhGRHkmb9EvGZJERCbFtv470RUS6JW3SD4WMGUU5OtIXEYmR\ntEkfYNb4XCp0pC8i0iOpk/6pE/J491Az9a0dQYciIjIiJHXSX1BSAMCmPfUBRyIiMjIkddKfPzkf\ngA1K+iIiQJIn/fH5mRTnZbB+T13QoYiIjAgJJX0zW2pmm82swszu6GN7hpk97m9/w8xK/fJPmNna\nmKnLzBYO7q9wbPMn57NRR/oiIkACSd/MwsD9wJXAPOB6M5sXV+0moNY5Nxu4F7gHwDn3iHNuoXNu\nIfDXwA7n3NrB/AUGsmByAVv3N9La0TmcbysiMiIlcqS/BKhwzlU659qBx4BlcXWWAQ/5y08Cl5qZ\nxdW5Hnj0ZII9EfMn59PZ5di8t2G431pEZMRJJOmXALti1qv8sj7rOOeiQB1QGFfnOvpJ+mZ2i5mV\nm1n5gQMHEok7Yd1X8Ly9W/36IiKJJP34I3aA+OcVH7OOmZ0DNDvn1vf1Bs65B5xzZc65suLi4gRC\nStyUsVkU5WawemftoP5cEZHRKJGkXwVMjVmfAuzpr46ZRYAC4FDM9uUE0LXjx8PZpWN5c8ehgSuL\niCS5RJL+m8AcM5thZul4CXxFXJ0VwI3+8rXA884fvcTMQsBH8c4FBKKsdBxVtS1U12mgdBFJbQMm\nfb+P/jZgJbAJeMI5t8HM7jazq/1qDwKFZlYB3A7EXtZ5IVDlnKsc3NATt6R0HABv7lAXj4iktkgi\nlZxzzwDPxJV9JWa5Fe9ovq/XvgCce+IhnrzTJuWRnR6mfMchrj5zcpChiIgEKqnvyO0WCYdYNG0s\nf9mufn0RSW0pkfQBzpkxjs37GqhpbAs6FBGRwKRM0r9gbjHOwcsVB4MORUQkMCmT9E8vKWBsdhov\nbhncm79EREaTlEn64ZBx/pxiXtpykK6u+HvLRERSQ8okfYAL5xRxsLGNTXv11E0RSU0plfQvmus9\n4uGFzeriEZHUlFJJf3x+JgunjuH36/cGHYqISCBSKukDXLlgIut217HrUHPQoYiIDLsUTPqTAHS0\nLyIpKeWS/rTCbOZPzufZ9dVBhyIiMuxSLukDXHX6JFa/e5h3a9TFIyKpJSWT/ofPKsEMfrm6KuhQ\nRESGVUom/cljsjh/dhG/XF2lG7VEJKWkZNIHuHbxFKpqW3h9e03QoYiIDJuUTfpXzJtIXmaER/+y\na+DKIiJJImWTflZ6mI8unsqz66rZV98adDgiIsMiZZM+wA3nTafTOR55492gQxERGRYpnfRLi3K4\n5JTx/PyNnbR2dAYdjojIkEvppA/wd+fP4GBjO0+u0uWbIpL8Uj7pnzerkIVTx/DdF7bR0dkVdDgi\nIkMqoaRvZkvNbLOZVZjZHX1szzCzx/3tb5hZacy2M8zsNTPbYGbrzCxz8MI/eWbGbZfMZvfhFn6z\nZnfQ4YiIDKkBk76ZhYH7gSuBecD1ZjYvrtpNQK1zbjZwL3CP/9oI8DPgVufcfOBioGPQoh8kl542\nnvmT87nv+a20R3W0LyLJK5Ej/SVAhXOu0jnXDjwGLIurswx4yF9+ErjUzAy4AnjbOfcWgHOuxjk3\n4s6Ymhn/vPRUdh1q4edv7Aw6HBGRIZNI0i8BYu9gqvLL+qzjnIsCdUAhMBdwZrbSzFab2T/39QZm\ndouZlZtZ+YEDwYxqdeGcIt4zq5D7nq+gvnXEfRkRERkUiSR966Ms/oE1/dWJAOcDn/DnHzazS4+q\n6NwDzrky51xZcXFxAiENPjPjzitPo7a5nW8/tzWQGEREhloiSb8KmBqzPgXY018dvx+/ADjkl7/o\nnDvonGsGngEWnWzQQ+X0KQUsP3saP3l1B1v2NQQdjojIoEsk6b8JzDGzGWaWDiwHVsTVWQHc6C9f\nCzzvnHPASuAMM8v2G4OLgI2DE/rQ+Of3n0JeZoQ7f7WOTj2BU0SSzIBJ3++jvw0vgW8CnnDObTCz\nu83sar/ag0ChmVUAtwN3+K+tBb6J13CsBVY7554e/F9j8IzNSeeuD85n1c5afvDnyqDDEREZVOYd\nkI8cZWVlrry8PNAYnHN89pHV/Pem/az43Hs5dWJ+oPGIiAzEzFY558oGqpfyd+T2xcz41w8tID8r\nwu2Pv6Vr90UkaSjp96MwN4N/u+YMNlbX863ntgQdjojIoFDSP4bL501g+dlT+a8XtrFyw96gwxER\nOWlK+gO46+r5nDmlgNsfX0vFfl3GKSKjm5L+ADLTwnzvrxeTlR7mlodX6W5dERnVlPQTMKkgi/s/\nvoh3DzXz+UfX6BHMIjJqKekn6JyZhdy9bAEvbD7AHb9cx0i71FVEJBGRoAMYTT5+zjT2N7Tyree2\nMj4/gy8tPTXokEREjouS/nH6wqVz2N/Qxndf2EZhTjp/d8HMoEMSEUmYkv5xMjP+ZdkCapva+den\nN5EeCXHDeaVBhyUikhD16Z+AcMj49vKzuHzeBL7y1AYefm1H0CGJiCRESf8EpUdC3P/xRT2J/yev\nbA86JBGRASnpn4TuxH/FvAnc9duNfOu5LbqqR0RGNCX9k5QeCXH/JxbxkUVT+NZzW/nKUxv0HH4R\nGbF0IncQpIVDfOOjZ1CUm873X6qkpqmNb35sIZlp4aBDExHpRUl/kJgZd151GkW5Gfy/Zzex+/Dr\n/OCGxYzPyww6NBGRHureGWQ3XziT735iMVv2NvCh77zCxj31QYckItJDSX8ILF0wkV/ceh5dDj7y\n3Vd5au3uoEMSEQGU9IfMgpICVtz2Xk4vKeALj63lf/1mHW3RzqDDEpEUp6Q/hMbnZ/Lzm8/h0xfN\n5Gevv8u1332NXYeagw5LRFKYkv4Qi4RD3HnlafzghjJ21DTxgfv+zO/XaxQuEQlGQknfzJaa2WYz\nqzCzO/rYnmFmj/vb3zCzUr+81MxazGytP31vcMMfPS6fN4GnP3cB0wtzuPVnq7j9ibXUtWhAFhEZ\nXgMmfTMLA/cDVwLzgOvNbF5ctZuAWufcbOBe4J6Ybduccwv96dZBintUmlaYza8++x4+f+kcnlq7\nh6XfeomXtx4MOiwRSSGJHOkvASqcc5XOuXbgMWBZXJ1lwEP+8pPApWZmgxdm8kgLh7j98rn88jPv\nISs9zCcffIOvPrWeprZo0KGJSApIJOmXALti1qv8sj7rOOeiQB1Q6G+bYWZrzOxFM7ugrzcws1vM\nrNzMyg8cOHBcv8BotXDqGJ75/AX87XtLeei1nVz2zRdZuWGvnt0jIkMqkaTf1xF7fGbqr041MM05\ndxZwO/BzM8s/qqJzDzjnypxzZcXFxQmElBwy08J89YPz+eVn3kNBVhqf/ukqbn64nKpaXeEjIkMj\nkaRfBUyNWZ8C7OmvjplFgALgkHOuzTlXA+CcWwVsA+aebNDJZvH0sfz2c+fz5atO49VtNVz2zRe5\n/08VtHboun4RGVyJJP03gTlmNsPM0oHlwIq4OiuAG/3la4HnnXPOzIr9E8GY2UxgDlA5OKEnl7Rw\niJsvnMkfb7+IC+cU8/WVm7n0P17kN2t206WndorIIBkw6ft99LcBK4FNwBPOuQ1mdreZXe1XexAo\nNLMKvG6c7ss6LwTeNrO38E7w3uqcOzTYv0QyKRmTxQM3lPHzm89hTHYaX3x8LR/+r1f4y3btNhE5\neTbSThyWlZW58vLyoMMYEbq6HL9es5uvr9zM3vpWLjmlmH+4fC5nTBkTdGgiMsKY2SrnXNmA9ZT0\nR76W9k5+/Op2HnipksPNHVx22ni+eNlcFpQUBB2aiIwQSvpJqKG1g4de3cEDL1VS3xrl8nkTuPWi\nWSyePjbo0EQkYEr6Say+tYMfv7yDH72ynbqWDs4uHcunL5zF+04dTyike+JEUpGSfgpoaovyRPku\nfvjn7ew+3MLs8bl86r0zWLZwMjkZGhRNJJUo6aeQjs4unllXzfdfrGRjdT15GRGuWVTCJ8+dzpwJ\neUGHJyLDQEk/BTnnWP3uYX72+k6efrua9s4uzp05jk+eO50r5k0kPaInaYskKyX9FFfT2MYT5VU8\n8sZOqmpbGJeTztVnTuaaRSWcXlKAnocnklyU9AWAzi7HS1sO8OSqKv64aR/t0S5mFedwzaIpfPis\nEiaPyQo6RBEZBEr6cpS6lg6eWVfNr1ZX8eaOWsxgSek4rjp9EksXTGRCfmbQIYrICUq9pL9nLfz6\nVrj6P2Hq2YMfWJLZWdPEr9fs5um3q9m6vxHwHvx25YKJLF0wkSljswOOUESOR+ol/boquHc+XPUN\nWHLz4AeWxCr2N/Dsur08u34vG6vrATi9pIBLTh3PJacUc+aUMbr+X2SES72k7xx8fRacciUsu3/w\nA0sROw428ez6vTy3aR9r3q2ly0FhTjoXzS3mklPHc+GcYgqy04IOU0TiJJr0k+cOHjOYdCZUvxV0\nJKNaaVEOn7l4Fp+5eBa1Te28tPUAf3pnP89v3s+v1uwmZLCgpIDzZhVy3sxCzi4dpxvBREaR5Ppv\nnXQmvPqfEG2DSEbQ0Yx6Y3PSWbawhGULS+jscqzddZgXtxzg9W01/Ojl7Xz/xUoiIWPh1DG8Z1Yh\n580q4qxpY8hMCwcduoj0I/mSflcU9m+CyQuDjiaphEPG4uljvYe7XQ7N7VHKd9TyWmUNr26r4Tt/\nquC+5ytICxvzJxewaNrYnvoTC3RVkMhIkXxJH7wuHiX9IZWdHuHCucVcONcb07i+tYO/VB6ifGct\nq3fW8sgbO/nRK9sBmFyQyaLpY1k0bSxnTi3gtEn5ZKcn10dPZLRIrv+8sTMgo0D9+gHIz0zjsnkT\nuGzeBADao11sqq5n1c5aVr/rNQS/e7sagJDBrOJcFpQUsKCkgNNLCpg3OZ9cnRsQGXLJ9V9mBhNP\nhz1rgo4k5aVHQpw5dQxnTh3Dp5gBwL76VtZV1bFudx3rd9fxSsVBfr1mN+D96UoLczhlQh5zJ+Zx\n6sQ85k7Io7Qwm0hYzwwSGSzJlfQBpp0LL98LrfWQmR90NBJjQn4mE+Zl9nwbANhf38r6PXWsq6pn\nY3Udm/c1sHLjXrqvJE6PhJhdnMspE/M4ZWIecyfkMrMolyljs9QYiJyA5Ev6My+CP38Ddr7iXbMv\nI9r4/Ezel5/J+0490hC0dnRSsb+RzXsb2Lyvgc17G3i9sqbnWwFAJGRMK8xmZlEOM4pymFGUy4yi\nHGYW5zA+L0MPlBPpR/Il/SlLIJIFlS8q6Y9SmWnhnv7+WHUtHVTsb6DyQBPbDzb1zF/aepD2aFdP\nvZz0MDOKc5hemMOUsVlMHZvN1HHZTB2bxeQxWbqkVFJaQknfzJYC3wbCwA+dc1+L254BPAwsBmqA\n65xzO2K2TwM2Anc5574xOKH3Iy3T6+KpfGFI30aGX0FWGounj2Px9HG9yru6HHvqWth+8EhjUHmw\niQ276/jDhr10dPa+63xCfkZPQ9DdKEwek8XEgkwmFWTqZjNJagN+us0sDNwPXA5UAW+a2Qrn3MaY\najcBtc652Wa2HLgHuC5m+73As4MX9gBmXgzPfRUa9kHehIFqyygXChlTxmYzZWw2F8wp7rWts8ux\nv6GVXYda2HWomaraFnbVNrPrUDN/2X6Ip9a20BX3JJK8zAiTCjKZWJDF5ILMnsZgYkGWP88kLyOi\nLiQZlRI5pFkCVDjnKgHM7DFgGd6Re7dlwF3+8pPAd8zMnHPOzD4EVAJNgxb1QGZe7M23/Tcs/Piw\nva2MPOGQMakgi0kFWSyZMe6o7R2dXVQfbqW6roW99a3sOdzK3roWquta2Vvfyqbqeg40tB31uqy0\nMMV5Gd6Um3FkOWa9KC+Dotx0MiLqTpKRI5GkXwLsilmvAs7pr45zLmpmdUChmbUAX8L7lvCP/b2B\nmd0C3AIwbdq0hIPv16QzoWAqbHxKSV+OKS0cYlphNtMK+3+UdHu0i331XiNQXddK9eEW9je0ccCf\nKg408vr2Gg43d/T5+oKstJ7GoDA3nXE56YzN9uc56RT2Wk9TIyFDKpGk39d32PhHc/ZX5/8A9zrn\nGo/1Vdg59wDwAHhP2UwgpmMzg3nL4I3vQ8thyBpz0j9SUld6JOSdCB537DEG2qKd1DS29zQGBxqP\nNAwHG9vY39DGxj311DS1U9fSdwMBkJsRYWxOGuOyvUZhnN8gjMlOoyArjfys3vPuKU2XsEoCEkn6\nVcDUmPUpwJ5+6lSZWQQoAA7hfSO41sz+HRgDdJlZq3PuOycd+UDmXwOvfQc2P6OjfRkWGZEwk8dk\nJTQEZbSzi8MtHdQ2tXOoe2pu99c7qG32ymoa29m6r5Ha5naa2zuP+TOz0sK9GoH8rDTysyK9yvIy\n08jNiHhTpjfPy4yQkxEhOy2scRNSQCJJ/01gjpnNAHYDy4H4LLoCuBF4DbgWeN55D+q/oLuCmd0F\nNA5LwgcoWQQF02D9r5T0ZcSJhEMU5WZQlJv402Dbop3UtXRQ39JBXUvUnx+Z4teraptpqI5S19JB\nY1t0wJ9vBrnpXmOQk3GkQcjN6L0eu5yVFiY7PUJWephsf/KWvW1hNSIjzoBJ3++jvw1YiXfJ5o+c\ncxvM7G6g3Dm3AngQ+KmZVeAd4S8fyqATYganfwReuQ/q90D+5KAjEjkpGZEw4/PCjM87/qeWRju7\nqG+N0tgapaGtg8bWKE3tURpaozS2RWlq694WPWrbvvrWnm1NbdGjrnY6dswhrxFIi2kMYhuItAhZ\n6aGeRqK70ciMhMlIC5Hhz4+sh8hMC5MR8bZl+nXSwqarqRKUPCNn9eXQdrjvLLjoS3DJnYPzM0VS\nmHOOlo7Onkagpb2T5vZOmttjljs6aWmP0tzeSUt7Jy0dnT3Lzd3lfZS1xdxgd7xCxjEbiNiGIiMt\nRHo4RFr3FLHe62EjPRK33lM3bj0cIj1iMXX9n+2XRULD1xil3shZfRk3A2ZfCqsfggv/EcIa5k/k\nZJgZ2ekRstMjjB/kn93Z5TUoLe2dtEW9RqC1w5u3dXTRGu2kraPL2+bPW7vX+6nbPW9qi1LTeOQ1\nHZ3dk6PdXx6q49+0sBEJeQ1AJGyEQ17DEQ55DUM4ZD3bLppbzD+9/9ShCcSX3EkfoOwmeOx6eOd3\nMP/DQUcjIv0Ih6znJPNwc87R2eV6NQIdnV10ROPW/Yaie7k96npta+90dER7r7dHu+js6iLadeQ9\nOru6iHY6ol2OaMxybsbQH5gmf9Kf+34YNwv+/B8w70NeX7+ISAwz70g7EoYskvs+ieS/sDcU9rp2\n9q6DLb8POhoRkUAlf9IHOP2jMLYUXvgadJ34ySIRkdEuNZJ+OA0uugOq18K6XwQdjYhIYFIj6QOc\ncR1MXuQ9fbOtMehoREQCkTpJPxSCK++Bhmp44d+CjkZEJBCpk/QBpi6BxX8Lr90Pu/4SdDQiIsMu\ntZI+wBX/4j12+TefUTePiKSc1Ev6GXnw4e/CoUr47RcYstvwRERGoNRL+gCl58MlX4b1T3rP3BcR\nSRGpmfQBzr8dTvkArLwTNg/f8L0iIkFK3aQfCsFHfuANrfjkp2Dna0FHJCIy5FI36QOk58DHn4D8\nEnjko7DrzaAjEhEZUqmd9AFyx8ONKyCnCB6+GrasDDoiEZEho6QP3qhaN/0BiubCo9fD6oeDjkhE\nZEgo6XfLHQ9/8zTMugRWfA6euws6Bx5XVERkNFHSj5WRC9c/Bov/Bl6+Fx76INTtDjoqEZFBo6Qf\nL5wGH/w2XPMDqH4Lvne+LukUkaShpN+fMz4Gn37Ju7Ln0eXw5E3QdDDoqERETkpCSd/MlprZZjOr\nMLM7+tieYWaP+9vfMLNSv3yJma31p7fMbHQNUls0G25+Hi7+n7DxKfjO2bD2UQ3EIiKj1oBJ38zC\nwP3AlcA84HozmxdX7Sag1jk3G7gXuMcvXw+UOecWAkuB75vZ6BqXN5IOF38Jbn0ZCmfDb26FBy+H\nqvKgIxMROW6JHOkvASqcc5XOuXbgMWBZXJ1lwEP+8pPApWZmzrlm51z3JTCZwOh9utn4U+FTK+FD\n34O6XfDDS+GXN0PNtqAjExFJWCJJvwTYFbNe5Zf1WcdP8nVAIYCZnWNmG4B1wK0xjUAPM7vFzMrN\nrPzAgQPH/1sMl1AIFl4Pn1vlPbtn02+9Lp+n/h5qdwYdnYjIgBJJ+tZHWfwRe791nHNvOOfmA2cD\nd5pZ5lEVnXvAOVfmnCsrLi5OIKSAZeTBZV+FL7wFS26Bt38B/7kYfvcPcPjdoKMTEelXIkm/Cpga\nsz4F2NNfHb/PvgA4FFvBObcJaAIWnGiwI07eBLjya/D5NbDoBlj9U/j2Qu8BbnvWBB2diMhREkn6\nbwJzzGyGmaUDy4EVcXVWADf6y9cCzzvnnP+aCICZTQdOAXYMSuQjSUEJ/NU34Qtr4bzPwpY/wAMX\nw0/+Cjb/Hro6g45QRARIIOn7ffC3ASuBTcATzrkNZna3mV3tV3sQKDSzCuB2oPuyzvOBt8xsLfBr\n4LPOueS92L1gClzxr3D7Bm9+qBIevQ6+fSa89HVo2Bt0hCKS4syNsOECy8rKXHl5klwO2dkB7zwN\n5T+C7S9CKAKnXAVn/bX3jJ9wWtARikiSMLNVzrmygeqNrmvmR5twGsz/kDfVbINVP4Y1j8CmFZBd\nBAuugdM/BlPKwPo6Fy4iMrh0pD/com1Q8Ry8/QRs+T1EW2HsDO+xD/OvgeJT1ACIyHFL9EhfST9I\nrfXetf5vPw7bXwKcd9fvqR/wxu+dcrZ3b4CIyACU9Eeb+mrY/LR3DmD7S9AVhZzxcMqV3lR6gffo\nZxGRPijpj2Yth70uoHd+B1v/CO2NEEqDqefA7PfBrPfBxDP1LUBEeijpJ4toG7z7Gmx73pv2rvPK\nswthxkVQ+l6Yfr7OBYikOCX9ZNWwDypf8BqA7S9CQ7VXnl0I086D0vO9+YT5uiRUJIXoks1klTcB\nzrzOm5yD2u2w81V/esXrEgKIZMGkM73LQUsWQUkZjJmmbwMiKU5JfzQzg3EzvemsT3pldVXw7uuw\nezXsLoc3fwivtXrbcoqhZDFMPgsmLPC+DYyZrnMDIilEST/ZFEyB06/1JvDuCt63wWsAdq/2Bn/Z\nspKeB6Wm53nJf8J8mLgAJpwOE+ZBek5gv4KIDB0l/WQXToPJC73pbL+svRn2b4J962Dveti3Htb9\nAsof9CsYjC2ForlQNMef+1NOYUC/iIgMBiX9VJSeDVMWe1M357yxAPat9xqC/RuhpsI7adzZdqRe\nduGRxqBwtnc38dhSb8rMH+ZfRESOl5K+eMxg7HRvOvUDR8q7Or3hIQ9uhYNb/GkrbH4WmuJGOcsa\nd6QBiJ/ySyCsj5tI0PRfKMcWCh9J3HMu772t5TDU7vCmwzuPLFev9R4q1xUzMqaFIHeiN/ZAfol3\n7iG/xF+f4s1zxuukssgQU9LUnC6HAAAJTUlEQVSXE5c1BrL88wXxOqPQsMdrBA5t974t1O2G+t1e\nF9KWlRBt6f2aUATyJkPeRO/S1Nz4afyRue5BEDkhSvoyNMIR776AMdNgxoVHb3cOWmq9S0zrd8fM\nd0PjPq8LacfLXp2+ZBce3SDkFHnl8VNmge5PEPEp6UswzCB7nDdNOqP/etE279xBwz6vMWjcB437\nY5b3wbvbvLJoa98/IxTxzjdkF/oNw7i4hqHI+9aSOcafF3jLkfSh+d1FAqSkLyNbJMPr/y+Ycux6\nzkFHMzQdhOYaaD7kz7vX/ampBva/4y23HALXdYz3zjrSGGQW9LFccHRDkVngPQ01PU8nrmVE0qdS\nkoOZd0NZeo53BVIiujqhtc5rKFoPe8sth/3lw/5y3ZFt9Xu8S1lb67yxEBjguVWRLMjI8xqBjDyv\nIei1ngsZ+f2Uda/neGU6hyGDRElfUlcofKSL6Xh1dUJbfUxDEdM4tDV6j8Nuq4e2Bm+9rcErq686\nst7W0PseiGPGmubdX5GeC2nZx1j2G75ey1leA5SW6c9jl/15JEPnPVKEkr7IiQiFIWusN409iZ8T\nbfcbiIYjDUNbw5EGo73Ju4O6oyluudlbbz4Ih/3l7vJEG5JezG8cMmPm2X6j4JfFNx6RdG9b2J9H\nMrwpnDHAtu7lmG2h8EnsRDkeCSV9M1sKfBsIAz90zn0tbnsG8DCwGKgBrnPO7TCzy4GvAelAO/BP\nzrnnBzF+kdEtkg6RE/y20Z/OaO+GIdoCHa3eOY9oK3S0+PNmr/yo7d1lLUfqNu73l/260VbvJPsJ\nNTB9sLDXCITTvXMh4XTv2004rXdZON07MR9ft1d5zOt6/Yw+ykNh771DEX8KefNjloX9qY+yXq8L\ne/enjLBvUAMmfTMLA/cDlwNVwJtmtsI5tzGm2k1ArXNutpktB+4BrgMOAh90zu0xswXASqBksH8J\nEYkRjkC4wDupPNScg852vxFo9xqBaPfU2ntbz3of2zrbvIcDdnZ45V0dvdc7O/wyf7mjxa8X9cva\nvcau12vbe98gGJReDUMkrnGIa0Dmvh/e/3+HNJxEjvSXABXOuUoAM3sMWAbEJv1lwF3+8pPAd8zM\nnHNrYupsADLNLMM5N0iHByISKLMj3TUjkXNHNxixDUJXpz+Pguv01wcq6+rjtV1HlrvLEyrr9N/D\nL8sf+mPiRJJ+CbArZr0KOKe/Os65qJnVAYV4R/rdPgKs6Svhm9ktwC0A06ZNSzh4EZFjMvPvt0gH\n9LhwgEQedNJXh1T8tWrHrGNm8/G6fD7d1xs45x5wzpU558qKi4sTCElERE5EIkm/Cpgasz4F2NNf\nHTOLAAXAIX99CvBr4Abn3LaTDVhERE5cIkn/TWCOmc0ws3RgObAirs4K4EZ/+VrgeeecM7MxwNPA\nnc65VwYraBEROTEDJn3nXBS4De/Km03AE865DWZ2t5ld7Vd7ECg0swrgduAOv/w2YDbwv81srT+N\nH/TfQkREEmLODXAr+TArKytz5eXlQYchIjKqmNkq51zZQPU0YoWISApR0hcRSSFK+iIiKWTE9emb\n2QFg50n8iCJ63xQ2Uiiu46O4jt9IjU1xHZ8TjWu6c27AG51GXNI/WWZWnsjJjOGmuI6P4jp+IzU2\nxXV8hjoude+IiKQQJX0RkRSSjEn/gaAD6IfiOj6K6/iN1NgU1/EZ0riSrk9fRET6l4xH+iIi0o+k\nSfpmttTMNptZhZndMfArhiyOqWb2JzPbZGYbzOwLfvldZrY75hlEVwUU3w4zW+fHUO6XjTOzP5rZ\nVn9+MqO+nkhMp8Tsl7VmVm9mXwxin5nZj8xsv5mtjynrc/+Y5z7/M/e2mS0a5ri+bmbv+O/9a/8B\nh5hZqZm1xOy37w1VXMeIrd+/nZnd6e+zzWb2/mGO6/GYmHaY2Vq/fNj22TFyxPB8zpxzo37CG7t3\nGzATb7SEt4B5AcUyCVjkL+cBW4B5eCOL/eMI2Fc7gKK4sn8H7vCX7wDuCfhvuReYHsQ+Ay4EFgHr\nB9o/wFXAs3jjSZwLvDHMcV0BRPzle2LiKo2tF9A+6/Nv5/8vvAVkADP8/9vwcMUVt/0/gK8M9z47\nRo4Yls9Zshzp9wzp6JxrB7qHdBx2zrlq59xqf7kB78mkI31c4GXAQ/7yQ8CHAozlUmCbc+5kbtA7\nYc65l/DHgojR3/5ZBjzsPK8DY8xs0nDF5Zz7g/OeggvwOt5YF8Oun33Wn2XAY865NufcdqAC7/93\nWOMyMwM+Bjw6FO99LMfIEcPyOUuWpN/XkI6BJ1ozKwXOAt7wi27zv579aLi7UGI44A9mtsq8YSoB\nJjjnqsH7QAJBPv56Ob3/EUfCPutv/4ykz92n8I4Gu80wszVm9qKZXRBQTH397UbKPrsA2Oec2xpT\nNuz7LC5HDMvnLFmSfiJDOg4rM8sFfgl80TlXD3wXmAUsBKrxvloG4b3OuUXAlcDfm9mFAcVxFPMG\n6bka+IVfNFL2WX9GxOfOzL4MRIFH/KJqYJpz7iy88S1+bmb5wxxWf3+7EbHPgOvpfXAx7PusjxzR\nb9U+yk54nyVL0k9kSMdhY2ZpeH/MR5xzvwJwzu1zznU657qAHzBEX2kH4pzb48/34w1juQTY1/11\n0Z/vDyI2vIZotXNunx/jiNhn9L9/Av/cmdmNwF8Bn3B+B7DfdVLjL6/C6zefO5xxHeNvNxL2WQS4\nBni8u2y491lfOYJh+pwlS9JPZEjHYeH3FT4IbHLOfTOmPLYP7sPA+vjXDkNsOWaW172MdyJwPb2H\nu7wReGq4Y/P1OvoaCfvM19/+WQHc4F9dcS5Q1/31fDiY2VLgS8DVzrnmmPJiMwv7yzOBOUDlcMXl\nv29/f7sVwHIzyzCzGX5sfxnO2IDLgHecc1XdBcO5z/rLEQzX52w4zlYPx4R3hnsLXgv95QDjOB/v\nq9fbwFp/ugr4KbDOL18BTAogtpl4V068BWzo3k9AIfDfwFZ/Pi6A2LKBGqAgpmzY9xleo1MNdOAd\nYd3U3/7B+9p9v/+ZWweUDXNcFXh9vd2fs+/5dT/i/33fAlYDHwxgn/X7twO+7O+zzcCVwxmXX/4T\n4Na4usO2z46RI4blc6Y7ckVEUkiydO+IiEgClPRFRFKIkr6ISApR0hcRSSFK+iIiKURJX0QkhSjp\ni4ikECV9EZEU8v8BvU6j3PPX8jwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8bc031c978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 4s 43ms/step\n",
      "Test loss: 0.028913464647932693\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/home/isa/FYPJ/Model/model8.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6152515e-04  6.4852519e-04  4.2907643e-04  3.2760357e-04\n",
      "  0.0000000e+00  1.8101986e-05  2.6504029e-05 -1.5218931e-04\n",
      "  8.5043408e-05  0.0000000e+00  0.0000000e+00  7.2234150e-05\n",
      "  0.0000000e+00  0.0000000e+00 -3.6015952e-04  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      "  2.9508330e-05 -0.0000000e+00 -5.8348201e-06 -2.7481738e-05\n",
      "  4.9577520e-04 -5.3213415e-05 -3.2236669e-05  4.0976236e-05\n",
      "  1.4231662e-04 -6.9386333e-05  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  1.6702113e-04  0.0000000e+00  7.5214099e-05\n",
      "  2.9537801e-04  0.0000000e+00 -1.3572493e-04  1.2237437e-04\n",
      "  0.0000000e+00 -5.6378037e-04  0.0000000e+00  1.5889206e-04\n",
      "  5.3192102e-06  2.5520705e-05  1.3286610e-04  0.0000000e+00\n",
      " -4.0692059e-04  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -2.4506799e-04 -1.5327842e-05  3.4902859e-04  7.6615880e-04\n",
      "  6.3144129e-05 -3.8257778e-05 -1.2044582e-06 -4.0702947e-04\n",
      "  7.4925998e-07 -5.6654480e-06  7.3192859e-05 -1.1860649e-03]\n",
      "[-2.32381775e-04  1.77862355e-03  5.92727214e-04  9.62538528e-04\n",
      " -2.08370802e-05  7.61674237e-05  7.49108658e-05 -4.77838301e-04\n",
      "  2.98782456e-04 -1.62083825e-06 -3.25475885e-05  1.47458151e-04\n",
      "  2.88898100e-05 -2.86891282e-06 -7.71954248e-04 -3.88975104e-06\n",
      "  1.67794700e-04 -7.47636614e-06 -8.69457202e-04 -8.29620785e-06\n",
      "  4.36764257e-03 -3.06403992e-04 -1.15245370e-04 -1.44785547e-04\n",
      "  2.75388011e-03 -4.99541988e-04 -2.10674800e-04  7.41576659e-05\n",
      "  2.78963242e-04 -4.54964640e-04  1.95700541e-05 -2.70745168e-05\n",
      "  1.41237115e-05  2.75587291e-03  5.88469811e-05  2.50865851e-04\n",
      "  1.03584654e-03  1.59111023e-05 -5.64478280e-04  4.07650456e-04\n",
      "  3.56658897e-03 -1.37350534e-03 -3.89710658e-05  2.06775288e-03\n",
      "  9.60797770e-05  1.45603670e-04  1.89211511e-03  1.31714123e-05\n",
      " -1.05710898e-03 -4.94409978e-05  1.00684783e-03  2.56497087e-03\n",
      " -8.88546696e-04  1.44767807e-06  7.56426307e-04  2.22133333e-03\n",
      "  1.91539337e-04 -3.98971402e-04  7.98266865e-06 -3.16229695e-03\n",
      "  2.35364438e-04 -2.62961166e-05  3.34946264e-04 -4.94995527e-03]\n",
      "[ 1.6492519e-05 -1.8642918e-03  7.4733351e-03 -5.9794943e-04\n",
      "  1.3346469e-02  4.5830929e-03  6.0712326e-05 -6.4997616e-06\n",
      " -4.0522739e-03  9.5130941e-03 -3.9044949e-03  4.5996876e-03\n",
      " -2.6167845e-03 -5.5139204e-03  2.8073002e-04 -5.4232907e-03\n",
      "  3.7744598e-05 -2.2020469e-04  6.2529407e-03  1.2680390e-03\n",
      "  4.0623173e-03 -2.5170331e-03  1.4412064e-02  8.2093049e-03\n",
      "  1.2589699e-03  3.2386184e-04 -4.0588141e-03  7.5984686e-03\n",
      "  1.0299523e-03 -3.5643787e-03  1.8992044e-04  4.4009872e-03\n",
      "  6.9158436e-03 -1.1713111e-03  1.3437211e-05  2.0304725e-03\n",
      "  9.0615143e-04 -1.4427763e-03  3.4942231e-04  1.8215701e-02\n",
      " -2.7941860e-04  7.5543119e-04  3.3447839e-06  1.3135260e-03\n",
      "  1.6235776e-03 -4.2878027e-04  6.9087796e-04  8.2916528e-04\n",
      " -7.2813890e-04  8.4287263e-03  2.1895522e-03  1.3640125e-03\n",
      "  7.3379334e-03  2.6042879e-05  1.0515435e-03 -4.4245720e-03\n",
      "  4.0364055e-05 -5.1282709e-03 -3.4573330e-03 -5.1850523e-03\n",
      " -6.7939721e-03 -4.1388697e-04  6.4035889e-04 -5.2216975e-03]\n",
      "[-1.3171464e-04 -1.2780144e-03  4.5153564e-03 -7.6658203e-04\n",
      "  6.4478931e-03  3.2773584e-03 -2.5898713e-04 -4.1668623e-05\n",
      " -2.3935332e-03  6.7267446e-03 -3.6800427e-03  3.3357898e-03\n",
      " -3.0508006e-03 -2.5663620e-03 -1.1951326e-04 -2.4741380e-03\n",
      " -6.4794905e-04 -4.0832255e-04  3.5188526e-03 -8.2994619e-04\n",
      "  1.9874014e-03 -2.0412852e-03  6.1306539e-03  1.9668560e-03\n",
      "  1.0727360e-03 -4.5559140e-05 -2.4551738e-03  8.6256471e-03\n",
      "  9.6560945e-04 -1.9743701e-03  2.3535129e-05  3.2556294e-03\n",
      "  4.2822566e-03 -1.0947263e-04  5.5369939e-05  1.3736330e-03\n",
      "  3.6182743e-04 -3.1036508e-04  9.7261620e-04  9.2405360e-03\n",
      " -4.7954125e-03  1.8209360e-04  1.1710188e-04  7.0708641e-04\n",
      "  9.2579715e-04 -4.1703213e-04  2.2373501e-04  8.1628555e-04\n",
      " -2.2953238e-04  4.0263305e-03  9.1571262e-04  5.5319699e-03\n",
      "  1.9635945e-03  1.2864273e-04  8.3734270e-04 -3.8170533e-03\n",
      "  1.2912057e-04 -5.2585527e-03 -3.4451857e-03 -3.7490793e-03\n",
      " -5.8161011e-03 -1.0848012e-03  3.9707145e-04 -2.8038213e-03]\n",
      "[-1.7760418e-04  1.5308244e-04  4.2972504e-03 -1.2361241e-03\n",
      "  3.2830937e-03  2.5188811e-03 -3.9480621e-04 -4.1436397e-05\n",
      " -2.2930354e-03  5.5778287e-03 -2.5708626e-03  2.5744683e-03\n",
      " -3.0352282e-03 -2.6288100e-03 -3.9280130e-04 -1.9299494e-03\n",
      " -1.0374517e-03 -5.3787237e-04  2.9213184e-03  1.0142681e-03\n",
      "  8.5722125e-04 -1.7389908e-03  5.6797341e-03  2.9324079e-03\n",
      "  9.7029429e-04 -4.9737078e-04 -1.5785542e-03  6.1567007e-03\n",
      "  1.2253879e-03 -2.1360326e-03  2.5595789e-05  3.7244188e-03\n",
      "  3.5504494e-03  3.2515737e-04  7.8784084e-05  1.1864657e-03\n",
      "  1.2461451e-04 -2.0886296e-04  2.1194641e-03  7.3701739e-03\n",
      " -3.4216726e-03 -7.2684707e-05  1.9487902e-04  8.6297200e-04\n",
      "  5.9799233e-04 -4.6855933e-04  1.1809224e-04  5.4154359e-04\n",
      "  6.7027258e-06  3.9135651e-03  5.0066540e-04  5.8428636e-03\n",
      "  8.8571813e-03 -8.3609266e-05 -1.7623758e-04 -2.5473197e-03\n",
      "  2.3192531e-04 -4.2076651e-03 -3.9125886e-03 -3.0775876e-03\n",
      " -4.8168069e-03 -3.5157689e-04  3.1540150e-04 -2.2986145e-03]\n",
      "[-4.6964051e-04 -1.2993070e-03  4.0017741e-04 -1.2086313e-03\n",
      "  2.8862220e-03  2.0597430e-03 -1.7141095e-04 -1.2336070e-04\n",
      " -1.3219524e-03  2.8832832e-03  5.1194844e-03  2.3434635e-03\n",
      " -1.3804954e-04 -5.1230821e-04 -8.1863074e-04 -5.3991203e-04\n",
      " -1.3615534e-03 -7.6880178e-04  2.1552334e-03 -1.5724816e-04\n",
      " -8.9938127e-05 -1.5878514e-03  6.6622118e-03  5.9378645e-03\n",
      "  5.3996732e-04  3.7490053e-04 -6.5709522e-04  3.8772747e-03\n",
      "  8.6284999e-04 -9.4386935e-04 -4.6369227e-05  3.8664429e-03\n",
      "  1.5908581e-03  2.7282213e-04 -8.8428824e-05 -9.9466757e-05\n",
      " -3.9114984e-04 -7.7525177e-04 -2.3032913e-03  4.2750672e-03\n",
      "  1.3913228e-03 -1.0223673e-03  6.0586882e-04  1.5467827e-03\n",
      " -2.8693795e-04  3.5956429e-04 -6.4807558e-05  1.9625117e-04\n",
      "  5.8784703e-05  2.1987315e-03  1.7916251e-04  5.0777206e-03\n",
      " -6.5028686e-03 -2.0435786e-04  4.4864677e-03 -2.2988641e-03\n",
      "  3.7768550e-04 -9.0001058e-03  2.2292398e-03 -2.8581764e-03\n",
      " -3.2675751e-03  5.3732342e-04 -9.0538844e-05 -8.4316242e-04]\n",
      "[ 5.36834821e-03 -6.15190249e-03 -1.23172428e-03  3.25443433e-03\n",
      " -6.02394808e-03  9.35586728e-03  7.77685216e-07  1.10729306e-03\n",
      "  2.25977972e-03 -2.47481205e-02  3.13379476e-03 -1.39046470e-02\n",
      "  9.07588471e-03 -2.02722680e-02  2.24944297e-03  7.96039402e-03\n",
      " -2.80430051e-03 -3.69808084e-04 -6.21669134e-03 -8.46003927e-03\n",
      " -9.62526014e-04 -3.24228290e-03  1.86804403e-02  1.75286736e-03\n",
      " -3.55013435e-05  8.97290767e-04  1.31151350e-02 -8.61314125e-03\n",
      "  1.23429094e-02  1.67165920e-02 -2.31065787e-03 -1.88972000e-02\n",
      "  1.10450972e-04  5.52702742e-03 -3.24908597e-03  6.68148929e-03\n",
      " -2.33898032e-03 -1.84607413e-02 -3.79496050e-04  6.59797573e-03\n",
      " -6.03738800e-03 -1.04592079e-02 -5.73732564e-03  5.65290335e-04\n",
      "  2.16737017e-03  3.10566998e-03 -9.07152798e-03  4.79813525e-03\n",
      "  4.34987573e-03 -4.21126751e-04  5.49075950e-04  1.67333260e-02\n",
      " -1.69818047e-02 -1.43383295e-04  1.40109640e-02 -3.54362372e-03\n",
      "  9.46638267e-03 -1.05791427e-02  3.04643717e-02 -9.32031404e-03\n",
      " -1.27445357e-02  1.00390278e-02 -5.85496332e-03  1.42662395e-02]\n",
      "[ 1.8269585e-03 -3.5723732e-04 -1.7837209e-03  6.8768382e-04\n",
      " -1.0198699e-03  9.3849108e-04 -1.1773531e-05  7.9993898e-04\n",
      " -3.3260856e-03 -8.6151296e-03 -5.2551627e-03 -3.2184622e-03\n",
      "  4.2653969e-03 -1.0751868e-02 -3.6491122e-04  2.5570039e-03\n",
      " -4.4655520e-04 -1.9113994e-03 -7.7336380e-04  1.7364958e-03\n",
      " -2.7455820e-03  6.7728502e-04  3.3439399e-04 -7.2282791e-04\n",
      " -2.5260421e-03 -4.6872563e-04  3.8176875e-03  5.6827669e-03\n",
      "  5.7664574e-03 -3.5891775e-04  1.1449466e-04 -1.6463757e-02\n",
      " -4.6431730e-03 -3.0491236e-03 -2.2362182e-03  8.0486611e-03\n",
      " -9.5235379e-03 -1.6835448e-03  3.5476498e-03  1.4392079e-03\n",
      " -1.3473686e-02 -1.9076206e-02 -1.5575168e-04 -3.4432434e-03\n",
      "  3.2506385e-04  7.8342957e-03 -4.3228893e-03  1.4808633e-03\n",
      "  1.6773009e-03  1.9095207e-03 -4.7911049e-04  1.4769154e-03\n",
      " -6.6489493e-03  3.0550692e-04  2.3439098e-03 -5.6322925e-03\n",
      "  2.4724279e-03  4.4243112e-03  1.4707900e-02  2.8141632e-03\n",
      "  2.6387477e-03  5.9525594e-03 -1.4966507e-03  4.4969064e-03]\n",
      "[ 1.0928655e-03 -4.6566250e-03 -2.6413207e-03  3.7807945e-04\n",
      " -2.0679927e-03 -1.4242265e-04 -1.8114560e-04  5.4369256e-04\n",
      " -2.2469303e-03 -9.2404783e-03 -1.2533778e-03 -1.3306192e-03\n",
      "  4.7354209e-03 -4.6947487e-03 -6.1839656e-04  1.7325880e-03\n",
      "  1.1029506e-04 -1.6601896e-03  1.7930935e-03  3.0093777e-04\n",
      " -3.1056260e-03  4.2918124e-04 -8.1108359e-04  2.1760247e-03\n",
      " -3.4154484e-03 -9.8956036e-05  2.6904552e-03  4.3846089e-03\n",
      "  8.0347005e-03 -6.0091750e-04 -1.6931417e-05 -1.2475788e-02\n",
      " -3.3089737e-03 -2.5129712e-03 -1.1071659e-03  6.4546317e-03\n",
      " -7.7319606e-03 -1.8312582e-03  2.7809916e-03  1.6185674e-03\n",
      " -1.5619812e-02 -1.4571717e-02  1.2713986e-03 -2.9827312e-03\n",
      " -9.6226591e-05  5.8101127e-03 -3.5655566e-03  1.6083190e-03\n",
      "  1.6688772e-03  1.3188983e-03 -2.0911027e-04  9.3649066e-04\n",
      " -1.0360011e-02 -1.1691004e-05  6.9905217e-03 -2.5547640e-03\n",
      "  2.2836118e-03  7.6386804e-04  1.5843038e-02  3.9123744e-03\n",
      "  2.3966564e-03  5.7192575e-03 -1.6249573e-03  4.4276337e-03]\n",
      "[ 8.28994787e-04 -2.16195686e-03 -2.21427367e-03  1.24184298e-05\n",
      " -3.64930648e-03 -6.22372900e-04 -1.48555744e-04  1.08255015e-04\n",
      " -9.96471266e-04 -8.06458201e-03 -5.09907084e-04 -1.79991941e-03\n",
      "  3.49689205e-03 -4.24031401e-03 -3.79013160e-04  2.20945294e-04\n",
      "  9.05099267e-04 -8.76320701e-04  7.59377668e-04  1.81665062e-03\n",
      " -2.91496632e-03  7.04526668e-04  6.54541009e-06  2.99266935e-03\n",
      " -3.19751352e-03  6.99287557e-05  1.76794955e-03  3.89697519e-03\n",
      "  5.61512308e-03 -6.19486265e-04  1.12297976e-05 -1.04665579e-02\n",
      " -2.36380449e-03 -1.51075667e-03  2.56143278e-04  5.73722040e-03\n",
      " -5.09013515e-03 -1.43280602e-03  3.38706118e-03  1.92417495e-03\n",
      " -1.22121796e-02 -1.13646695e-02  2.18965812e-03 -2.22246489e-03\n",
      " -7.80282571e-05  3.07799457e-03 -2.55756290e-03  2.13888730e-03\n",
      "  1.42078171e-03  1.08627521e-03  4.86927202e-05  9.75691830e-04\n",
      " -1.39832066e-03 -2.36103879e-04  4.65911767e-03 -1.22948003e-03\n",
      "  2.29134597e-03  1.41148979e-03  1.14739593e-02  3.23158363e-03\n",
      "  2.09887116e-03  4.40416113e-03 -1.30634068e-03  3.40566784e-03]\n",
      "[ 3.5579293e-04 -7.0593576e-04 -2.1227705e-03 -1.1412046e-04\n",
      " -2.7108528e-03 -2.1955321e-04 -4.8755730e-05 -1.7646886e-04\n",
      "  6.6898763e-05 -6.0116453e-03 -4.2040199e-03 -1.3061925e-03\n",
      "  9.8612963e-04 -1.5726124e-03 -1.9657708e-04 -4.0267128e-04\n",
      "  1.0044918e-03 -5.4601877e-04 -1.3612284e-04 -3.3519957e-03\n",
      " -1.5945880e-03  3.3272291e-04 -3.0956026e-03 -5.6144590e-03\n",
      " -1.9732830e-03  3.5227320e-04  6.9072150e-04  3.0937504e-03\n",
      "  4.0930151e-03 -5.1219593e-04  1.3023107e-05 -6.9561894e-03\n",
      " -1.2855848e-03 -5.4373220e-04  9.3482924e-04  4.3938621e-03\n",
      " -3.2159546e-03 -1.1720645e-03  4.2496193e-03  2.2417093e-03\n",
      " -5.9429305e-03 -6.2700072e-03  2.0004921e-03 -1.5321022e-03\n",
      " -1.6165218e-04  1.2627541e-03 -1.7228095e-03  2.2157787e-03\n",
      "  1.2465166e-03  1.1828570e-03 -1.5245366e-04  1.5466909e-03\n",
      " -5.0933030e-03  7.1488263e-05  1.9576803e-03 -1.6429322e-03\n",
      "  1.4838213e-03  7.5425487e-03  4.1277599e-04  2.1770110e-03\n",
      "  1.3894446e-03  5.1336829e-04 -6.1110011e-04  1.7354138e-03]\n",
      "[-1.20647319e-05  3.63169238e-04 -2.05062749e-03 -1.71096603e-04\n",
      " -2.25126231e-03 -2.00787224e-04  3.64982916e-05 -3.40542872e-04\n",
      "  3.08220420e-04 -4.28399444e-03 -6.88623125e-03 -8.23042879e-04\n",
      " -2.99281004e-04 -1.43834632e-05 -1.92811116e-04 -6.46020751e-04\n",
      "  9.46500397e-04 -2.63774593e-04 -3.82422993e-04 -6.40858291e-03\n",
      " -9.45962791e-04  1.59688410e-04 -4.94095124e-03 -1.06007028e-02\n",
      " -1.39205414e-03  4.84551914e-04  1.19510514e-04  2.32845242e-03\n",
      "  2.86623440e-03 -2.86336464e-04  2.14278098e-05 -4.56926879e-03\n",
      " -7.55866524e-04 -1.92580599e-04  1.07763556e-03  3.22003174e-03\n",
      " -1.95003510e-03 -8.39104992e-04  4.52530105e-03  2.00102408e-03\n",
      " -2.31938367e-03 -3.60085885e-03  1.77113526e-03 -1.14965823e-03\n",
      " -1.87590194e-04  4.42201446e-04 -1.17699825e-03  1.83628267e-03\n",
      "  9.23395215e-04  9.77281830e-04 -1.35804090e-04  2.19001458e-03\n",
      " -7.88135268e-03  2.33146257e-05  3.24984372e-04 -2.04873993e-03\n",
      "  1.11106294e-03  1.16422055e-02 -5.94907021e-03  1.69510895e-03\n",
      "  9.71342379e-04 -1.53597258e-03 -3.30711511e-04  9.51505732e-04]\n",
      "[-1.4730236e-04  1.5178568e-03 -7.4670359e-04 -1.2712544e-04\n",
      " -1.3939530e-03 -5.9864519e-04  3.3856504e-05 -3.2136356e-04\n",
      "  5.2135430e-05 -1.6133619e-03 -2.6259450e-03 -4.3189118e-04\n",
      "  3.0401815e-04 -2.5643583e-04 -1.4267370e-04 -6.4812385e-04\n",
      "  6.1796879e-04  1.6725842e-05 -3.9262031e-05 -1.5835650e-03\n",
      " -6.0985395e-04  1.3400672e-04 -1.0522312e-03 -2.7209348e-03\n",
      " -8.7766862e-04  2.5187052e-04 -8.9668261e-05  1.2637883e-03\n",
      "  1.2084362e-03  6.7825116e-05  2.9985256e-05 -1.6356384e-03\n",
      " -4.6281383e-04 -3.1842003e-04  7.5965526e-04  1.2979765e-03\n",
      " -6.3760416e-04 -3.3071442e-04  1.8891525e-03  6.9535366e-04\n",
      " -6.0733687e-04 -1.5786886e-03  1.0157344e-03 -6.6167716e-04\n",
      " -7.4715019e-05  1.2572319e-04 -4.5333689e-04  7.4953248e-04\n",
      "  3.0869636e-04  1.5133737e-04  1.4097166e-04  1.3800743e-03\n",
      " -1.3388938e-03 -3.7487349e-04  3.4752669e-04 -6.3254533e-04\n",
      "  8.7854348e-04  3.2334130e-03 -2.5899764e-03  1.0334505e-03\n",
      "  4.4400489e-04  1.1861562e-04 -2.8299238e-04  3.9455685e-04]\n",
      "[-1.4083603e-04  2.2545918e-03 -9.9975274e-05 -8.0266065e-05\n",
      " -1.0692666e-03 -6.2311825e-04  2.4640860e-05 -2.6334473e-04\n",
      " -6.4645727e-05 -8.1899273e-04 -1.6719395e-04 -2.8629202e-04\n",
      "  5.2366121e-04 -4.7350387e-04 -7.6441844e-05 -5.3582812e-04\n",
      "  4.1641726e-04  5.4486012e-05 -1.0081772e-05  9.7423428e-05\n",
      " -4.2931730e-04  1.2261127e-04  5.3029694e-04  3.4974384e-04\n",
      " -5.0210190e-04  1.4058087e-04 -1.1652817e-04  7.3865347e-04\n",
      "  7.1388367e-04  1.3863729e-04  2.4456669e-05 -7.5009925e-04\n",
      " -3.9050553e-04 -3.0195390e-04  5.4163858e-04  7.0758618e-04\n",
      " -2.6368719e-04 -1.4016124e-04  1.0763621e-03  2.9077005e-04\n",
      "  4.1451162e-06 -8.1387319e-04  6.7220745e-04 -5.1848893e-04\n",
      " -2.1229343e-05  3.9775532e-05 -2.6401726e-04  3.1836171e-04\n",
      "  1.6852474e-04 -2.4085222e-05  1.5504130e-04  1.3009669e-03\n",
      "  7.8145694e-04 -4.0752991e-04  4.4229825e-04  1.9611974e-05\n",
      "  6.8726297e-04  4.4873042e-04 -9.3357085e-04  7.8332948e-04\n",
      "  2.2613443e-04  6.3964026e-04 -2.2958331e-04  1.7812251e-04]\n",
      "[-2.45864998e-04  2.08959659e-03 -1.90355189e-04 -1.62706943e-04\n",
      "  2.14577769e-03 -5.12189930e-04  7.76584347e-05 -2.56936095e-04\n",
      " -1.57657982e-04 -5.54949802e-04 -3.59895173e-04 -1.01655285e-04\n",
      "  3.05840716e-04  5.05997596e-05 -8.23707251e-06 -5.95197896e-04\n",
      "  4.79543465e-04  1.04410610e-04  1.22609035e-05 -1.11944799e-03\n",
      " -3.79080971e-04  1.41675018e-05  2.19153968e-04 -3.21362808e-04\n",
      " -3.24074557e-04  8.32464575e-05 -1.90796956e-04  5.85027505e-04\n",
      "  6.99157536e-04  1.59589268e-04  1.84383953e-05 -7.59349379e-04\n",
      " -3.77173099e-04 -2.52225174e-04  5.02397132e-04  5.75757644e-04\n",
      " -1.11835383e-04 -2.96767568e-04  8.16551619e-04  3.69517569e-04\n",
      " -9.43902298e-04 -7.49920437e-04  6.92863949e-04 -6.87019783e-04\n",
      " -8.03982985e-05  5.54039652e-05 -2.73884740e-04  1.91457657e-04\n",
      "  1.71601190e-04  1.09177192e-04  1.95570494e-04 -3.45834182e-04\n",
      " -5.43202681e-04 -3.70310154e-04  5.00714406e-04  1.60955708e-04\n",
      "  6.18745340e-04  1.30353170e-03 -1.19111466e-03  7.83564406e-04\n",
      "  1.89376296e-04  5.37845772e-04 -1.60125710e-04  1.66930404e-04]\n",
      "[-2.11434613e-04 -2.73943529e-04 -5.56692074e-04 -1.61731252e-04\n",
      " -1.88992196e-03 -3.79230827e-04  3.98263219e-05 -2.06784345e-04\n",
      " -7.90529593e-05 -1.94212786e-04  4.42796998e-04  3.35629331e-03\n",
      "  9.15577344e-04  1.17453129e-03 -1.66003054e-04 -4.42064949e-04\n",
      "  3.56463279e-04  7.03772748e-05  2.72944872e-03  1.66103744e-03\n",
      " -1.24306395e-03 -1.81193725e-04 -1.76346541e-04 -2.83626432e-04\n",
      " -6.37352568e-05  1.18088996e-04 -8.92238022e-05 -7.66273763e-04\n",
      "  5.92820463e-04 -4.55480185e-05 -6.28723574e-05  5.35244704e-04\n",
      " -2.79228727e-04  1.41642769e-04  3.27318266e-04 -2.39314570e-04\n",
      " -1.28080035e-04 -5.08911675e-04  6.73358038e-04 -6.44044412e-05\n",
      " -2.07366305e-03  4.34868678e-04  5.17850800e-04 -1.14243907e-04\n",
      " -3.65183572e-04  4.14031092e-05 -6.16471807e-05  2.44124767e-05\n",
      " -6.83982080e-07 -5.72429126e-05  1.42392382e-04  6.94342947e-04\n",
      " -7.93955114e-04 -2.48105003e-04  6.33145217e-03  2.78263493e-03\n",
      "  4.16588533e-04 -1.74543331e-03  5.24520758e-04  1.25370477e-03\n",
      "  9.09891460e-05  3.43791093e-04 -1.59453572e-04  8.15416570e-05]\n",
      "Offset\n",
      "[ 0.0000000e+00 -1.4733989e-04 -2.9087487e-05 -0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00  5.6611538e-05 -0.0000000e+00\n",
      "  0.0000000e+00  1.1325798e-04 -0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -0.0000000e+00 -1.4896348e-05  0.0000000e+00\n",
      "  1.6267692e-04 -0.0000000e+00 -0.0000000e+00  4.3266334e-04\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00 -0.0000000e+00  0.0000000e+00  3.8864826e-05\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00 -1.4026666e-05 -0.0000000e+00 -1.1367845e-06\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00]\n",
      "[ 1.59016126e-04 -1.88083359e-04 -0.00000000e+00 -6.75264346e-06\n",
      "  0.00000000e+00  2.18071673e-05  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.66382073e-04\n",
      " -0.00000000e+00  1.02282014e-04  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -4.09426139e-06  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00  4.76489076e-05\n",
      "  0.00000000e+00  2.75391312e-06  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  2.70032178e-05  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "x_predict = test_dataset['x'][:1]\n",
    "predictions = model.predict(x_predict)\n",
    "print(predictions[0][0]) # 1\n",
    "print(predictions[0][1]) # 2 (1st row)\n",
    "print(predictions[0][2]) # 1\n",
    "print(predictions[0][3]) # 2 (2nd row)\n",
    "print(predictions[0][4]) # 1\n",
    "print(predictions[0][5]) # 2 (3rd row)\n",
    "print(predictions[0][6]) # 1\n",
    "print(predictions[0][7]) # 2 (4th row)\n",
    "print(predictions[0][8]) # 1\n",
    "print(predictions[0][9]) # 2 (5th row)\n",
    "print(predictions[0][10])# 1\n",
    "print(predictions[0][11])# 2 (6th row)\n",
    "print(predictions[0][12])# 1\n",
    "print(predictions[0][13])# 2 (7th row)\n",
    "print(predictions[0][14])# 1\n",
    "print(predictions[0][15])# 2 (8th row)\n",
    "print(\"Offset\")\n",
    "print(predictions[0][788])# 1\n",
    "print(predictions[0][789])# 2 (OFFSET 0xC50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
