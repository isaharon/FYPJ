{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of seed files collected: 92\n",
      "Data collection of XY simulated to 1% sampling rate, Dataset split: 67/20/5\n",
      "No. of samples: 1775, No. of timesteps: 2560, Chunksize: 64\n",
      "Training shape:  (1775, 2560, 64) (1775, 2560, 64)\n",
      "Validation shape:  (500, 2560, 64) (500, 2560, 64)\n",
      "Test shape:  (120, 2560, 64) (120, 2560, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train = \"Data/readelf/10/train_dataset.npz\"\n",
    "val = \"Data/readelf/10/val_dataset.npz\"\n",
    "test = \"Data/readelf/10/test_dataset.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset = np.load(train)\n",
    "val_dataset = np.load(val)\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = training_dataset['x'][:1775]\n",
    "y_train = training_dataset['y'][:1775]\n",
    "\n",
    "x_val = val_dataset['x']\n",
    "y_val = val_dataset['y']\n",
    "\n",
    "x_test = test_dataset['x']\n",
    "y_test = test_dataset['y']\n",
    "\n",
    "print(\"No. of seed files collected: 92\")\n",
    "print(\"Data collection of XY simulated to 1% sampling rate, Dataset split: 67/20/5\")\n",
    "samples, timesteps, chunksize = x_train.shape\n",
    "print(\"No. of samples: \" + str(samples) + \", No. of timesteps: \" + str(timesteps) + \", Chunksize: \" + str(chunksize))\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, y_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 2560, 64)          33024     \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1775 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "1775/1775 [==============================] - 278s 157ms/step - loss: 0.0961 - val_loss: 0.0861\n",
      "Epoch 2/200\n",
      "1775/1775 [==============================] - 269s 152ms/step - loss: 0.0842 - val_loss: 0.0781\n",
      "Epoch 3/200\n",
      "1775/1775 [==============================] - 268s 151ms/step - loss: 0.0772 - val_loss: 0.0729\n",
      "Epoch 4/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0725 - val_loss: 0.0693\n",
      "Epoch 5/200\n",
      "1775/1775 [==============================] - 267s 150ms/step - loss: 0.0692 - val_loss: 0.0666\n",
      "Epoch 6/200\n",
      "1775/1775 [==============================] - 269s 151ms/step - loss: 0.0667 - val_loss: 0.0645\n",
      "Epoch 7/200\n",
      "1775/1775 [==============================] - 265s 150ms/step - loss: 0.0647 - val_loss: 0.0628\n",
      "Epoch 8/200\n",
      "1775/1775 [==============================] - 268s 151ms/step - loss: 0.0630 - val_loss: 0.0613\n",
      "Epoch 9/200\n",
      "1775/1775 [==============================] - 268s 151ms/step - loss: 0.0615 - val_loss: 0.0600\n",
      "Epoch 10/200\n",
      "1775/1775 [==============================] - 267s 150ms/step - loss: 0.0602 - val_loss: 0.0588\n",
      "Epoch 11/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0590 - val_loss: 0.0577\n",
      "Epoch 12/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0579 - val_loss: 0.0567\n",
      "Epoch 13/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0569 - val_loss: 0.0558\n",
      "Epoch 14/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0560 - val_loss: 0.0550\n",
      "Epoch 15/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0551 - val_loss: 0.0542\n",
      "Epoch 16/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0544 - val_loss: 0.0535\n",
      "Epoch 17/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0536 - val_loss: 0.0529\n",
      "Epoch 18/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0530 - val_loss: 0.0523\n",
      "Epoch 19/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0524 - val_loss: 0.0517\n",
      "Epoch 20/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0518 - val_loss: 0.0512\n",
      "Epoch 21/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0513 - val_loss: 0.0507\n",
      "Epoch 22/200\n",
      "1775/1775 [==============================] - 260s 146ms/step - loss: 0.0508 - val_loss: 0.0503\n",
      "Epoch 23/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0504 - val_loss: 0.0498\n",
      "Epoch 24/200\n",
      "1775/1775 [==============================] - 267s 150ms/step - loss: 0.0499 - val_loss: 0.0494\n",
      "Epoch 25/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0495 - val_loss: 0.0491\n",
      "Epoch 26/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0492 - val_loss: 0.0487\n",
      "Epoch 27/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0488 - val_loss: 0.0484\n",
      "Epoch 28/200\n",
      "1775/1775 [==============================] - 268s 151ms/step - loss: 0.0485 - val_loss: 0.0481\n",
      "Epoch 29/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0482 - val_loss: 0.0478\n",
      "Epoch 30/200\n",
      "1775/1775 [==============================] - 267s 150ms/step - loss: 0.0479 - val_loss: 0.0476\n",
      "Epoch 31/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0477 - val_loss: 0.0473\n",
      "Epoch 32/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0474 - val_loss: 0.0471\n",
      "Epoch 33/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0472 - val_loss: 0.0469\n",
      "Epoch 34/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0470 - val_loss: 0.0467\n",
      "Epoch 35/200\n",
      "1775/1775 [==============================] - 264s 148ms/step - loss: 0.0468 - val_loss: 0.0465\n",
      "Epoch 36/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0466 - val_loss: 0.0463\n",
      "Epoch 37/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0464 - val_loss: 0.0461\n",
      "Epoch 38/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0462 - val_loss: 0.0459\n",
      "Epoch 39/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0460 - val_loss: 0.0457\n",
      "Epoch 40/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0459 - val_loss: 0.0456\n",
      "Epoch 41/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0457 - val_loss: 0.0454\n",
      "Epoch 42/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0456 - val_loss: 0.0453\n",
      "Epoch 43/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0454 - val_loss: 0.0452\n",
      "Epoch 44/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0453 - val_loss: 0.0450\n",
      "Epoch 45/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0452 - val_loss: 0.0449\n",
      "Epoch 46/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0450 - val_loss: 0.0448\n",
      "Epoch 47/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0449 - val_loss: 0.0447\n",
      "Epoch 48/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0448 - val_loss: 0.0446\n",
      "Epoch 49/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0447 - val_loss: 0.0445\n",
      "Epoch 50/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0446 - val_loss: 0.0444\n",
      "Epoch 51/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0445 - val_loss: 0.0443\n",
      "Epoch 52/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0444 - val_loss: 0.0442\n",
      "Epoch 53/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0443 - val_loss: 0.0441\n",
      "Epoch 54/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0442 - val_loss: 0.0440\n",
      "Epoch 55/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0441 - val_loss: 0.0439\n",
      "Epoch 56/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0441 - val_loss: 0.0438\n",
      "Epoch 57/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0440 - val_loss: 0.0437\n",
      "Epoch 58/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0439 - val_loss: 0.0437\n",
      "Epoch 59/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0438 - val_loss: 0.0436\n",
      "Epoch 60/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0438 - val_loss: 0.0435\n",
      "Epoch 61/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0437 - val_loss: 0.0435\n",
      "Epoch 62/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0437 - val_loss: 0.0434\n",
      "Epoch 63/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0436 - val_loss: 0.0434\n",
      "Epoch 64/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0435 - val_loss: 0.0433\n",
      "Epoch 65/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0435 - val_loss: 0.0432\n",
      "Epoch 66/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0434 - val_loss: 0.0432\n",
      "Epoch 67/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0434 - val_loss: 0.0431\n",
      "Epoch 68/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0433 - val_loss: 0.0431\n",
      "Epoch 69/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0433 - val_loss: 0.0431\n",
      "Epoch 70/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0433 - val_loss: 0.0430\n",
      "Epoch 71/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0432 - val_loss: 0.0430\n",
      "Epoch 72/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0432 - val_loss: 0.0429\n",
      "Epoch 73/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0431 - val_loss: 0.0429\n",
      "Epoch 74/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0431 - val_loss: 0.0429\n",
      "Epoch 75/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0431 - val_loss: 0.0429\n",
      "Epoch 76/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0431 - val_loss: 0.0428\n",
      "Epoch 77/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0430 - val_loss: 0.0428\n",
      "Epoch 78/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0430 - val_loss: 0.0428\n",
      "Epoch 79/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0430 - val_loss: 0.0427\n",
      "Epoch 80/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0430 - val_loss: 0.0427\n",
      "Epoch 81/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0429 - val_loss: 0.0427\n",
      "Epoch 82/200\n",
      "1775/1775 [==============================] - 264s 148ms/step - loss: 0.0429 - val_loss: 0.0427\n",
      "Epoch 83/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0429 - val_loss: 0.0427\n",
      "Epoch 84/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0429 - val_loss: 0.0427\n",
      "Epoch 85/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0429 - val_loss: 0.0426\n",
      "Epoch 86/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0429 - val_loss: 0.0426\n",
      "Epoch 87/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0428 - val_loss: 0.0426\n",
      "Epoch 88/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0428 - val_loss: 0.0426\n",
      "Epoch 89/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0428 - val_loss: 0.0426\n",
      "Epoch 90/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0428 - val_loss: 0.0426\n",
      "Epoch 91/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0428 - val_loss: 0.0426\n",
      "Epoch 92/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0428 - val_loss: 0.0426\n",
      "Epoch 93/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0428 - val_loss: 0.0426\n",
      "Epoch 94/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0428 - val_loss: 0.0425\n",
      "Epoch 95/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0428 - val_loss: 0.0425\n",
      "Epoch 96/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0428 - val_loss: 0.0425\n",
      "Epoch 97/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0428 - val_loss: 0.0425\n",
      "Epoch 98/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 99/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 100/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 101/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 102/200\n",
      "1775/1775 [==============================] - 264s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 103/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 104/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 105/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 106/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 107/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 108/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 109/200\n",
      "1775/1775 [==============================] - 258s 146ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 110/200\n",
      "1775/1775 [==============================] - 262s 147ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 111/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 112/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 113/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 114/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 115/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 116/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 117/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 118/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0425\n",
      "Epoch 119/200\n",
      "1775/1775 [==============================] - 261s 147ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 120/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 121/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 122/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 123/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 124/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 125/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 126/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 127/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 128/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 129/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 130/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0427 - val_loss: 0.0424\n",
      "Epoch 131/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 132/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 133/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 134/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 135/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 136/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 137/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 138/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 139/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 140/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 141/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 142/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 143/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 144/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 145/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 146/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 148/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 149/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 150/200\n",
      "1775/1775 [==============================] - 261s 147ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 151/200\n",
      "1775/1775 [==============================] - 259s 146ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 152/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 153/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 154/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 155/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 156/200\n",
      "1775/1775 [==============================] - 264s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 157/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 158/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 159/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 160/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 161/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 162/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 163/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 164/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 165/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 166/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 167/200\n",
      "1775/1775 [==============================] - 264s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 168/200\n",
      "1775/1775 [==============================] - 266s 150ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 169/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 170/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 171/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 172/200\n",
      "1775/1775 [==============================] - 264s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 173/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 174/200\n",
      "1775/1775 [==============================] - 267s 150ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 175/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 176/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 177/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 178/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 179/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 180/200\n",
      "1775/1775 [==============================] - 268s 151ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 181/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 182/200\n",
      "1775/1775 [==============================] - 261s 147ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 183/200\n",
      "1775/1775 [==============================] - 261s 147ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 184/200\n",
      "1775/1775 [==============================] - 260s 147ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 185/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 186/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 187/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 188/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 189/200\n",
      "1775/1775 [==============================] - 265s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 190/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 191/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 192/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 193/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 194/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 195/200\n",
      "1775/1775 [==============================] - 261s 147ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 196/200\n",
      "1775/1775 [==============================] - 259s 146ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 197/200\n",
      "1775/1775 [==============================] - 261s 147ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 198/200\n",
      "1775/1775 [==============================] - 263s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 199/200\n",
      "1775/1775 [==============================] - 262s 148ms/step - loss: 0.0426 - val_loss: 0.0424\n",
      "Epoch 200/200\n",
      "1775/1775 [==============================] - 264s 149ms/step - loss: 0.0426 - val_loss: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(2560, 64), return_sequences=True))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error')\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200, #change accordingly\n",
    "                    batch_size=25, #change accordingly\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"/home/isa/FYPJ/Model/model10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efebad67438>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xt4XPV95/H3dy6SdbNsSzLYlo2E\nbQgmcQwohoZLUi4JkBTnQhKTbMMT2NJsy+6mabZLN1seQp9mS0jCNi1pyhYSCFAgJNmIrhOnjQlx\nCAbLYIMNNhY2xvJVvsm63+a7f8yRGY9npLEtz4gzn9fzzDNnfuc3mu8cjT7npzPnYu6OiIgUh0ih\nCxARkfxR6IuIFBGFvohIEVHoi4gUEYW+iEgRUeiLiBQRhb6ISBFR6IuIFBGFvohIEYkVuoB0tbW1\n3tDQUOgyRETeUdasWbPP3evG6jfhQr+hoYGWlpZClyEi8o5iZtty6afNOyIiRUShLyJSRBT6IiJF\nRKEvIlJEFPoiIkVEoS8iUkQU+iIiRSQ0ob/zUC/f/uUmtu7rLnQpIiITVmhCf19XP99Z0cobe7sK\nXYqIyIQVmtCfFI8C0Dc0XOBKREQmrvCEfiwI/cFEgSsREZm4whP68eRb6RvUSF9EJJvQhH7pkZG+\nQl9EJJvwhH4w0u8f0uYdEZFswhP6sQhmGumLiIwmNKFvZpTGIhrpi4iMIjShD8ndNjXSFxHJLlyh\nH1Poi4iMJlyhH49oP30RkVGELPQ10hcRGU2oQr80HqVPX+SKiGQVqtCfFItopC8iMopwhX48Sr9C\nX0Qkq5CFvr7IFREZTahCvzQW1amVRURGEarQnxSP0K+RvohIViELfY30RURGk1Pom9nVZrbJzFrN\n7LYM80vN7PFg/vNm1hC0l5jZ983sFTNbZ2YfHNfq02g/fRGR0Y0Z+mYWBe4FrgEWADeY2YK0bjcD\nB919HnAPcFfQ/kcA7v4e4CrgW2Z2yv67SO6ymcDdT9VLiIi8o+USwIuBVnff4u4DwGPAkrQ+S4AH\ng+kngSvMzEiuJH4F4O57gUNA03gUnklpcJ1cnWlTRCSzXEJ/FrA95XFb0Jaxj7sPAR1ADbAOWGJm\nMTNrBC4AZp9s0dmMXBxdX+aKiGQWy6GPZWhL336Src8DwDlAC7AN+B0wdMwLmN0C3AIwZ86cHErK\n7Mh1coeGqSZ+wj9HRCSschnpt3H06Lwe2Jmtj5nFgGrggLsPufufufsid18CTAE2p7+Au9/n7k3u\n3lRXV3ci7wNInloZdPUsEZFscgn91cB8M2s0sxJgKdCc1qcZuDGYvh5Y4e5uZuVmVgFgZlcBQ+7+\n6jjVfoyRzTs6KldEJLMxN++4+5CZ3QosB6LAA+6+wczuBFrcvRm4H/ihmbUCB0iuGACmA8vNLAHs\nAP7wVLyJEZOOXBxdI30RkUxy2aaPuy8DlqW13Z4y3Qd8KsPz3gTOPrkSc6eRvojI6EJ1RG5pLPgi\nV9v0RUQyClXovz3SV+iLiGQSstAf2WVTm3dERDIJVeiXapdNEZFRhSr03z4iV6EvIpJJyEJ/5Itc\nbd4REckkZKGvzTsiIqMJVejHoxGiEdOFVEREsghP6O9/A370BRbF3tJZNkVEsghP6A90wYaf0Bjb\np5G+iEgW4Qn9kkoAqqP9+iJXRCSL8IR+aRUAkyN9+iJXRCSL8IR+MNKvimikLyKSTXhCP14GFmWy\n9enUyiIiWYQn9M2gtJIq69XmHRGRLMIT+gAlVVTQq807IiJZhCv0SyupoI+egWOuvS4iIoQt9Esq\nqbA+OvsU+iIimYQr9EurKPdehb6ISBYhC/1KyryH3sFhBoe1XV9EJF24Qr+kitJED4BG+yIiGYQr\n9EsrKRkeCf3BAhcjIjLxhCv0SyqJDXUDrpG+iEgG4Qr90ioiPkwpgxzu1UhfRCRd6EIfoJJeDmuk\nLyJyjHCFfnDStUrr1TZ9EZEMwhX6pUHoowO0REQyCVfoByP9Cno5rJG+iMgxcgp9M7vazDaZWauZ\n3ZZhfqmZPR7Mf97MGoL2uJk9aGavmNlrZvaX41t+mtLJANTEBzTSFxHJYMzQN7MocC9wDbAAuMHM\nFqR1uxk46O7zgHuAu4L2TwGl7v4e4ALgj0dWCKdEsHmnJj6obfoiIhnkMtJfDLS6+xZ3HwAeA5ak\n9VkCPBhMPwlcYWYGOFBhZjGgDBgADo9L5ZkEm3emxfs53KuRvohIulxCfxawPeVxW9CWsY+7DwEd\nQA3JFUA3sAt4C/imux84yZqzC0b602L9dPZrpC8iki6X0LcMbZ5jn8XAMDATaAT+3MzOPOYFzG4x\nsxYza2lvb8+hpCyCkf6UaL+26YuIZJBL6LcBs1Me1wM7s/UJNuVUAweAzwK/cPdBd98LPAs0pb+A\nu9/n7k3u3lRXV3f872JEJArxCiZHFPoiIpnkEvqrgflm1mhmJcBSoDmtTzNwYzB9PbDC3Z3kJp3L\nLakCuAjYOD6lZxFcJ1enYRAROdaYoR9so78VWA68Bjzh7hvM7E4zuy7odj9QY2atwJeBkd067wUq\ngfUkVx7fd/eXx/k9HE1XzxIRySqWSyd3XwYsS2u7PWW6j+TumenP68rUfkqVVlLe38PAcIK+wWEm\nxaN5fXkRkYksXEfkApROpsx7AXRUrohImvCFfkklk4a7AV09S0QkXfhCv2wqpUPJ478U+iIiRwtf\n6FfUEO8/CKA9eERE0oQv9MtriA73UabTK4uIHCOEoV8LwDQ6OdAzUOBiREQmlhCGfg0ANZFO9nX2\nF7gYEZGJJXyhX5Ec6c+Z1Mu+LoW+iEiq8IV+MNKfXdKj0BcRSRPa0J9R0sO+Lm3TFxFJFb7Qn1QN\nkRinx7o00hcRSRO+0DeD8hpqI4fZr5G+iMhRwhf6AOW1TPFOuvqH6BscLnQ1IiITRkhDfxpViQ4A\n2rXbpojIEeEM/YpayocOAWi7vohIinCGfnkNpQPJ8+9oDx4RkbeFNPRrifZ3EGVYI30RkRQhDf0a\nDGcKXexX6IuIHBHO0K9IHqBVP6lXm3dERFKEM/SDo3Iby3pp10hfROSIkIZ+8qRrs0t7dKZNEZEU\n4Qz9qhkAzI4d0he5IiIpwhn65dMgNon6yAH2HO7H3QtdkYjIhBDO0DeD6npOZx9d/UN06Fq5IiJA\nWEMfYPIspg21A9B2sLfAxYiITAzhDf3qeir6dwPQdrCnwMWIiEwMoQ79WPceYgxppC8iEghv6E+e\nheE0lnYq9EVEAjmFvpldbWabzKzVzG7LML/UzB4P5j9vZg1B++fMbG3KLWFmi8b3LWRRPQuAhVUK\nfRGREWOGvplFgXuBa4AFwA1mtiCt283AQXefB9wD3AXg7o+4+yJ3XwT8IfCmu68dzzeQVfVsAM4u\n69A2fRGRQC4j/cVAq7tvcfcB4DFgSVqfJcCDwfSTwBVmZml9bgD+5WSKPS6TkyP9xvghdhzSSF9E\nBHIL/VnA9pTHbUFbxj7uPgR0ADVpfT5DltA3s1vMrMXMWtrb23Ope2yllTBpCjNtP5192ldfRARy\nC/30ETtA+iGuo/YxswuBHndfn+kF3P0+d29y96a6urocSspRdT21iZF99bWJR0Qkl9BvA2anPK4H\ndmbrY2YxoBo4kDJ/KfnctDNi8iwm9+8BdICWiAjkFvqrgflm1mhmJSQDvDmtTzNwYzB9PbDCgxPe\nmFkE+BTJ7wLya8ocJnW9BThv7ddIX0QkNlYHdx8ys1uB5UAUeMDdN5jZnUCLuzcD9wM/NLNWkiP8\npSk/4jKgzd23jH/5Y6g9Cxvo4uyKHjbv7cz7y4uITDRjhj6Auy8DlqW13Z4y3UdyNJ/pub8GLjrx\nEk9C7XwALp5ygJf2dhWkBBGRiSS8R+QC1J4FwHll7bTu6dIplkWk6IU79CfPhHgF8yK76OwfYs9h\nXVBFRIpbuEPfDGrnMWMoeZiBtuuLSLELd+gD1J5FVddWADbv0XZ9ESlu4Q/9mvlEDrcxoyzBZn2Z\nKyJFLvyhXzsfw7lk2mE279HmHREpbkUQ+sk9eM6vaOf1PZ3ag0dEilr4Q79mHkRiLIxt53DfEG8d\n0JG5IlK8wh/68UlQdw5z+l8H4OW2jgIXJCJSOOEPfYCZ76XywHpKYsbLbYcKXY2ISMEUR+jPWIT1\nHuAD0/tYp5G+iBSx4gj9mecBcHn1Tjbs6GA4oS9zRaQ4FUfon3YuWJRF0W10DwyzpV3764tIcSqO\n0I+XwfRzmNO/CUCbeESkaBVH6APMWET5vleomhRlzbYDY/cXEQmh4gn9ORdivQf4g5ndPL9VoS8i\nxal4Qv+MiwG4uuoNtrR3s7ezr8AFiYjkX/GE/rQzofI03j20AYAXNNoXkSJUPKFvBme8n6n7Wigv\niSr0RaQoFU/oA5xxMdbRxodmDfD8FoW+iBSfIgv99wPw0clb2LSnk/ZOXT5RRIpLcYV+3TlQXsMF\nw+sAWLm5vcAFiYjkV3GFfiQCcy9nyq6V1JbHeOZ1hb6IFJfiCn2AeVdi3e3cMKeDlZv3kdB5eESk\niBRf6M+9HIBrytZzoHuA9Tt1SgYRKR7FF/qV02HGe5nf+Txm8PRGbeIRkeJRfKEPMO9K4jtWc8ms\nKP/22u5CVyMikjfFGfpnfwR8mC/UbWL9jsPsONRb6IpERPIip9A3s6vNbJOZtZrZbRnml5rZ48H8\n582sIWXeQjN7zsw2mNkrZjZp/Mo/QTPPg6qZXNj/OwB+uUGjfREpDmOGvplFgXuBa4AFwA1mtiCt\n283AQXefB9wD3BU8NwY8DHzR3c8FPggMjlv1JyoSgXd9hIq3nuE90+MsV+iLSJHIZaS/GGh19y3u\nPgA8BixJ67MEeDCYfhK4wswM+BDwsruvA3D3/e4+PD6ln6RzPgpDvdx8+hZe2HqA/V06OldEwi+X\n0J8FbE953Ba0Zezj7kNAB1ADnAW4mS03sxfN7C8yvYCZ3WJmLWbW0t6ep71pzrgYyqZyeeJZEg7L\nXtmVn9cVESmgXELfMrSlH9GUrU8MuAT4XHD/cTO74piO7ve5e5O7N9XV1eVQ0jiIxmHBx5i87d9Z\nOD1K87qd+XldEZECyiX024DZKY/rgfSEPNIn2I5fDRwI2p9x933u3gMsA84/2aLHzcJPw2APt87c\nzOo3D2ovHhEJvVxCfzUw38wazawEWAo0p/VpBm4Mpq8HVri7A8uBhWZWHqwMPgC8Oj6lj4PZF8Hk\nei7p+zUAT2m0LyIhN2boB9vobyUZ4K8BT7j7BjO708yuC7rdD9SYWSvwZeC24LkHgW+TXHGsBV50\n9/83/m/jBEUi8J5PUv7Wr/n9evjxmjaS6yoRkXCyiRZyTU1N3tLSkr8X3LsRvnshL73rK3x87fn8\n9E/ez3lzpubv9UVExoGZrXH3prH6FecRuammvwvqF7OwvZmyeIQnWtoKXZGIyCmj0Ac4/w+J7n+d\n/zT3AE+t20nPwFChKxIROSUU+gDnfgJKKrkhtoKu/iF+tlZf6IpIOCn0AUorYeFnqN36FO+b7jz0\n3DZ9oSsioaTQH7H4j7Dhfv7HjBZe23WYNdsOFroiEZFxp9AfMf0caLiU9+5+kupJEb7/uzcLXZGI\nyLhT6Ke68ItEOrbztbmb+fkru9h+oKfQFYmIjCuFfqqzr4W6d/GRjkeJmnP/b7cWuiIRkXGl0E8V\nicAlXya+7zVum7uNx1dv52D3QKGrEhEZNwr9dO/+JExt4LP9T9A3NMR9K7cUuiIRkXGj0E8XjcHF\nX6Js71q+MncXP3j2Tdo7dYEVEQkHhX4miz4LVTO5KfFjBoYTfPfXrYWuSERkXCj0M4mVwvv/M2U7\nn+MvztrNI6veYqfOtS8iIaDQz6bpJqiewxe6/hljmH94WqN9EXnnU+hnE58EV91Byb4N/O3c9Tyx\nejtb93UXuioRkZOi0B/NuZ+A+sVct/9+psUHuKN5g87JIyLvaAr90ZjBh79OtHsv9zWu5JnX21m+\nYXehqxIROWEK/bHMfh+8+3re2/YwH5zew51PvUp3v863LyLvTAr9XFz1NSwS4zuV32dnRy/fWbG5\n0BWJiJwQhX4uquvhqjuZvPNZ7m5cy/0rt7Jx9+FCVyUictwU+rm64AvQcCmf3P+PnF12iC89tpa+\nweFCVyUiclwU+rmKROC6vyfiCR6qe5SNuw9z9/JNha5KROS4KPSPx7RGuPJr1OxeyT/MW8P9v93K\nys3tha5KRCRnCv3j9b7/CPM/xEd2/j0fqdnJnz+xjgM6/bKIvEMo9I9XJAIf/yesagb32D3Qc4A/\nfeRFBocTha5MRGRMCv0TUT4NPv0gJb3tNM96iFVb2vnaUxsKXZWIyJgU+idq1vlw9f/i9L0r+ZeG\nn/Pwqrf44aptha5KRGRUOYW+mV1tZpvMrNXMbsswv9TMHg/mP29mDUF7g5n1mtna4Pa98S2/wJpu\nhqabuWj3I3x95rPc0byB37yuL3ZFZOKKjdXBzKLAvcBVQBuw2sya3f3VlG43AwfdfZ6ZLQXuAj4T\nzHvD3ReNc90Tgxlcezd07eGGjd9ly5Qqbvmh8dBNF7K4cVqhqxMROUYuI/3FQKu7b3H3AeAxYEla\nnyXAg8H0k8AVZmbjV+YEFonCJ/8Zm72Yr/bfw0cqX+emH6xm3fZDha5MROQYuYT+LGB7yuO2oC1j\nH3cfAjqAmmBeo5m9ZGbPmNmlJ1nvxBQvgxsew2rm8s2Br/PhSev5/AMvsGFnR6ErExE5Si6hn2nE\nnn5S+Wx9dgFz3P084MvAo2Y2+ZgXMLvFzFrMrKW9/R26Tbx8Gtz4r1jdfL45+LdcFX2Jpf+0ilVb\n9he6MhGRI3IJ/TZgdsrjemBntj5mFgOqgQPu3u/u+wHcfQ3wBnBW+gu4+33u3uTuTXV1dcf/LiaK\nihr4fDN2+rncnbibT5e9wOcfeIFfrNc5+EVkYsgl9FcD882s0cxKgKVAc1qfZuDGYPp6YIW7u5nV\nBV8EY2ZnAvOBLeNT+gRVPg0+/zOsvom/6vsmt09exp880sIPnt2qq26JSMGNGfrBNvpbgeXAa8AT\n7r7BzO40s+uCbvcDNWbWSnIzzshunZcBL5vZOpJf8H7R3Q+M95uYcCZVw+d/Bgs/w3/oeYiHa37A\n159ax5//aJ3OzCkiBWUTbfTZ1NTkLS0thS5jfLjDb+6Gp/+G3ZULuH7/LUyZOY9//NwFzJ5WXujq\nRCREzGyNuzeN1U9H5J5KZvCBv4DPPMzpgzt4uvKvmLf/aa79u5X85MU2be4RkbxT6OfDOX8AX/wN\n8br5/G++xbfLv8/tTzzHnz76os7QKSJ5pdDPl6kNcNNyeP9/4cq+5TxX/T/pe205V377GZ5co1G/\niOSHQj+fYiXwob/GbvolVVWTeSB2F/fEv8s3fvRrPnPfKjbt7ix0hSIScgr9Qpj9PvjjlXDpV7hs\n8Lc8W/Hf+L1dD7Pk737FbT9+mT2H+wpdoYiElEK/UOKT4Iq/wv5kFfF5H+TPeJjnqv6S4Zce5fK7\nV/CNX2xkf1d/oasUkZDRLpsTReuv4N/vgN0vs6ukgb/pvo4VkYv49PsauOWyM5k5pazQFYrIBJbr\nLpsK/YkkkYDXmuHpr8O+TeyPz+Tvez/Mj/0DXHPemXzh4kbOmXHMqYtERBT672iJYdi0DJ79O2hb\nTU+0mgcGr+TBgcuZc8aZfO7COVz7nhlMikcLXamITBAK/TBwh7dWwe++A5uWkbAov4s0cX/vZawr\nvYCPX3AGHz9vFufOnEyxXL5ARDJT6IfN/jfgxYfwtY9g3e0cjNXxaP8l/HTo90jUns3HFs3iuvfO\npKG2otCVikgBKPTDangQNv0cXnwQf2MF5gm2xRr4Ud9inhq+iIrTz+KqBadx1YLT9B+ASBFR6BeD\nzj3w6s9g/Y9h+yoAtsUa+Ne+hawYPo89Ve/m9xfM5NL5tVw0t4bJk+IFLlhEThWFfrHpaIMN/xde\n/wW+7XeYD9MZmczTQwv596FFPMtCzqiv55J5tVw8r5bz5kylJKbDNETCQqFfzHoPwRsr4PXleOu/\nYT3JSzZujTXy676zWJVYwMvRc5lTX88FZ0zl/DlTOf+MqUyrKClw4SJyohT6kpQYhh1rYOsz8OZv\n8beex4Z6AdgabeTZgXm8ODyXtT4Pps1l0RnTeM+sas6dWc05M6qo0iYhkXcEhb5kNjQAO1+CN1fC\nmyvxthZsoAuAnkgFr/hcXhg8k1cSjbzqZxCfOodzZk3h3JnVzJ9eybzplcyZVk4sqk1DIhOJQl9y\nkxiGfa9DWwvsWIPvaIE9r2KevKxjr5XTanNYNzCLjT6HjYnZbI/Momra6cydXsncuuTtzLoK6qeW\nU1tZoj2GRApAoS8nbqAH9r4Ke9bDng2wZwO++xWs//CRLt2RSrYxk42Dp9GamMGbfjptXkd7dDpl\nU05j1tRy6qeWMWtKGbOmljGjuozaylLqqkqZPCmmFYPIOMs19GP5KEbeYUrKob4peQuYe3IPofaN\nsG8zFfs3s2DfZs7Z/wbWufKopw90l7K3t47t22vYOjSNVq/jWaayz6vZ55PpiEwlWllHdVUFdVWl\n1FaWUl0ep7os+61qUpxoRCsKkZOl0JfcmMGU2cnb/Kvebgbo74IDW6BjOxzaTknHduo7tlN/aDsX\nHVqP9bQf+/P6oXuggoMHptDuk9kzXMX+RBWdlLPDy+iknMNeTifldAb3Q/FKhuJVJEoqKSspoawk\nSnlwKyuJUR6PHmkriUWIRyOURCPEo0ZJLBrcJ9vjI+3RCPGgLRYxosEtYsG0GZEIKdMp96nzg/76\nD0YmOoW+nLzSSpixMHlLYwCDvdC1B7raobsduvdCdzsVXe1UdLdT392Od7dD9xboP4wNj3Ld4GFI\n9BqDfaX0Wyn9lNBPCX3E6fFSehJxejxOr8fpo5Q+L2GQKD1EGSLGIFGGPJq8T2kbJnpk3hBRBokF\n8yM4ERIYw0RwjIRHGMZIjDw+Mi+Cm+EWBSIQMbAoWAS3kfkRwJKPLYITTT4HwyyCmSWX2UhbxICR\nlUny3lOmk+2RoB9EzDBLrqMNI/n05M80C+bz9vzUviPTkWDFlb4CS1+dpa/fjp1/fM8/tkcur5E+\n30afP0b/MR6e8pX6+XOm8IWLG0/payj05dSLlyWvETy1IWuXo/6UBvug/zD0HYb+DujrCKaTbZG+\nDkoHeygd7IWhvuRKZbAXhnqTzx3qxQcPwWAvPtgHiUEYHsQSQ5AYxDxxit/wKDztfpwlkpGOA35k\nOvl4ZEXy9suPzBtpe7uvZ/s5xtE/86ifN+LoYMz0Vv2YPseGafrzMvUZj9fK7TlZOo6zXQcvhYu/\nd0pfQ6EvE098UvJWOf2Ef4Sl3R8lkTiyIkjeJ1cGJIaCtqG35yUS4Anw4eA+kdzjaWQ6/ZZt3lHt\nI9N+9LyRVPFk1Ga/J+v8yKjPS1njeK6vNcZ9umOaMvU5Ns4L1ifjjiy59Dk1GmYf+9/yeFPoS/GJ\nRCBSCrHSQlciknc6wkZEpIgo9EVEiohCX0SkiOQU+mZ2tZltMrNWM7stw/xSM3s8mP+8mTWkzZ9j\nZl1m9pXxKVtERE7EmKFvZlHgXuAaYAFwg5ktSOt2M3DQ3ecB9wB3pc2/B/j5yZcrIiInI5eR/mKg\n1d23uPsA8BiwJK3PEuDBYPpJ4AoLjmIws48BW4AN41OyiIicqFxCfxawPeVxW9CWsY+7DwEdQI2Z\nVQD/HfjayZcqIiInK5fQz3R8S/rRCtn6fA24x927Rn0Bs1vMrMXMWtrbM5ynRURExkUuB2e1AbNT\nHtcDO7P0aTOzGFANHAAuBK43s28AU4CEmfW5+z+kPtnd7wPuAzCzdjPbdiJvJlAL7DuJ558qquv4\nqK7jN1FrU13H50TrOiOXTrmE/mpgvpk1AjuApcBn0/o0AzcCzwHXAys8eaL+S0c6mNkdQFd64Kdz\n97pcCs/GzFpyOad0vqmu46O6jt9ErU11HZ9TXdeYoe/uQ2Z2K7AciAIPuPsGM7sTaHH3ZuB+4Idm\n1kpyhL/0VBUsIiInLqdz77j7MmBZWtvtKdN9wKfG+Bl3nEB9IiIyjsJ4RO59hS4gC9V1fFTX8Zuo\ntamu43NK65pw18gVEZFTJ4wjfRERySI0oT/W+YHyWMdsM3vazF4zsw1m9l+D9jvMbIeZrQ1u1xao\nvjfN7JWghpagbZqZ/ZuZbQ7up+a5prNTlstaMztsZl8qxDIzswfMbK+ZrU9py7h8LOk7wWfuZTM7\nP8913W1mG4PX/qmZTQnaG8ysN2W5ndJLMWWpLevvzsz+Mlhmm8zsw3mu6/GUmt40s7VBe96W2SgZ\nkZ/Pmbu/428k9yp6AzgTKAHWAQsKVMsM4Pxgugp4neQ5i+4AvjIBltWbQG1a2zeA24Lp24C7Cvy7\n3E1yn+O8LzPgMuB8YP1Yywe4luQ5pQy4CHg+z3V9CIgF03el1NWQ2q9Ayyzj7y74W1gHlAKNwd9t\nNF91pc3/FnB7vpfZKBmRl89ZWEb6uZwfKC/cfZe7vxhMdwKvcexpKyaa1HMnPQh8rIC1XAG84e4n\nc4DeCXP335Dc7ThVtuWzBHjIk1YBU8xsRr7qcvdfevK0JwCrSB44mXdZllk2S4DH3L3f3bcCrST/\nfvNal5kZ8GngX07Fa49mlIzIy+csLKGfy/mB8s6Sp5g+D3g+aLo1+PfsgXxvQknhwC/NbI2Z3RK0\nnebuuyD5gQRO/OK0J28pR/8hToRllm35TKTP3U0cfSbbRjN7ycyeMbNLsz3pFMv0u5soy+xSYI+7\nb05py/syS8uIvHzOwhL6uZwfKK/MrBL4MfAldz8M/CMwF1gE7CL5r2UhXOzu55M8VfafmtllBarj\nGGZWAlwH/ChomijLLJsJ8bkzs68CQ8AjQdMuYI67nwd8GXjUzCbnuaxsv7sJscyAGzh6cJH3ZZYh\nI7J2zdB2wsssLKGfy/mB8sbM4iR/mY+4+08A3H2Puw+7ewL4P5yif2nH4u47g/u9wE+DOvaM/LsY\n3O8tRG0kV0QvuvueoMYJsczIvnwK/rkzsxuBjwKf82ADcLDpZH8wvYbkdvOz8lnXKL+7ibDMYsAn\ngMdH2vK9zDJlBHn6nIUl9I8nRK+UAAABW0lEQVScHygYLS4leT6gvAu2Fd4PvObu305pT90G93Fg\nffpz81BbhZlVjUyT/CJwPW+fO4ng/mf5ri1w1OhrIiyzQLbl0wx8Pti74iKgY+Tf83wws6tJnrr8\nOnfvSWmvs+TFjzCzM4H5JK9pkTej/O6agaWWvNpeY1DbC/msDbgS2OjubSMN+Vxm2TKCfH3O8vFt\ndT5uJL/hfp3kGvqrBazjEpL/er0MrA1u1wI/BF4J2puBGQWo7UySe06sI3lRm68G7TXAr4DNwf20\nAtRWDuwHqlPa8r7MSK50dgGDJEdYN2dbPiT/7b43+My9AjTlua5Wktt6Rz5n3wv6fjL4/a4DXgT+\noADLLOvvDvhqsMw2Adfks66g/QfAF9P65m2ZjZIRefmc6YhcEZEiEpbNOyIikgOFvohIEVHoi4gU\nEYW+iEgRUeiLiBQRhb6ISBFR6IuIFBGFvohIEfn/G1I+/SyH6p8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff3224c198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 26ms/step\n",
      "Test loss: 0.04148182570934296\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0000000e+00 -4.9997658e-07  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  2.1105069e-07  0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -1.7963988e-09\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00  5.7449182e-07  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.6148406e-06\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -1.1633709e-06]\n",
      "[ 3.24734160e-08 -7.99857617e-07  1.24661449e-06 -8.59019735e-07\n",
      " -1.96377277e-06 -8.94791242e-07  1.30483670e-06 -1.51262455e-07\n",
      " -7.45659065e-07  6.11137693e-06  3.88147669e-07 -4.47532329e-06\n",
      " -7.02037084e-07  9.27574717e-07  6.76086756e-07 -1.43721223e-07\n",
      " -3.14451000e-07 -1.44016838e-06  4.81362633e-07  1.45767686e-07\n",
      "  1.63223049e-05 -1.34931895e-07  2.07535959e-06 -6.08435471e-07\n",
      " -9.76254341e-06  2.45152273e-06 -1.09478215e-07 -4.57432321e-07\n",
      " -1.41010003e-06 -1.10713052e-06 -2.06193613e-06 -1.01973626e-06\n",
      " -6.65813559e-07 -6.67070822e-07  2.59666308e-06  2.61325340e-07\n",
      "  2.56850058e-06 -1.15571663e-06  1.76535792e-07  7.42821442e-07\n",
      "  1.79919846e-06 -2.46405108e-07  3.84638952e-06 -1.23911354e-06\n",
      "  1.72972295e-06  2.67264340e-07 -5.33919717e-08 -7.71943860e-07\n",
      "  6.43643830e-07 -1.32935349e-06 -2.23482402e-07 -9.83231530e-07\n",
      " -9.55278168e-08 -1.53872065e-06 -2.94359069e-07  1.69629129e-06\n",
      " -2.50850167e-06  2.72879743e-06 -2.45914407e-06  7.04936019e-06\n",
      "  3.47757975e-07  1.83575719e-06 -9.49843241e-07 -7.14255066e-06]\n",
      "[-6.44914655e-09 -2.24804785e-06  1.54554198e-06 -9.34920990e-06\n",
      " -5.09485017e-06  1.19288950e-06  1.03393368e-05 -4.21108643e-06\n",
      "  2.89972595e-05  8.68991265e-06 -9.83993777e-06 -3.30242392e-06\n",
      "  5.66080689e-06 -3.20706704e-06 -5.13594205e-06  2.60366619e-07\n",
      " -5.04853597e-06  5.67473194e-07 -1.27831868e-06  1.05099725e-05\n",
      "  2.51149886e-05  1.36354047e-05  4.55045347e-06 -5.32511604e-06\n",
      " -1.03924976e-05  5.93017648e-06 -2.06645541e-06  9.17013676e-06\n",
      " -1.56062163e-06  3.68615088e-06 -6.11363021e-06  2.74867489e-06\n",
      " -5.76815455e-06  3.02566917e-07  6.72401029e-06  6.19992670e-06\n",
      "  2.27275341e-05 -4.60409666e-07  4.01271700e-06  1.29781010e-05\n",
      "  8.40746623e-07  4.18736499e-06 -1.18427579e-05  8.35632818e-06\n",
      " -9.99217946e-07  1.26156128e-05 -4.72295551e-06  4.37921608e-06\n",
      "  4.40576105e-06  2.54276765e-05 -2.89447348e-06 -4.00021690e-06\n",
      "  8.43199905e-06 -1.02219756e-05 -1.68555016e-06  7.03699561e-06\n",
      "  8.22410038e-06 -1.09058501e-05  5.99661371e-06  4.84531029e-06\n",
      " -9.09469691e-06  7.52617780e-07 -5.22792698e-06 -1.53034853e-05]\n",
      "[ 1.14485738e-05 -7.39872712e-06  2.88140632e-06 -1.04696082e-05\n",
      "  1.06545592e-07 -5.95721394e-06  1.07120295e-05 -8.34006551e-06\n",
      "  2.55162413e-05  1.20099085e-05 -9.53638482e-06 -8.41740166e-06\n",
      "  5.01903060e-06  2.86964905e-06 -9.55119594e-06 -6.35280776e-06\n",
      " -7.70196311e-06  3.40840802e-06  2.84841985e-06  5.67158258e-06\n",
      "  2.38139201e-05  7.23719540e-06 -1.62847329e-07 -5.98418501e-06\n",
      " -9.18298701e-06  7.68814698e-06 -3.31339743e-06  5.86211172e-06\n",
      "  1.97381041e-06  7.22563470e-07 -8.32089154e-06  4.37113840e-06\n",
      " -4.14938677e-06 -5.39131361e-06  9.30312217e-06  6.36156437e-06\n",
      "  1.57230679e-05 -5.00325632e-06  5.59933278e-06  1.19578417e-05\n",
      "  5.91738490e-06  2.80945369e-06 -6.99232669e-06  6.28356656e-06\n",
      "  1.28327247e-06  1.00030811e-05 -2.51583492e-06 -6.03982608e-06\n",
      "  2.50780681e-06  1.65045894e-05  6.40549274e-07 -1.68191514e-06\n",
      "  9.31645081e-06 -1.33362992e-05  2.28045928e-06  1.48097217e-06\n",
      "  5.43852002e-06 -9.11374264e-06  1.01596606e-05  9.52423761e-06\n",
      " -6.33693253e-06 -8.98402050e-07 -9.25613949e-06 -1.48936460e-05]\n",
      "[ 1.11695335e-05 -6.31440798e-06  1.06004620e-06 -1.02075855e-05\n",
      " -1.38549512e-06 -9.29647740e-06  9.77759737e-06 -8.28418251e-06\n",
      "  2.06136465e-05  1.23369218e-05 -1.30682793e-05 -1.10653309e-05\n",
      "  2.88418573e-06  7.09385495e-06 -1.42551180e-05 -5.14797239e-06\n",
      " -5.37125470e-06  1.49158768e-06  4.08683991e-06  2.71791623e-06\n",
      "  2.83230984e-05  8.33709964e-06 -8.31035241e-09 -7.52530104e-06\n",
      " -9.07676349e-06  7.87587760e-06 -5.36839480e-06  2.98957798e-06\n",
      "  1.16836316e-06 -3.94975331e-07 -1.13808192e-05  4.73754108e-06\n",
      " -6.52082963e-06 -6.54618634e-06  8.77303683e-06  6.63510264e-06\n",
      "  2.05591587e-05 -3.77730248e-06  4.34722006e-06  1.28483171e-05\n",
      "  8.11165319e-06  2.41357498e-06 -2.84741782e-06  5.00160286e-06\n",
      "  1.16281353e-06  1.09659504e-05 -2.66758548e-06 -7.54319944e-06\n",
      "  2.85622536e-06  1.40440161e-05  1.53364590e-06 -3.06147695e-06\n",
      "  7.21140850e-06 -1.54377776e-05  1.93064352e-06  2.88274055e-06\n",
      "  2.03432592e-06 -8.38284814e-06  7.16784007e-06  9.92564583e-06\n",
      " -2.59347757e-06 -1.75603600e-06 -1.08151589e-05 -1.00717116e-05]\n",
      "[ 1.30260796e-05 -4.30462677e-07 -7.92693481e-06 -1.58008140e-06\n",
      "  7.40913856e-06 -1.66040700e-05  4.26664701e-06  3.10365806e-07\n",
      "  2.27072269e-05  1.39410067e-05 -1.49044799e-05 -8.44354327e-06\n",
      " -1.04545471e-07  1.40003185e-05 -1.63680106e-05  3.86343487e-07\n",
      "  7.80343782e-07  1.58671196e-06  4.37266726e-06 -1.39522939e-07\n",
      "  2.55392824e-05  4.25341705e-06  3.05821663e-06  1.72782961e-06\n",
      " -2.59979806e-05  8.64242520e-06 -4.10708481e-06 -4.14813121e-06\n",
      "  3.87999762e-06  2.31743957e-06 -1.34732400e-05  2.91406286e-06\n",
      " -8.66497430e-06 -4.81136476e-06  1.89571961e-06  3.47900755e-06\n",
      "  2.81760840e-05 -8.39369568e-06  9.32839703e-06  1.09357443e-05\n",
      "  5.59982709e-06  5.46615502e-06 -5.14120529e-06  5.82769053e-06\n",
      " -1.01237511e-05  9.59464342e-06 -4.78970605e-06 -3.87533237e-06\n",
      " -4.01620639e-07  1.25121842e-05 -3.26947634e-06  1.39580550e-06\n",
      "  5.62581909e-06 -1.73649987e-05 -1.94812924e-06  8.68469488e-06\n",
      "  2.38439043e-06 -8.78727769e-06  2.90564503e-06  1.00167199e-05\n",
      " -4.87394755e-06 -6.64523668e-06 -1.16494302e-05 -1.07547203e-05]\n",
      "[ 5.6067743e-06 -1.3164108e-05 -1.0572093e-05 -1.5519106e-06\n",
      "  7.1028908e-06 -2.4500723e-05  5.8454734e-06 -1.3703920e-05\n",
      "  3.4219029e-05  6.9572397e-06 -1.1498998e-05 -6.0216926e-06\n",
      "  1.7908480e-06 -7.0956969e-07 -3.1904121e-05 -5.8641599e-06\n",
      " -2.9419036e-06  1.1329454e-05  1.0811257e-05  6.9447528e-06\n",
      "  4.0663297e-05  9.3984654e-06 -1.4461282e-06 -1.8989193e-05\n",
      " -1.4309230e-05  1.7276319e-05 -2.0684345e-06  9.2379321e-07\n",
      "  1.0674812e-06  1.0384535e-05 -1.5330298e-06 -6.1388700e-06\n",
      " -1.0208032e-05 -8.2719731e-07  1.6323767e-05  1.2846114e-05\n",
      "  2.8718874e-05 -3.3907957e-06 -1.0829936e-05  2.0873567e-05\n",
      "  5.7187467e-06 -6.3819703e-06  6.2080990e-06  1.8116501e-05\n",
      " -8.1875196e-06  1.7798982e-05 -3.4958800e-06 -1.1234117e-05\n",
      "  6.6466359e-06  1.5480653e-05  8.0502587e-06  1.8014886e-06\n",
      " -1.1828616e-05 -1.6581922e-05  1.1582523e-05 -1.2241035e-06\n",
      " -7.9932406e-06 -3.4704788e-06  6.1340957e-06  1.4615602e-05\n",
      "  1.5031410e-05 -1.0672862e-06 -3.4937261e-06 -1.2053873e-05]\n",
      "[ 1.8231348e-06 -6.0893062e-06 -3.8940730e-06 -4.2665937e-07\n",
      "  3.0250173e-06 -5.6720087e-06  2.2610031e-06 -3.3300582e-06\n",
      "  2.0579577e-05  2.2878319e-06 -1.9944334e-06 -1.5482408e-06\n",
      "  1.1083647e-06  6.5467128e-07 -7.7952200e-06 -4.9156929e-06\n",
      " -1.9017093e-06  5.4083557e-07  2.8379711e-06  4.2948759e-08\n",
      "  2.8343939e-05  2.6001594e-06  2.6479286e-06 -3.8571980e-06\n",
      " -1.1295783e-05  6.1631790e-06 -1.1089825e-06  7.6301114e-07\n",
      "  1.0072318e-07  4.5313791e-06 -1.4955206e-06  1.6866557e-06\n",
      " -6.6085936e-06 -7.0055080e-07  7.5708440e-06  5.2618693e-06\n",
      "  2.0119040e-05  9.2912893e-07 -1.5687924e-06  1.5058242e-05\n",
      " -5.4621256e-07 -3.4601585e-06 -3.9539327e-06  1.1007978e-05\n",
      " -3.5670660e-06  5.2618138e-06 -1.0267390e-06 -8.1228036e-06\n",
      "  5.0523122e-06  1.5841340e-05  6.8347113e-08  1.1501619e-06\n",
      " -3.2517955e-06 -5.0462818e-06  1.9382903e-06 -7.4842097e-07\n",
      " -9.2172928e-07 -6.6398661e-06  3.6135780e-06  8.9143778e-06\n",
      "  5.1164016e-06 -4.7618155e-08 -7.2340358e-06 -7.5625835e-06]\n",
      "[ 6.59323860e-06 -1.11688505e-05 -1.38852929e-05 -2.37147879e-06\n",
      "  6.63808669e-06 -2.30946462e-05  2.89197715e-06 -9.39803886e-06\n",
      "  4.27012819e-05  8.52744779e-06 -8.34845196e-06 -1.57474778e-06\n",
      "  3.38309815e-06  3.79483640e-06 -1.13510932e-05 -1.31064235e-05\n",
      " -1.90114088e-06 -1.73626586e-06  4.62135949e-06 -1.89392415e-06\n",
      "  3.95415991e-05  8.22511993e-07  5.21092807e-06 -8.56083079e-06\n",
      " -1.57041359e-05  3.60716717e-06 -1.01868159e-06 -3.22871142e-06\n",
      "  9.62156037e-07  9.80955519e-06 -1.11759273e-05  3.53451355e-06\n",
      " -1.56493788e-05 -4.87375473e-07  1.30941899e-05  7.54243865e-06\n",
      "  3.35003278e-05 -1.15083719e-06 -1.33842047e-06  2.37876066e-05\n",
      "  2.38008784e-06 -1.15383773e-06 -1.04848305e-05  1.37654988e-05\n",
      " -1.27859585e-05  1.40662196e-05  2.18257583e-06 -1.13143133e-05\n",
      "  1.14946870e-05  2.06124460e-05 -8.71175928e-07  1.91456888e-06\n",
      " -1.35264656e-06 -1.44955757e-05  4.13667067e-06 -1.19851006e-06\n",
      " -2.82518272e-07 -9.31996146e-06  4.00935960e-06  1.31046299e-05\n",
      "  5.45403691e-06 -8.14162775e-07 -1.35273876e-05 -1.18811540e-05]\n",
      "[ 8.50945617e-06 -7.91884395e-06 -1.12554471e-05 -6.15411864e-07\n",
      "  3.55836823e-06 -2.22548933e-05  4.28574367e-06 -7.61888623e-06\n",
      "  3.10800824e-05  8.50907691e-06 -1.02806043e-05 -3.62813876e-06\n",
      "  7.41790075e-07  7.63115349e-06 -1.40306802e-05 -9.45227930e-06\n",
      " -4.52223219e-07 -2.34430399e-06  4.84404472e-06 -4.31341414e-06\n",
      "  3.41134728e-05 -6.12141946e-07  4.69746692e-06 -9.74246177e-06\n",
      " -1.51802415e-05  4.46843433e-06 -3.55806833e-06 -5.05626213e-06\n",
      "  1.09722706e-07  7.38568042e-06 -1.13513997e-05  5.05411026e-06\n",
      " -1.67077742e-05 -2.91016022e-06  1.25562628e-05  6.53111192e-06\n",
      "  2.99265230e-05 -4.23708457e-07 -2.86792556e-06  1.82172735e-05\n",
      "  3.56398095e-06 -8.31715113e-07 -7.70729184e-06  1.11293293e-05\n",
      " -1.01293263e-05  1.02846880e-05  1.71844647e-06 -1.04023429e-05\n",
      "  1.06816160e-05  1.65002057e-05 -8.49208277e-07 -1.87340163e-07\n",
      " -3.48284357e-06 -1.31484267e-05  3.33468961e-06  3.12106721e-07\n",
      " -3.48882122e-06 -5.94081030e-06  1.79041297e-06  1.23563614e-05\n",
      "  7.76252000e-06 -1.99090482e-06 -1.25487113e-05 -8.78209903e-06]\n",
      "[ 1.0119460e-05 -6.5304962e-06 -6.6862804e-06  1.8189896e-06\n",
      "  5.0772624e-06 -1.8888752e-05  3.4605055e-06 -2.9292628e-06\n",
      "  2.0163794e-05  7.7757622e-06 -7.2735352e-06 -5.8302862e-06\n",
      " -8.7401298e-07  1.2795580e-05 -1.1515167e-05 -7.7900095e-06\n",
      "  4.0308837e-06 -3.8834896e-06  1.4185634e-06 -7.6079191e-06\n",
      "  2.4735686e-05  3.4197765e-07  3.7070320e-06 -4.9763048e-06\n",
      " -1.2291361e-05  3.0416713e-06 -5.3270701e-06 -4.1367293e-06\n",
      "  3.1294161e-07  4.4338485e-06 -9.4023599e-06  3.1375175e-06\n",
      " -1.1030647e-05 -3.3306828e-06  9.6348713e-06  5.0633853e-06\n",
      "  2.2449914e-05 -2.1701594e-06 -1.8565203e-06  1.4107612e-05\n",
      "  4.7579611e-06  2.1298356e-06 -4.1779986e-06  5.5779506e-06\n",
      " -5.1952370e-06  9.5810738e-06  9.1998601e-07 -9.2730115e-06\n",
      "  6.6633315e-06  1.2078391e-05  1.0851087e-06 -2.1678286e-06\n",
      " -4.7863177e-06 -1.2007831e-05  3.7216573e-06  1.8060268e-06\n",
      " -1.5772047e-06 -5.8962901e-06  1.4944723e-07  8.9193227e-06\n",
      "  9.2744613e-06 -4.6438240e-06 -1.1428297e-05 -4.3385899e-06]\n",
      "[ 1.1185333e-05 -5.9443123e-06 -4.1044241e-06  3.0700403e-06\n",
      "  5.9158201e-06 -1.8886958e-05  2.8477014e-06 -4.9319698e-07\n",
      "  1.5105416e-05  7.6999795e-06 -5.8991859e-06 -7.3440974e-06\n",
      " -1.4997788e-06  1.6171025e-05 -1.0045592e-05 -7.4205254e-06\n",
      "  7.1498762e-06 -5.3327190e-06 -6.0424009e-07 -1.0148891e-05\n",
      "  2.1228134e-05  1.7344640e-06  3.4342117e-06 -2.5783436e-06\n",
      " -1.0704293e-05  2.2001516e-06 -6.2345098e-06 -3.6176318e-06\n",
      "  3.6678185e-07  2.8726788e-06 -8.6497030e-06  1.7539223e-06\n",
      " -8.3997866e-06 -3.0635490e-06  8.3332006e-06  4.5057382e-06\n",
      "  2.0126370e-05 -3.5670153e-06 -1.2157273e-07  1.2161736e-05\n",
      "  5.8671776e-06  3.8389221e-06 -2.2361412e-06  2.2583422e-06\n",
      " -2.5047723e-06  9.7453576e-06  5.1743973e-07 -9.2989276e-06\n",
      "  4.3206660e-06  9.3098824e-06  2.1143933e-06 -3.3186820e-06\n",
      " -5.6111903e-06 -1.1699303e-05  3.8930889e-06  2.8405416e-06\n",
      " -7.8629404e-07 -6.4193464e-06 -7.5812500e-07  7.1872537e-06\n",
      "  1.0296459e-05 -6.3124694e-06 -1.1538252e-05 -1.3075210e-06]\n",
      "[ 1.25076658e-05 -2.13155681e-06 -3.45078388e-06  1.13724627e-06\n",
      "  5.31887054e-06 -1.33784633e-05 -1.37205234e-06 -2.15813498e-06\n",
      "  1.24197186e-05  1.11208146e-05 -1.05788449e-05 -4.91062019e-06\n",
      "  2.27497912e-06  1.58821713e-05 -9.80106961e-06 -5.46937872e-06\n",
      "  6.20447099e-06 -2.32015714e-06  5.41390591e-06 -5.94488256e-06\n",
      "  1.95235407e-05  6.23651431e-06  4.25072017e-07 -1.08771917e-06\n",
      " -7.63338448e-06  2.09863515e-06 -2.95555787e-06 -4.24255086e-06\n",
      "  3.28002307e-06  2.21889968e-06 -9.34710533e-06  4.39920586e-06\n",
      " -6.24624954e-06 -4.98353984e-06  4.14685519e-06  3.43616944e-06\n",
      "  2.13308012e-05 -7.11844950e-06  1.45520107e-06  4.68956569e-06\n",
      "  3.56698320e-06  8.66312348e-06  6.45714010e-07 -1.73987155e-06\n",
      " -3.23566769e-06  4.25829785e-06  4.46256934e-07 -8.59577085e-06\n",
      "  7.92045796e-07  5.03685760e-06  1.42815202e-06 -8.75245746e-07\n",
      " -2.54470251e-06 -8.33851936e-06  1.66096413e-06  9.75061539e-07\n",
      " -3.71106330e-06 -6.65141124e-06  1.07319033e-06  5.20182630e-06\n",
      "  7.80412211e-06 -3.03405113e-06 -1.09054845e-05 -1.85662793e-06]\n",
      "[ 1.43960979e-05 -4.91383958e-07 -3.99520104e-06  3.92202253e-07\n",
      "  5.92555261e-06 -1.49586958e-05 -3.23105428e-06 -3.24567668e-06\n",
      "  1.41522505e-05  1.37994302e-05 -1.41645332e-05 -4.99153293e-06\n",
      "  4.11796600e-06  1.69464365e-05 -1.22317897e-05 -5.72167073e-06\n",
      "  6.88924320e-06 -1.28786473e-06  9.13174790e-06 -5.55368297e-06\n",
      "  2.21692535e-05  9.35345270e-06 -7.46412411e-07 -7.45383488e-07\n",
      " -8.10597612e-06  2.15719274e-06 -2.69876432e-06 -4.83687882e-06\n",
      "  4.84635575e-06  2.32458638e-06 -1.14792874e-05  6.37659832e-06\n",
      " -6.46060198e-06 -6.25717894e-06  2.67240534e-06  3.50726100e-06\n",
      "  2.36219912e-05 -9.84599137e-06  2.47488310e-06  3.45544254e-06\n",
      "  3.82345479e-06  1.19844117e-05  2.29765465e-06 -3.93454548e-06\n",
      " -4.48022092e-06  3.48275012e-06  3.98328268e-07 -8.68950519e-06\n",
      " -7.84019790e-07  4.36348455e-06  1.51970539e-06  1.44914196e-07\n",
      " -2.21605410e-06 -8.92979551e-06  8.37407299e-07  3.58695502e-07\n",
      " -5.75636477e-06 -7.49977016e-06  2.17771640e-06  5.28237706e-06\n",
      "  7.60020112e-06 -2.63856145e-06 -1.18116641e-05 -2.12715986e-06]\n",
      "[ 1.5823451e-05  3.6824329e-07 -4.0900776e-08 -3.9742781e-06\n",
      "  6.6131079e-06 -2.5377831e-05 -5.1855627e-06 -5.7246843e-06\n",
      "  1.6238931e-05  1.6437934e-05 -1.4915078e-05 -6.3401585e-06\n",
      "  4.3620307e-06  1.6011896e-05 -1.4559081e-05 -6.2869985e-06\n",
      "  8.6237706e-06 -2.1206806e-06  1.5068755e-05 -5.9372287e-06\n",
      "  2.6247637e-05  6.4403803e-06  3.3553921e-10  2.6163843e-07\n",
      " -9.9495310e-06  4.0596287e-06 -4.7503659e-06 -4.0732693e-06\n",
      "  4.8759903e-06  2.3844614e-06 -1.5829673e-05  7.2115677e-06\n",
      " -8.2009283e-06 -8.2417173e-06  3.4494863e-06  6.0013340e-06\n",
      "  2.2104996e-05 -1.1625048e-05 -5.3505159e-06  3.0293641e-06\n",
      "  7.4922518e-06  1.2635469e-05  6.2171580e-06 -6.7344577e-06\n",
      " -4.5713177e-06  3.9286579e-06  5.3122983e-07 -1.1300226e-05\n",
      "  1.6025821e-06  3.0850424e-06  2.8374006e-06  1.7467552e-06\n",
      " -3.3247220e-06 -1.0103045e-05  1.2626837e-06  2.6141884e-06\n",
      " -1.3586175e-05 -8.0548289e-06  4.3549633e-07  6.3449020e-06\n",
      "  7.8465446e-06 -1.8988889e-06 -1.0770649e-05 -2.6756652e-06]\n",
      "[ 1.31242114e-05 -2.57596889e-06 -5.31640262e-06 -4.93647212e-06\n",
      "  7.07456093e-06 -2.01409639e-05 -6.61429510e-07 -4.09019503e-06\n",
      "  1.98220660e-05  1.19464139e-05 -1.27494168e-05 -2.36596088e-06\n",
      "  3.26979762e-06  1.41616829e-05 -1.00142370e-05 -1.13893411e-05\n",
      "  4.43058707e-06 -2.54262022e-06  1.15325065e-05 -3.61478419e-06\n",
      "  2.95673344e-05  3.76383082e-06  2.40621648e-06  3.83882622e-07\n",
      " -1.52270522e-05  1.29473995e-06 -1.68302790e-06 -5.42447742e-06\n",
      "  2.10595408e-06  2.50020366e-06 -1.61373719e-05  2.50919447e-06\n",
      " -9.54471398e-06 -4.55081499e-06  2.60105116e-06  5.90705349e-06\n",
      "  2.74690174e-05 -8.55247345e-06 -1.69145653e-06  3.02484546e-06\n",
      "  8.13838687e-06  1.01718178e-05 -1.35815981e-06 -4.03997234e-07\n",
      " -8.36799336e-06  5.72980935e-06  2.81503094e-06 -7.07972367e-06\n",
      "  3.31810293e-06  1.02101731e-05 -6.14449959e-07  1.06773405e-06\n",
      " -3.08092126e-07 -1.36099507e-05  6.98972713e-07  2.45128422e-06\n",
      " -8.14441046e-06 -6.32697220e-06  1.26621171e-06  4.64464074e-06\n",
      "  5.06886863e-06 -2.00037266e-06 -6.85449550e-06 -4.36326400e-06]\n",
      "[-0. -0.  0. -0.  0. -0.  0. -0.  0.  0. -0. -0.  0.  0. -0.  0. -0.  0.\n",
      "  0.  0.  0.  0.  0. -0. -0.  0.  0.  0. -0.  0. -0.  0. -0.  0.  0.  0.\n",
      "  0. -0.  0.  0.  0. -0. -0.  0. -0.  0. -0.  0.  0.  0. -0. -0.  0. -0.\n",
      "  0.  0.  0. -0.  0.  0.  0. -0. -0. -0.]\n",
      "[-0. -0.  0. -0.  0. -0.  0. -0.  0.  0. -0. -0.  0.  0. -0.  0. -0.  0.\n",
      "  0.  0.  0.  0.  0. -0. -0.  0.  0.  0. -0.  0. -0.  0. -0.  0.  0.  0.\n",
      "  0. -0.  0.  0.  0. -0. -0.  0. -0.  0. -0.  0.  0.  0. -0. -0.  0. -0.\n",
      "  0.  0.  0. -0.  0.  0.  0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "x_predict = test_dataset['x'][:1]\n",
    "predictions = model.predict(x_predict)\n",
    "print(predictions[0][0]) # 1\n",
    "print(predictions[0][1]) # 2 (1st row)\n",
    "print(predictions[0][2]) # 1\n",
    "print(predictions[0][3]) # 2 (2nd row)\n",
    "print(predictions[0][4]) # 1\n",
    "print(predictions[0][5]) # 2 (3rd row)\n",
    "print(predictions[0][6]) # 1\n",
    "print(predictions[0][7]) # 2 (4th row)\n",
    "print(predictions[0][8]) # 1\n",
    "print(predictions[0][9]) # 2 (5th row)\n",
    "print(predictions[0][10])# 1\n",
    "print(predictions[0][11])# 2 (6th row)\n",
    "print(predictions[0][12])# 1\n",
    "print(predictions[0][13])# 2 (7th row)\n",
    "print(predictions[0][14])# 1\n",
    "print(predictions[0][15])# 2 (8th row)\n",
    "print(predictions[0][788])# 1\n",
    "print(predictions[0][789])# 2 (OFFSET 0xC50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0000000e+00 -0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -0.0000000e+00 -0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  1.3063593e-15 -0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00 -0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00 -0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00  0.0000000e+00  0.0000000e+00 -0.0000000e+00\n",
      "  0.0000000e+00  5.1961989e-18  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00 -0.0000000e+00 -0.0000000e+00  0.0000000e+00\n",
      " -0.0000000e+00 -0.0000000e+00 -0.0000000e+00 -0.0000000e+00]\n",
      "[ 0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00  0.000000e+00\n",
      " -0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00  0.000000e+00\n",
      " -0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 -0.000000e+00\n",
      " -0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 -0.000000e+00\n",
      "  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 -0.000000e+00\n",
      "  0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00  0.000000e+00\n",
      " -0.000000e+00  0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00\n",
      "  0.000000e+00  0.000000e+00 -0.000000e+00  0.000000e+00  0.000000e+00\n",
      "  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 -0.000000e+00\n",
      "  0.000000e+00  0.000000e+00 -0.000000e+00  0.000000e+00  0.000000e+00\n",
      "  0.000000e+00  0.000000e+00  0.000000e+00 -0.000000e+00  0.000000e+00\n",
      "  0.000000e+00 -0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00\n",
      " -2.416666e-21 -0.000000e+00 -0.000000e+00 -0.000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0][212])# 1\n",
    "print(predictions[0][213])# 2 (OFFSET 0xD50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
