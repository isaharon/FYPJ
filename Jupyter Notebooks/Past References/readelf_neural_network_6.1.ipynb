{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of seed files collected from: 20 (Split 12/4/4)\n",
      "Dataset collected limited to 100\n",
      "Ratio of seed file to mutated files = 1:100\n",
      "Training shape:  (1200, 3840, 64) (1200, 3840, 64)\n",
      "Validation shape:  (400, 3840, 64) (400, 3840, 64)\n",
      "Test shape:  (400, 3840, 64) (400, 3840, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "train = \"Data/readelf/train.npz\"\n",
    "val = \"Data/readelf/val.npz\"\n",
    "test = \"Data/readelf/test.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "training_dataset = np.load(train)\n",
    "val_dataset = np.load(val)\n",
    "test_dataset = np.load(test)\n",
    "\n",
    "x_train = training_dataset['x']\n",
    "y_train = training_dataset['y']\n",
    "\n",
    "x_val = val_dataset['x']\n",
    "y_val = val_dataset['y']\n",
    "\n",
    "x_test = test_dataset['x']\n",
    "y_test = test_dataset['y']\n",
    "\n",
    "print(\"No. of seed files collected from: 20 (Split 12/4/4)\")\n",
    "print(\"Dataset collected limited to 100\")\n",
    "print(\"Ratio of seed file to mutated files = 1:100\")\n",
    "\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, x_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 3840, 64)          33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3840, 64)          4160      \n",
      "=================================================================\n",
      "Total params: 37,184\n",
      "Trainable params: 37,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1200 samples, validate on 400 samples\n",
      "Epoch 1/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.4973 - mean_absolute_error: 0.4973 - val_loss: 0.4965 - val_mean_absolute_error: 0.4965\n",
      "Epoch 2/100\n",
      "1200/1200 [==============================] - 224s 187ms/step - loss: 0.4916 - mean_absolute_error: 0.4916 - val_loss: 0.4921 - val_mean_absolute_error: 0.4921\n",
      "Epoch 3/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.4835 - mean_absolute_error: 0.4835 - val_loss: 0.4854 - val_mean_absolute_error: 0.4854\n",
      "Epoch 4/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.4705 - mean_absolute_error: 0.4705 - val_loss: 0.4741 - val_mean_absolute_error: 0.4741\n",
      "Epoch 5/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.4500 - mean_absolute_error: 0.4500 - val_loss: 0.4523 - val_mean_absolute_error: 0.4523\n",
      "Epoch 6/100\n",
      "1200/1200 [==============================] - 222s 185ms/step - loss: 0.3854 - mean_absolute_error: 0.3854 - val_loss: 0.3257 - val_mean_absolute_error: 0.3257\n",
      "Epoch 7/100\n",
      "1200/1200 [==============================] - 224s 187ms/step - loss: 0.2935 - mean_absolute_error: 0.2935 - val_loss: 0.2714 - val_mean_absolute_error: 0.2714\n",
      "Epoch 8/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.2478 - mean_absolute_error: 0.2478 - val_loss: 0.2313 - val_mean_absolute_error: 0.2313\n",
      "Epoch 9/100\n",
      "1200/1200 [==============================] - 222s 185ms/step - loss: 0.2114 - mean_absolute_error: 0.2114 - val_loss: 0.1980 - val_mean_absolute_error: 0.1980\n",
      "Epoch 10/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.1804 - mean_absolute_error: 0.1804 - val_loss: 0.1690 - val_mean_absolute_error: 0.1690\n",
      "Epoch 11/100\n",
      "1200/1200 [==============================] - 223s 185ms/step - loss: 0.1536 - mean_absolute_error: 0.1536 - val_loss: 0.1438 - val_mean_absolute_error: 0.1438\n",
      "Epoch 12/100\n",
      "1200/1200 [==============================] - 222s 185ms/step - loss: 0.1308 - mean_absolute_error: 0.1308 - val_loss: 0.1223 - val_mean_absolute_error: 0.1223\n",
      "Epoch 13/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.1115 - mean_absolute_error: 0.1115 - val_loss: 0.1041 - val_mean_absolute_error: 0.1041\n",
      "Epoch 14/100\n",
      "1200/1200 [==============================] - 223s 185ms/step - loss: 0.0958 - mean_absolute_error: 0.0958 - val_loss: 0.0899 - val_mean_absolute_error: 0.0899\n",
      "Epoch 15/100\n",
      "1200/1200 [==============================] - 223s 185ms/step - loss: 0.0837 - mean_absolute_error: 0.0837 - val_loss: 0.0790 - val_mean_absolute_error: 0.0790\n",
      "Epoch 16/100\n",
      "1200/1200 [==============================] - 222s 185ms/step - loss: 0.0743 - mean_absolute_error: 0.0743 - val_loss: 0.0705 - val_mean_absolute_error: 0.0705\n",
      "Epoch 17/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.0668 - mean_absolute_error: 0.0668 - val_loss: 0.0635 - val_mean_absolute_error: 0.0635\n",
      "Epoch 18/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.0606 - mean_absolute_error: 0.0606 - val_loss: 0.0577 - val_mean_absolute_error: 0.0577\n",
      "Epoch 19/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.0553 - mean_absolute_error: 0.0553 - val_loss: 0.0528 - val_mean_absolute_error: 0.0528\n",
      "Epoch 20/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.0485 - val_mean_absolute_error: 0.0485\n",
      "Epoch 21/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.0469 - mean_absolute_error: 0.0469 - val_loss: 0.0448 - val_mean_absolute_error: 0.0448\n",
      "Epoch 22/100\n",
      "1200/1200 [==============================] - 223s 185ms/step - loss: 0.0435 - mean_absolute_error: 0.0435 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
      "Epoch 23/100\n",
      "1200/1200 [==============================] - 224s 186ms/step - loss: 0.0406 - mean_absolute_error: 0.0406 - val_loss: 0.0388 - val_mean_absolute_error: 0.0388\n",
      "Epoch 24/100\n",
      "1200/1200 [==============================] - 222s 185ms/step - loss: 0.0380 - mean_absolute_error: 0.0380 - val_loss: 0.0363 - val_mean_absolute_error: 0.0363\n",
      "Epoch 25/100\n",
      "1200/1200 [==============================] - 226s 188ms/step - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0341 - val_mean_absolute_error: 0.0341\n",
      "Epoch 26/100\n",
      "1200/1200 [==============================] - 225s 188ms/step - loss: 0.0336 - mean_absolute_error: 0.0336 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321\n",
      "Epoch 27/100\n",
      "1200/1200 [==============================] - 222s 185ms/step - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0303 - val_mean_absolute_error: 0.0303\n",
      "Epoch 28/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0287 - val_mean_absolute_error: 0.0287\n",
      "Epoch 29/100\n",
      "1200/1200 [==============================] - 223s 186ms/step - loss: 0.0284 - mean_absolute_error: 0.0284 - val_loss: 0.0271 - val_mean_absolute_error: 0.0271\n",
      "Epoch 30/100\n",
      "1200/1200 [==============================] - 228s 190ms/step - loss: 0.0269 - mean_absolute_error: 0.0269 - val_loss: 0.0257 - val_mean_absolute_error: 0.0257\n",
      "Epoch 31/100\n",
      "1200/1200 [==============================] - 220s 183ms/step - loss: 0.0256 - mean_absolute_error: 0.0256 - val_loss: 0.0245 - val_mean_absolute_error: 0.0245\n",
      "Epoch 32/100\n",
      "1200/1200 [==============================] - 218s 182ms/step - loss: 0.0244 - mean_absolute_error: 0.0244 - val_loss: 0.0233 - val_mean_absolute_error: 0.0233\n",
      "Epoch 33/100\n",
      "1200/1200 [==============================] - 222s 185ms/step - loss: 0.0232 - mean_absolute_error: 0.0232 - val_loss: 0.0222 - val_mean_absolute_error: 0.0222\n",
      "Epoch 34/100\n",
      "1200/1200 [==============================] - 220s 183ms/step - loss: 0.0222 - mean_absolute_error: 0.0222 - val_loss: 0.0212 - val_mean_absolute_error: 0.0212\n",
      "Epoch 35/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0212 - mean_absolute_error: 0.0212 - val_loss: 0.0202 - val_mean_absolute_error: 0.0202\n",
      "Epoch 36/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0203 - mean_absolute_error: 0.0203 - val_loss: 0.0194 - val_mean_absolute_error: 0.0194\n",
      "Epoch 37/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0194 - mean_absolute_error: 0.0194 - val_loss: 0.0185 - val_mean_absolute_error: 0.0185\n",
      "Epoch 38/100\n",
      "1200/1200 [==============================] - 218s 182ms/step - loss: 0.0187 - mean_absolute_error: 0.0187 - val_loss: 0.0178 - val_mean_absolute_error: 0.0178\n",
      "Epoch 39/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0179 - mean_absolute_error: 0.0179 - val_loss: 0.0170 - val_mean_absolute_error: 0.0170\n",
      "Epoch 40/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0172 - mean_absolute_error: 0.0172 - val_loss: 0.0164 - val_mean_absolute_error: 0.0164\n",
      "Epoch 41/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0165 - mean_absolute_error: 0.0165 - val_loss: 0.0157 - val_mean_absolute_error: 0.0157\n",
      "Epoch 42/100\n",
      "1200/1200 [==============================] - 218s 182ms/step - loss: 0.0159 - mean_absolute_error: 0.0159 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151\n",
      "Epoch 43/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0153 - mean_absolute_error: 0.0153 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145\n",
      "Epoch 44/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0148 - mean_absolute_error: 0.0148 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0142 - mean_absolute_error: 0.0142 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135\n",
      "Epoch 46/100\n",
      "1200/1200 [==============================] - 220s 183ms/step - loss: 0.0137 - mean_absolute_error: 0.0137 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130\n",
      "Epoch 47/100\n",
      "1200/1200 [==============================] - 217s 180ms/step - loss: 0.0133 - mean_absolute_error: 0.0133 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125\n",
      "Epoch 48/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0128 - mean_absolute_error: 0.0128 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121\n",
      "Epoch 49/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0124 - mean_absolute_error: 0.0124 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117\n",
      "Epoch 50/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0120 - mean_absolute_error: 0.0120 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113\n",
      "Epoch 51/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0116 - mean_absolute_error: 0.0116 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109\n",
      "Epoch 52/100\n",
      "1200/1200 [==============================] - 219s 182ms/step - loss: 0.0112 - mean_absolute_error: 0.0112 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106\n",
      "Epoch 53/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0109 - mean_absolute_error: 0.0109 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102\n",
      "Epoch 54/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0106 - mean_absolute_error: 0.0106 - val_loss: 0.0099 - val_mean_absolute_error: 0.0099\n",
      "Epoch 55/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0102 - mean_absolute_error: 0.0102 - val_loss: 0.0096 - val_mean_absolute_error: 0.0096\n",
      "Epoch 56/100\n",
      "1200/1200 [==============================] - 215s 179ms/step - loss: 0.0099 - mean_absolute_error: 0.0099 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 57/100\n",
      "1200/1200 [==============================] - 219s 182ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 58/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0094 - mean_absolute_error: 0.0094 - val_loss: 0.0087 - val_mean_absolute_error: 0.0087\n",
      "Epoch 59/100\n",
      "1200/1200 [==============================] - 219s 182ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0085 - val_mean_absolute_error: 0.0085\n",
      "Epoch 60/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0089 - mean_absolute_error: 0.0089 - val_loss: 0.0082 - val_mean_absolute_error: 0.0082\n",
      "Epoch 61/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0086 - mean_absolute_error: 0.0086 - val_loss: 0.0080 - val_mean_absolute_error: 0.0080\n",
      "Epoch 62/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0084 - mean_absolute_error: 0.0084 - val_loss: 0.0077 - val_mean_absolute_error: 0.0077\n",
      "Epoch 63/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0081 - mean_absolute_error: 0.0081 - val_loss: 0.0075 - val_mean_absolute_error: 0.0075\n",
      "Epoch 64/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0079 - mean_absolute_error: 0.0079 - val_loss: 0.0073 - val_mean_absolute_error: 0.0073\n",
      "Epoch 65/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0077 - mean_absolute_error: 0.0077 - val_loss: 0.0071 - val_mean_absolute_error: 0.0071\n",
      "Epoch 66/100\n",
      "1200/1200 [==============================] - 220s 184ms/step - loss: 0.0075 - mean_absolute_error: 0.0075 - val_loss: 0.0069 - val_mean_absolute_error: 0.0069\n",
      "Epoch 67/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0073 - mean_absolute_error: 0.0073 - val_loss: 0.0067 - val_mean_absolute_error: 0.0067\n",
      "Epoch 68/100\n",
      "1200/1200 [==============================] - 219s 183ms/step - loss: 0.0071 - mean_absolute_error: 0.0071 - val_loss: 0.0065 - val_mean_absolute_error: 0.0065\n",
      "Epoch 69/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0070 - mean_absolute_error: 0.0070 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
      "Epoch 70/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0068 - mean_absolute_error: 0.0068 - val_loss: 0.0062 - val_mean_absolute_error: 0.0062\n",
      "Epoch 71/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0066 - mean_absolute_error: 0.0066 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060\n",
      "Epoch 72/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0065 - mean_absolute_error: 0.0065 - val_loss: 0.0059 - val_mean_absolute_error: 0.0059\n",
      "Epoch 73/100\n",
      "1200/1200 [==============================] - 212s 177ms/step - loss: 0.0063 - mean_absolute_error: 0.0063 - val_loss: 0.0057 - val_mean_absolute_error: 0.0057\n",
      "Epoch 74/100\n",
      "1200/1200 [==============================] - 212s 176ms/step - loss: 0.0062 - mean_absolute_error: 0.0062 - val_loss: 0.0056 - val_mean_absolute_error: 0.0056\n",
      "Epoch 75/100\n",
      "1200/1200 [==============================] - 218s 181ms/step - loss: 0.0060 - mean_absolute_error: 0.0060 - val_loss: 0.0054 - val_mean_absolute_error: 0.0054\n",
      "Epoch 76/100\n",
      "1200/1200 [==============================] - 215s 179ms/step - loss: 0.0059 - mean_absolute_error: 0.0059 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053\n",
      "Epoch 77/100\n",
      "1200/1200 [==============================] - 215s 180ms/step - loss: 0.0057 - mean_absolute_error: 0.0057 - val_loss: 0.0051 - val_mean_absolute_error: 0.0051\n",
      "Epoch 78/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 79/100\n",
      "1200/1200 [==============================] - 218s 182ms/step - loss: 0.0055 - mean_absolute_error: 0.0055 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 80/100\n",
      "1200/1200 [==============================] - 215s 179ms/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0048 - val_mean_absolute_error: 0.0048\n",
      "Epoch 81/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0047 - val_mean_absolute_error: 0.0047\n",
      "Epoch 82/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045\n",
      "Epoch 83/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044\n",
      "Epoch 84/100\n",
      "1200/1200 [==============================] - 217s 180ms/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043\n",
      "Epoch 85/100\n",
      "1200/1200 [==============================] - 215s 180ms/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042\n",
      "Epoch 86/100\n",
      "1200/1200 [==============================] - 217s 181ms/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041\n",
      "Epoch 87/100\n",
      "1200/1200 [==============================] - 215s 180ms/step - loss: 0.0046 - mean_absolute_error: 0.0046 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040\n",
      "Epoch 88/100\n",
      "1200/1200 [==============================] - 215s 179ms/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0039 - val_mean_absolute_error: 0.0039\n",
      "Epoch 89/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0038 - val_mean_absolute_error: 0.0038\n",
      "Epoch 90/100\n",
      "1200/1200 [==============================] - 215s 179ms/step - loss: 0.0043 - mean_absolute_error: 0.0043 - val_loss: 0.0038 - val_mean_absolute_error: 0.0038\n",
      "Epoch 91/100\n",
      "1200/1200 [==============================] - 215s 179ms/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0037 - val_mean_absolute_error: 0.0037\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 220s 183ms/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0036 - val_mean_absolute_error: 0.0036\n",
      "Epoch 93/100\n",
      "1200/1200 [==============================] - 217s 180ms/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0035 - val_mean_absolute_error: 0.0035\n",
      "Epoch 94/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0034 - val_mean_absolute_error: 0.0034\n",
      "Epoch 95/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0034 - val_mean_absolute_error: 0.0034\n",
      "Epoch 96/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0033 - val_mean_absolute_error: 0.0033\n",
      "Epoch 97/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0032 - val_mean_absolute_error: 0.0032\n",
      "Epoch 98/100\n",
      "1200/1200 [==============================] - 216s 180ms/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0031 - val_mean_absolute_error: 0.0031\n",
      "Epoch 99/100\n",
      "1200/1200 [==============================] - 215s 179ms/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0031 - val_mean_absolute_error: 0.0031\n",
      "Epoch 100/100\n",
      "1200/1200 [==============================] - 218s 182ms/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0030 - val_mean_absolute_error: 0.0030\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(3840, 64), return_sequences=True))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error', metrics=['mae'])\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7eff20d3d898>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQwp3EZEY+v+yKnIT09/HcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff20c54550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# plot metrics\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 15s 38ms/step\n",
      "Test loss: [0.0031610911712050436, 0.003161091059446335]\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17865169 0.14213467 0.17927814 0.21294333 0.17830168 0.17682692\n",
      " 0.19136672 0.21069252 0.17176564 0.18923765 0.18141824 0.18507546\n",
      " 0.18156631 0.18416902 0.18459488 0.2271108  0.16668878 0.19885466\n",
      " 0.18004946 0.22962838 0.14717181 0.18809927 0.17021605 0.19302149\n",
      " 0.14770886 0.23624374 0.19731028 0.18297581 0.20148776 0.21629313\n",
      " 0.17354184 0.19736001 0.12708025 0.20266901 0.17210636 0.19010529\n",
      " 0.17664923 0.1725527  0.16351786 0.18916468 0.22901689 0.23535934\n",
      " 0.1665835  0.21275888 0.19129752 0.14567927 0.20266904 0.20582058\n",
      " 0.17682093 0.18910196 0.18339515 0.18070717 0.18371108 0.20092441\n",
      " 0.15683034 0.2146696  0.15238148 0.2057826  0.15737134 0.16089387\n",
      " 0.18354963 0.17092161 0.20576961 0.20182404]\n",
      "[0.12605384 0.11788929 0.13580592 0.15022135 0.1452184  0.13851392\n",
      " 0.1514621  0.15802509 0.13307025 0.14397487 0.13881922 0.14685374\n",
      " 0.13953437 0.15294161 0.13820943 0.17446077 0.12621866 0.15810868\n",
      " 0.14372829 0.18120311 0.11066235 0.14739238 0.11700618 0.147333\n",
      " 0.11958216 0.1903375  0.15121005 0.1439857  0.15015773 0.16521214\n",
      " 0.1346201  0.15167527 0.09828172 0.14476709 0.13297139 0.1519713\n",
      " 0.14992335 0.1428659  0.13101535 0.14635184 0.16061899 0.18643013\n",
      " 0.13632421 0.18525745 0.14549354 0.11565592 0.16464463 0.1532532\n",
      " 0.1402722  0.14018986 0.15115292 0.13606174 0.14174382 0.15029243\n",
      " 0.12490536 0.16350459 0.12228139 0.16213869 0.12523301 0.13421468\n",
      " 0.1483578  0.12975849 0.15952903 0.15149955]\n",
      "[0.0495306  0.04675987 0.05443958 0.06950029 0.0707728  0.06129672\n",
      " 0.06770105 0.07909101 0.06199845 0.06362548 0.05776887 0.06647678\n",
      " 0.06480359 0.07112234 0.06198793 0.09423506 0.0483509  0.0795907\n",
      " 0.05866402 0.08547556 0.042586   0.06314432 0.04284161 0.05895353\n",
      " 0.04462981 0.10028576 0.07172251 0.06987695 0.06956359 0.08385024\n",
      " 0.05910226 0.07659959 0.03870342 0.06243187 0.05986546 0.06620184\n",
      " 0.07482816 0.06167274 0.05750974 0.07170779 0.07084493 0.09537818\n",
      " 0.06064374 0.10096572 0.06924161 0.04623194 0.0849881  0.06659911\n",
      " 0.06279394 0.06472669 0.06894056 0.05225193 0.06236015 0.0616371\n",
      " 0.05110563 0.07822224 0.05039793 0.07950999 0.05441496 0.05983505\n",
      " 0.06742293 0.05143983 0.07341951 0.06552911]\n",
      "[0.17865169 0.14213467 0.17927814 0.21294333 0.17830168 0.17682692\n",
      " 0.19136672 0.21069252 0.17176564 0.18923765 0.18141824 0.18507546\n",
      " 0.18156631 0.18416902 0.18459488 0.2271108  0.16668878 0.19885466\n",
      " 0.18004946 0.22962838 0.14717181 0.18809927 0.17021605 0.19302149\n",
      " 0.14770886 0.23624374 0.19731028 0.18297581 0.20148776 0.21629313\n",
      " 0.17354184 0.19736001 0.12708025 0.20266901 0.17210636 0.19010529\n",
      " 0.17664923 0.1725527  0.16351786 0.18916468 0.22901689 0.23535934\n",
      " 0.1665835  0.21275888 0.19129752 0.14567927 0.20266904 0.20582058\n",
      " 0.17682093 0.18910196 0.18339515 0.18070717 0.18371108 0.20092441\n",
      " 0.15683034 0.2146696  0.15238148 0.2057826  0.15737134 0.16089387\n",
      " 0.18354963 0.17092161 0.20576961 0.20182404]\n",
      "[0.12605384 0.11788929 0.13580592 0.15022135 0.1452184  0.13851392\n",
      " 0.1514621  0.15802509 0.13307025 0.14397487 0.13881922 0.14685374\n",
      " 0.13953437 0.15294161 0.13820943 0.17446077 0.12621866 0.15810868\n",
      " 0.14372829 0.18120311 0.11066235 0.14739238 0.11700618 0.147333\n",
      " 0.11958216 0.1903375  0.15121005 0.1439857  0.15015773 0.16521214\n",
      " 0.1346201  0.15167527 0.09828172 0.14476709 0.13297139 0.1519713\n",
      " 0.14992335 0.1428659  0.13101535 0.14635184 0.16061899 0.18643013\n",
      " 0.13632421 0.18525745 0.14549354 0.11565592 0.16464463 0.1532532\n",
      " 0.1402722  0.14018986 0.15115292 0.13606174 0.14174382 0.15029243\n",
      " 0.12490536 0.16350459 0.12228139 0.16213869 0.12523301 0.13421468\n",
      " 0.1483578  0.12975849 0.15952903 0.15149955]\n",
      "[0.0495306  0.04675987 0.05443958 0.06950029 0.0707728  0.06129672\n",
      " 0.06770105 0.07909101 0.06199845 0.06362548 0.05776887 0.06647678\n",
      " 0.06480359 0.07112234 0.06198793 0.09423506 0.0483509  0.0795907\n",
      " 0.05866402 0.08547556 0.042586   0.06314432 0.04284161 0.05895353\n",
      " 0.04462981 0.10028576 0.07172251 0.06987695 0.06956359 0.08385024\n",
      " 0.05910226 0.07659959 0.03870342 0.06243187 0.05986546 0.06620184\n",
      " 0.07482816 0.06167274 0.05750974 0.07170779 0.07084493 0.09537818\n",
      " 0.06064374 0.10096572 0.06924161 0.04623194 0.0849881  0.06659911\n",
      " 0.06279394 0.06472669 0.06894056 0.05225193 0.06236015 0.0616371\n",
      " 0.05110563 0.07822224 0.05039793 0.07950999 0.05441496 0.05983505\n",
      " 0.06742293 0.05143983 0.07341951 0.06552911]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(predictions[0][0])\n",
    "print(predictions[0][1])\n",
    "print(predictions[0][2])\n",
    "print(predictions[1][0])\n",
    "print(predictions[1][1])\n",
    "print(predictions[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"/home/isa/FYPJ/Model/model6_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(predictions[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
