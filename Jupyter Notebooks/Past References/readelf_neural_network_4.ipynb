{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (10240, 1280, 64) (10240, 1280, 64)\n",
      "Validation shape:  (2560, 1280, 64) (2560, 1280, 64)\n",
      "Test shape:  (3200, 1280, 64) (3200, 1280, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "filename1 = \"Data/dataset.npz\"\n",
    "filename2 = \"Data/dataset2.npz\"\n",
    "\n",
    "# Load numpy array\n",
    "dataset1 = np.load(filename1)\n",
    "dataset2 = np.load(filename2)\n",
    "\n",
    "x_dataset_full = np.concatenate((dataset1['x'], dataset2['x']))\n",
    "y_dataset_full = np.concatenate((dataset1['y'], dataset2['y']))\n",
    "\n",
    "x_test = x_dataset_full[12800:16000]\n",
    "y_test = y_dataset_full[12800:16000]\n",
    "\n",
    "x_dataset = x_dataset_full[:12800]\n",
    "y_dataset = y_dataset_full[:12800]\n",
    "\n",
    "x_val = x_dataset[10240:]\n",
    "y_val = y_dataset[10240:]\n",
    "\n",
    "x_train = x_dataset[:10240]\n",
    "y_train = y_dataset[:10240]\n",
    "\n",
    "# Assign and reshape data\n",
    "x_train, y_train = x_train.reshape(10240, 1280, 64), y_train.reshape(10240, 1280, 64)\n",
    "x_val, y_val = x_val.reshape(2560, 1280, 64), y_val.reshape(2560, 1280, 64)\n",
    "x_test, y_test = x_test.reshape(3200, 1280, 64), y_test.reshape(3200, 1280, 64)\n",
    "\n",
    "print(\"Training shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Validation shape: \", x_val.shape, x_val.shape)\n",
    "print(\"Test shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1280, 64)          33024     \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10240 samples, validate on 2560 samples\n",
      "Epoch 1/500\n",
      "10240/10240 [==============================] - 332s 32ms/step - loss: 0.0882 - acc: 0.0183 - val_loss: 0.0566 - val_acc: 0.0121\n",
      "Epoch 2/500\n",
      "10240/10240 [==============================] - 349s 34ms/step - loss: 0.0719 - acc: 0.0128 - val_loss: 0.0479 - val_acc: 0.0107\n",
      "Epoch 3/500\n",
      "10240/10240 [==============================] - 351s 34ms/step - loss: 0.0645 - acc: 0.0115 - val_loss: 0.0433 - val_acc: 0.0090\n",
      "Epoch 4/500\n",
      "10240/10240 [==============================] - 309s 30ms/step - loss: 0.0600 - acc: 0.0110 - val_loss: 0.0400 - val_acc: 0.0085\n",
      "Epoch 5/500\n",
      "10240/10240 [==============================] - 312s 31ms/step - loss: 0.0567 - acc: 0.0109 - val_loss: 0.0375 - val_acc: 0.0075\n",
      "Epoch 6/500\n",
      "10240/10240 [==============================] - 313s 31ms/step - loss: 0.0541 - acc: 0.0147 - val_loss: 0.0355 - val_acc: 0.0075\n",
      "Epoch 7/500\n",
      "10240/10240 [==============================] - 312s 31ms/step - loss: 0.0519 - acc: 0.0103 - val_loss: 0.0337 - val_acc: 0.0069\n",
      "Epoch 8/500\n",
      "10240/10240 [==============================] - 314s 31ms/step - loss: 0.0501 - acc: 0.0105 - val_loss: 0.0323 - val_acc: 0.0069\n",
      "Epoch 9/500\n",
      "10240/10240 [==============================] - 311s 30ms/step - loss: 0.0485 - acc: 0.0104 - val_loss: 0.0311 - val_acc: 0.0066\n",
      "Epoch 10/500\n",
      "10240/10240 [==============================] - 307s 30ms/step - loss: 0.0472 - acc: 0.0112 - val_loss: 0.0301 - val_acc: 0.0067\n",
      "Epoch 11/500\n",
      "10240/10240 [==============================] - 309s 30ms/step - loss: 0.0461 - acc: 0.0163 - val_loss: 0.0292 - val_acc: 0.0073\n",
      "Epoch 12/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0451 - acc: 0.0131 - val_loss: 0.0284 - val_acc: 0.0072\n",
      "Epoch 13/500\n",
      "10240/10240 [==============================] - 309s 30ms/step - loss: 0.0443 - acc: 0.0103 - val_loss: 0.0277 - val_acc: 0.0071\n",
      "Epoch 14/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0435 - acc: 0.0235 - val_loss: 0.0271 - val_acc: 0.4239\n",
      "Epoch 15/500\n",
      "10240/10240 [==============================] - 313s 31ms/step - loss: 0.0429 - acc: 0.0148 - val_loss: 0.0266 - val_acc: 0.0065\n",
      "Epoch 16/500\n",
      "10240/10240 [==============================] - 310s 30ms/step - loss: 0.0423 - acc: 0.0103 - val_loss: 0.0262 - val_acc: 0.0072\n",
      "Epoch 17/500\n",
      "10240/10240 [==============================] - 313s 31ms/step - loss: 0.0418 - acc: 0.0107 - val_loss: 0.0258 - val_acc: 0.0084\n",
      "Epoch 18/500\n",
      "10240/10240 [==============================] - 312s 30ms/step - loss: 0.0414 - acc: 0.0304 - val_loss: 0.0254 - val_acc: 0.0085\n",
      "Epoch 19/500\n",
      "10240/10240 [==============================] - 312s 30ms/step - loss: 0.0409 - acc: 0.0118 - val_loss: 0.0251 - val_acc: 0.0096\n",
      "Epoch 20/500\n",
      "10240/10240 [==============================] - 311s 30ms/step - loss: 0.0406 - acc: 0.0129 - val_loss: 0.0248 - val_acc: 0.0096\n",
      "Epoch 21/500\n",
      "10240/10240 [==============================] - 308s 30ms/step - loss: 0.0402 - acc: 0.0162 - val_loss: 0.0245 - val_acc: 0.0110\n",
      "Epoch 22/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0399 - acc: 0.0178 - val_loss: 0.0243 - val_acc: 0.0119\n",
      "Epoch 23/500\n",
      "10240/10240 [==============================] - 307s 30ms/step - loss: 0.0396 - acc: 0.0167 - val_loss: 0.0241 - val_acc: 0.0137\n",
      "Epoch 24/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0394 - acc: 0.0204 - val_loss: 0.0239 - val_acc: 0.0146\n",
      "Epoch 25/500\n",
      "10240/10240 [==============================] - 308s 30ms/step - loss: 0.0392 - acc: 0.0256 - val_loss: 0.0237 - val_acc: 0.0174\n",
      "Epoch 26/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0389 - acc: 0.0243 - val_loss: 0.0235 - val_acc: 0.0176\n",
      "Epoch 27/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0388 - acc: 0.0301 - val_loss: 0.0234 - val_acc: 0.0197\n",
      "Epoch 28/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0386 - acc: 0.0274 - val_loss: 0.0232 - val_acc: 0.0232\n",
      "Epoch 29/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0384 - acc: 0.0384 - val_loss: 0.0231 - val_acc: 0.0275\n",
      "Epoch 30/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0383 - acc: 0.0337 - val_loss: 0.0230 - val_acc: 0.0313\n",
      "Epoch 31/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0382 - acc: 0.0391 - val_loss: 0.0229 - val_acc: 0.0343\n",
      "Epoch 32/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0380 - acc: 0.0404 - val_loss: 0.0228 - val_acc: 0.0369\n",
      "Epoch 33/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0379 - acc: 0.0472 - val_loss: 0.0227 - val_acc: 0.0407\n",
      "Epoch 34/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0379 - acc: 0.0468 - val_loss: 0.0227 - val_acc: 0.0441\n",
      "Epoch 35/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0378 - acc: 0.0497 - val_loss: 0.0226 - val_acc: 0.0451\n",
      "Epoch 36/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0377 - acc: 0.0587 - val_loss: 0.0226 - val_acc: 0.0484\n",
      "Epoch 37/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0377 - acc: 0.0624 - val_loss: 0.0225 - val_acc: 0.0517\n",
      "Epoch 38/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0376 - acc: 0.0664 - val_loss: 0.0225 - val_acc: 0.0527\n",
      "Epoch 39/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0376 - acc: 0.0650 - val_loss: 0.0225 - val_acc: 0.0561\n",
      "Epoch 40/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0375 - acc: 0.0705 - val_loss: 0.0224 - val_acc: 0.0592\n",
      "Epoch 41/500\n",
      "10240/10240 [==============================] - 307s 30ms/step - loss: 0.0375 - acc: 0.0721 - val_loss: 0.0224 - val_acc: 0.0651\n",
      "Epoch 42/500\n",
      "10240/10240 [==============================] - 311s 30ms/step - loss: 0.0375 - acc: 0.0747 - val_loss: 0.0224 - val_acc: 0.0711\n",
      "Epoch 43/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0375 - acc: 0.0837 - val_loss: 0.0224 - val_acc: 0.0764\n",
      "Epoch 44/500\n",
      "10240/10240 [==============================] - 308s 30ms/step - loss: 0.0374 - acc: 0.0901 - val_loss: 0.0223 - val_acc: 0.0820\n",
      "Epoch 45/500\n",
      "10240/10240 [==============================] - 309s 30ms/step - loss: 0.0374 - acc: 0.0926 - val_loss: 0.0223 - val_acc: 0.0860\n",
      "Epoch 46/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0374 - acc: 0.1029 - val_loss: 0.0223 - val_acc: 0.0913\n",
      "Epoch 47/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0374 - acc: 0.1044 - val_loss: 0.0223 - val_acc: 0.0958\n",
      "Epoch 48/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0374 - acc: 0.1158 - val_loss: 0.0223 - val_acc: 0.1136\n",
      "Epoch 49/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0374 - acc: 0.1215 - val_loss: 0.0223 - val_acc: 0.1140\n",
      "Epoch 50/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0374 - acc: 0.1279 - val_loss: 0.0223 - val_acc: 0.1145\n",
      "Epoch 51/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0374 - acc: 0.1221 - val_loss: 0.0223 - val_acc: 0.1116\n",
      "Epoch 52/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.1327 - val_loss: 0.0223 - val_acc: 0.1167\n",
      "Epoch 53/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.1317 - val_loss: 0.0223 - val_acc: 0.1256\n",
      "Epoch 54/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1352 - val_loss: 0.0223 - val_acc: 0.1238\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.1489 - val_loss: 0.0223 - val_acc: 0.1287\n",
      "Epoch 56/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1406 - val_loss: 0.0223 - val_acc: 0.1308\n",
      "Epoch 57/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.1446 - val_loss: 0.0222 - val_acc: 0.2283\n",
      "Epoch 58/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1501 - val_loss: 0.0222 - val_acc: 0.1352\n",
      "Epoch 59/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.1482 - val_loss: 0.0222 - val_acc: 0.1429\n",
      "Epoch 60/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.1554 - val_loss: 0.0222 - val_acc: 0.1407\n",
      "Epoch 61/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1605 - val_loss: 0.0222 - val_acc: 0.1553\n",
      "Epoch 62/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.1591 - val_loss: 0.0222 - val_acc: 0.1474\n",
      "Epoch 63/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.1708 - val_loss: 0.0222 - val_acc: 0.1456\n",
      "Epoch 64/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1668 - val_loss: 0.0222 - val_acc: 0.1496\n",
      "Epoch 65/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.1732 - val_loss: 0.0222 - val_acc: 0.1782\n",
      "Epoch 66/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.1695 - val_loss: 0.0222 - val_acc: 0.1553\n",
      "Epoch 67/500\n",
      "10240/10240 [==============================] - 296s 29ms/step - loss: 0.0373 - acc: 0.1821 - val_loss: 0.0222 - val_acc: 0.1590\n",
      "Epoch 68/500\n",
      "10240/10240 [==============================] - 294s 29ms/step - loss: 0.0373 - acc: 0.1704 - val_loss: 0.0222 - val_acc: 0.1557\n",
      "Epoch 69/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.1763 - val_loss: 0.0222 - val_acc: 0.1613\n",
      "Epoch 70/500\n",
      "10240/10240 [==============================] - 293s 29ms/step - loss: 0.0373 - acc: 0.1731 - val_loss: 0.0222 - val_acc: 0.2075\n",
      "Epoch 71/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.1712 - val_loss: 0.0222 - val_acc: 0.1608\n",
      "Epoch 72/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.1818 - val_loss: 0.0222 - val_acc: 0.1636\n",
      "Epoch 73/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.1775 - val_loss: 0.0222 - val_acc: 0.1634\n",
      "Epoch 74/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.1820 - val_loss: 0.0222 - val_acc: 0.1707\n",
      "Epoch 75/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.1867 - val_loss: 0.0222 - val_acc: 0.1742\n",
      "Epoch 76/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.1866 - val_loss: 0.0222 - val_acc: 0.1719\n",
      "Epoch 77/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.1857 - val_loss: 0.0222 - val_acc: 0.2291\n",
      "Epoch 78/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.1850 - val_loss: 0.0222 - val_acc: 0.1751\n",
      "Epoch 79/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.1945 - val_loss: 0.0222 - val_acc: 0.1750\n",
      "Epoch 80/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.1907 - val_loss: 0.0222 - val_acc: 0.1743\n",
      "Epoch 81/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.1878 - val_loss: 0.0222 - val_acc: 0.1753\n",
      "Epoch 82/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1940 - val_loss: 0.0222 - val_acc: 0.1791\n",
      "Epoch 83/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1911 - val_loss: 0.0222 - val_acc: 0.1780\n",
      "Epoch 84/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1919 - val_loss: 0.0222 - val_acc: 0.1764\n",
      "Epoch 85/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.1981 - val_loss: 0.0222 - val_acc: 0.1785\n",
      "Epoch 86/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.1899 - val_loss: 0.0222 - val_acc: 0.1776\n",
      "Epoch 87/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.1935 - val_loss: 0.0222 - val_acc: 0.1823\n",
      "Epoch 88/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.1944 - val_loss: 0.0222 - val_acc: 0.1838\n",
      "Epoch 89/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2116 - val_loss: 0.0222 - val_acc: 0.1811\n",
      "Epoch 90/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.1931 - val_loss: 0.0222 - val_acc: 0.1788\n",
      "Epoch 91/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2012 - val_loss: 0.0222 - val_acc: 0.1784\n",
      "Epoch 92/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.1954 - val_loss: 0.0222 - val_acc: 0.1792\n",
      "Epoch 93/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2015 - val_loss: 0.0222 - val_acc: 0.1833\n",
      "Epoch 94/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2069 - val_loss: 0.0222 - val_acc: 0.1812\n",
      "Epoch 95/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2085 - val_loss: 0.0222 - val_acc: 0.1855\n",
      "Epoch 96/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2018 - val_loss: 0.0222 - val_acc: 0.1827\n",
      "Epoch 97/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.1991 - val_loss: 0.0222 - val_acc: 0.1844\n",
      "Epoch 98/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.1993 - val_loss: 0.0222 - val_acc: 0.1827\n",
      "Epoch 99/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2022 - val_loss: 0.0222 - val_acc: 0.1850\n",
      "Epoch 100/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2083 - val_loss: 0.0222 - val_acc: 0.1871\n",
      "Epoch 101/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2021 - val_loss: 0.0222 - val_acc: 0.1839\n",
      "Epoch 102/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2008 - val_loss: 0.0222 - val_acc: 0.1848\n",
      "Epoch 103/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2023 - val_loss: 0.0222 - val_acc: 0.1850\n",
      "Epoch 104/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2115 - val_loss: 0.0222 - val_acc: 0.1863\n",
      "Epoch 105/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2066 - val_loss: 0.0222 - val_acc: 0.1849\n",
      "Epoch 106/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.1996 - val_loss: 0.0222 - val_acc: 0.1855\n",
      "Epoch 107/500\n",
      "10240/10240 [==============================] - 295s 29ms/step - loss: 0.0373 - acc: 0.2109 - val_loss: 0.0222 - val_acc: 0.1873\n",
      "Epoch 108/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2129 - val_loss: 0.0222 - val_acc: 0.1899\n",
      "Epoch 109/500\n",
      "10240/10240 [==============================] - 295s 29ms/step - loss: 0.0373 - acc: 0.2087 - val_loss: 0.0222 - val_acc: 0.1932\n",
      "Epoch 110/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2084 - val_loss: 0.0222 - val_acc: 0.1872\n",
      "Epoch 111/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2030 - val_loss: 0.0222 - val_acc: 0.1872\n",
      "Epoch 112/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2091 - val_loss: 0.0222 - val_acc: 0.1912\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2107 - val_loss: 0.0222 - val_acc: 0.1896\n",
      "Epoch 114/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2118 - val_loss: 0.0222 - val_acc: 0.1948\n",
      "Epoch 115/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2156 - val_loss: 0.0222 - val_acc: 0.1893\n",
      "Epoch 116/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2055 - val_loss: 0.0222 - val_acc: 0.1939\n",
      "Epoch 117/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2084 - val_loss: 0.0222 - val_acc: 0.1926\n",
      "Epoch 118/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2105 - val_loss: 0.0222 - val_acc: 0.1948\n",
      "Epoch 119/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2188 - val_loss: 0.0222 - val_acc: 0.1935\n",
      "Epoch 120/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2174 - val_loss: 0.0222 - val_acc: 0.1932\n",
      "Epoch 121/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2069 - val_loss: 0.0222 - val_acc: 0.1958\n",
      "Epoch 122/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2097 - val_loss: 0.0222 - val_acc: 0.1963\n",
      "Epoch 123/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2108 - val_loss: 0.0222 - val_acc: 0.1968\n",
      "Epoch 124/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.2176 - val_loss: 0.0222 - val_acc: 0.2049\n",
      "Epoch 125/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2070 - val_loss: 0.0222 - val_acc: 0.1957\n",
      "Epoch 126/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2136 - val_loss: 0.0222 - val_acc: 0.1935\n",
      "Epoch 127/500\n",
      "10240/10240 [==============================] - 296s 29ms/step - loss: 0.0373 - acc: 0.2097 - val_loss: 0.0222 - val_acc: 0.1954\n",
      "Epoch 128/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2144 - val_loss: 0.0222 - val_acc: 0.1971\n",
      "Epoch 129/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2150 - val_loss: 0.0222 - val_acc: 0.1978\n",
      "Epoch 130/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2146 - val_loss: 0.0222 - val_acc: 0.1951\n",
      "Epoch 131/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2160 - val_loss: 0.0222 - val_acc: 0.1995\n",
      "Epoch 132/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2107 - val_loss: 0.0222 - val_acc: 0.1990\n",
      "Epoch 133/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2243 - val_loss: 0.0222 - val_acc: 0.2043\n",
      "Epoch 134/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2135 - val_loss: 0.0222 - val_acc: 0.2006\n",
      "Epoch 135/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2121 - val_loss: 0.0222 - val_acc: 0.1969\n",
      "Epoch 136/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2249 - val_loss: 0.0222 - val_acc: 0.1947\n",
      "Epoch 137/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2140 - val_loss: 0.0222 - val_acc: 0.2004\n",
      "Epoch 138/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2097 - val_loss: 0.0222 - val_acc: 0.2071\n",
      "Epoch 139/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2144 - val_loss: 0.0222 - val_acc: 0.1965\n",
      "Epoch 140/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2176 - val_loss: 0.0222 - val_acc: 0.1991\n",
      "Epoch 141/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.2121 - val_loss: 0.0222 - val_acc: 0.1968\n",
      "Epoch 142/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2173 - val_loss: 0.0222 - val_acc: 0.2023\n",
      "Epoch 143/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2199 - val_loss: 0.0222 - val_acc: 0.1962\n",
      "Epoch 144/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2128 - val_loss: 0.0222 - val_acc: 0.2010\n",
      "Epoch 145/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2191 - val_loss: 0.0222 - val_acc: 0.1977\n",
      "Epoch 146/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2311 - val_loss: 0.0222 - val_acc: 0.2029\n",
      "Epoch 147/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2185 - val_loss: 0.0222 - val_acc: 0.2000\n",
      "Epoch 148/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2177 - val_loss: 0.0222 - val_acc: 0.2046\n",
      "Epoch 149/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2172 - val_loss: 0.0222 - val_acc: 0.2012\n",
      "Epoch 150/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2342 - val_loss: 0.0222 - val_acc: 0.2044\n",
      "Epoch 151/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2198 - val_loss: 0.0222 - val_acc: 0.1991\n",
      "Epoch 152/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2141 - val_loss: 0.0222 - val_acc: 0.2032\n",
      "Epoch 153/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2241 - val_loss: 0.0222 - val_acc: 0.2007\n",
      "Epoch 154/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2285 - val_loss: 0.0222 - val_acc: 0.2067\n",
      "Epoch 155/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2128 - val_loss: 0.0222 - val_acc: 0.2050\n",
      "Epoch 156/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2136 - val_loss: 0.0222 - val_acc: 0.1990\n",
      "Epoch 157/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2212 - val_loss: 0.0222 - val_acc: 0.2032\n",
      "Epoch 158/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2190 - val_loss: 0.0222 - val_acc: 0.2044\n",
      "Epoch 159/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2179 - val_loss: 0.0222 - val_acc: 0.2016\n",
      "Epoch 160/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2241 - val_loss: 0.0222 - val_acc: 0.2024\n",
      "Epoch 161/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2208 - val_loss: 0.0222 - val_acc: 0.2013\n",
      "Epoch 162/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2149 - val_loss: 0.0222 - val_acc: 0.2023\n",
      "Epoch 163/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2204 - val_loss: 0.0222 - val_acc: 0.2027\n",
      "Epoch 164/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2151 - val_loss: 0.0222 - val_acc: 0.2004\n",
      "Epoch 165/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2260 - val_loss: 0.0222 - val_acc: 0.2058\n",
      "Epoch 166/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2245 - val_loss: 0.0222 - val_acc: 0.2948\n",
      "Epoch 167/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2300 - val_loss: 0.0222 - val_acc: 0.2014\n",
      "Epoch 168/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2197 - val_loss: 0.0222 - val_acc: 0.2035\n",
      "Epoch 169/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2201 - val_loss: 0.0222 - val_acc: 0.2084\n",
      "Epoch 170/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2163 - val_loss: 0.0222 - val_acc: 0.2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2229 - val_loss: 0.0222 - val_acc: 0.2009\n",
      "Epoch 172/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2209 - val_loss: 0.0222 - val_acc: 0.2283\n",
      "Epoch 173/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2285 - val_loss: 0.0222 - val_acc: 0.2050\n",
      "Epoch 174/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2210 - val_loss: 0.0222 - val_acc: 0.2038\n",
      "Epoch 175/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2242 - val_loss: 0.0222 - val_acc: 0.2051\n",
      "Epoch 176/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2199 - val_loss: 0.0222 - val_acc: 0.2056\n",
      "Epoch 177/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2179 - val_loss: 0.0222 - val_acc: 0.2056\n",
      "Epoch 178/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2188 - val_loss: 0.0222 - val_acc: 0.2042\n",
      "Epoch 179/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2248 - val_loss: 0.0222 - val_acc: 0.2046\n",
      "Epoch 180/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2395 - val_loss: 0.0222 - val_acc: 0.2122\n",
      "Epoch 181/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2274 - val_loss: 0.0222 - val_acc: 0.2038\n",
      "Epoch 182/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2203 - val_loss: 0.0222 - val_acc: 0.2065\n",
      "Epoch 183/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2206 - val_loss: 0.0222 - val_acc: 0.2053\n",
      "Epoch 184/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.2292 - val_loss: 0.0222 - val_acc: 0.2039\n",
      "Epoch 185/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2319 - val_loss: 0.0222 - val_acc: 0.2099\n",
      "Epoch 186/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2237 - val_loss: 0.0222 - val_acc: 0.2091\n",
      "Epoch 187/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2276 - val_loss: 0.0222 - val_acc: 0.2067\n",
      "Epoch 188/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2258 - val_loss: 0.0222 - val_acc: 0.2090\n",
      "Epoch 189/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2238 - val_loss: 0.0222 - val_acc: 0.2060\n",
      "Epoch 190/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2307 - val_loss: 0.0222 - val_acc: 0.2065\n",
      "Epoch 191/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2228 - val_loss: 0.0222 - val_acc: 0.2075\n",
      "Epoch 192/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2369 - val_loss: 0.0222 - val_acc: 0.2087\n",
      "Epoch 193/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2289 - val_loss: 0.0222 - val_acc: 0.2070\n",
      "Epoch 194/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2213 - val_loss: 0.0222 - val_acc: 0.2093\n",
      "Epoch 195/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2262 - val_loss: 0.0222 - val_acc: 0.6536\n",
      "Epoch 196/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2317 - val_loss: 0.0222 - val_acc: 0.2057\n",
      "Epoch 197/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2352 - val_loss: 0.0222 - val_acc: 0.2058\n",
      "Epoch 198/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2275 - val_loss: 0.0222 - val_acc: 0.2054\n",
      "Epoch 199/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2225 - val_loss: 0.0222 - val_acc: 0.2085\n",
      "Epoch 200/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2239 - val_loss: 0.0222 - val_acc: 0.2123\n",
      "Epoch 201/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2308 - val_loss: 0.0222 - val_acc: 0.2072\n",
      "Epoch 202/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2298 - val_loss: 0.0222 - val_acc: 0.2058\n",
      "Epoch 203/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2212 - val_loss: 0.0222 - val_acc: 0.2043\n",
      "Epoch 204/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2424 - val_loss: 0.0222 - val_acc: 0.2040\n",
      "Epoch 205/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2280 - val_loss: 0.0222 - val_acc: 0.2205\n",
      "Epoch 206/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2251 - val_loss: 0.0222 - val_acc: 0.2083\n",
      "Epoch 207/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2254 - val_loss: 0.0222 - val_acc: 0.2085\n",
      "Epoch 208/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2263 - val_loss: 0.0222 - val_acc: 0.2096\n",
      "Epoch 209/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2353 - val_loss: 0.0222 - val_acc: 0.2089\n",
      "Epoch 210/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2314 - val_loss: 0.0222 - val_acc: 0.2130\n",
      "Epoch 211/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2337 - val_loss: 0.0222 - val_acc: 0.2108\n",
      "Epoch 212/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2343 - val_loss: 0.0222 - val_acc: 0.2102\n",
      "Epoch 213/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2371 - val_loss: 0.0222 - val_acc: 0.2095\n",
      "Epoch 214/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2219 - val_loss: 0.0222 - val_acc: 0.2092\n",
      "Epoch 215/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2291 - val_loss: 0.0222 - val_acc: 0.2162\n",
      "Epoch 216/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2277 - val_loss: 0.0222 - val_acc: 0.2126\n",
      "Epoch 217/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2287 - val_loss: 0.0222 - val_acc: 0.2099\n",
      "Epoch 218/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2278 - val_loss: 0.0222 - val_acc: 0.2123\n",
      "Epoch 219/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2228 - val_loss: 0.0222 - val_acc: 0.2080\n",
      "Epoch 220/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2230 - val_loss: 0.0222 - val_acc: 0.2125\n",
      "Epoch 221/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.2242 - val_loss: 0.0222 - val_acc: 0.2110\n",
      "Epoch 222/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2256 - val_loss: 0.0222 - val_acc: 0.2145\n",
      "Epoch 223/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2234 - val_loss: 0.0222 - val_acc: 0.2117\n",
      "Epoch 224/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2244 - val_loss: 0.0222 - val_acc: 0.2119\n",
      "Epoch 225/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2363 - val_loss: 0.0222 - val_acc: 0.2197\n",
      "Epoch 226/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2303 - val_loss: 0.0222 - val_acc: 0.2129\n",
      "Epoch 227/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2235 - val_loss: 0.0222 - val_acc: 0.2118\n",
      "Epoch 228/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2276 - val_loss: 0.0222 - val_acc: 0.2698\n",
      "Epoch 229/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2281 - val_loss: 0.0222 - val_acc: 0.2179\n",
      "Epoch 230/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2393 - val_loss: 0.0222 - val_acc: 0.2109\n",
      "Epoch 231/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2283 - val_loss: 0.0222 - val_acc: 0.2337\n",
      "Epoch 232/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2276 - val_loss: 0.0222 - val_acc: 0.2144\n",
      "Epoch 233/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2277 - val_loss: 0.0222 - val_acc: 0.2091\n",
      "Epoch 234/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2288 - val_loss: 0.0222 - val_acc: 0.2171\n",
      "Epoch 235/500\n",
      "10240/10240 [==============================] - 296s 29ms/step - loss: 0.0373 - acc: 0.2269 - val_loss: 0.0222 - val_acc: 0.2123\n",
      "Epoch 236/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2255 - val_loss: 0.0222 - val_acc: 0.2128\n",
      "Epoch 237/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2289 - val_loss: 0.0222 - val_acc: 0.2146\n",
      "Epoch 238/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2294 - val_loss: 0.0222 - val_acc: 0.2122\n",
      "Epoch 239/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2454 - val_loss: 0.0222 - val_acc: 0.2114\n",
      "Epoch 240/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2293 - val_loss: 0.0222 - val_acc: 0.2141\n",
      "Epoch 241/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2305 - val_loss: 0.0222 - val_acc: 0.2128\n",
      "Epoch 242/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2373 - val_loss: 0.0222 - val_acc: 0.2156\n",
      "Epoch 243/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2305 - val_loss: 0.0222 - val_acc: 0.2119\n",
      "Epoch 244/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2314 - val_loss: 0.0222 - val_acc: 0.2133\n",
      "Epoch 245/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2354 - val_loss: 0.0222 - val_acc: 0.2178\n",
      "Epoch 246/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2328 - val_loss: 0.0222 - val_acc: 0.2117\n",
      "Epoch 247/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2418 - val_loss: 0.0222 - val_acc: 0.2144\n",
      "Epoch 248/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2373 - val_loss: 0.0222 - val_acc: 0.2142\n",
      "Epoch 249/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2292 - val_loss: 0.0222 - val_acc: 0.2380\n",
      "Epoch 250/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2371 - val_loss: 0.0222 - val_acc: 0.2108\n",
      "Epoch 251/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2372 - val_loss: 0.0222 - val_acc: 0.2116\n",
      "Epoch 252/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2352 - val_loss: 0.0222 - val_acc: 0.2133\n",
      "Epoch 253/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2308 - val_loss: 0.0222 - val_acc: 0.2126\n",
      "Epoch 254/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2305 - val_loss: 0.0222 - val_acc: 0.2124\n",
      "Epoch 255/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2344 - val_loss: 0.0222 - val_acc: 0.2143\n",
      "Epoch 256/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2332 - val_loss: 0.0222 - val_acc: 0.2153\n",
      "Epoch 257/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2345 - val_loss: 0.0222 - val_acc: 0.2131\n",
      "Epoch 258/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2322 - val_loss: 0.0222 - val_acc: 0.2145\n",
      "Epoch 259/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2377 - val_loss: 0.0222 - val_acc: 0.2351\n",
      "Epoch 260/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2390 - val_loss: 0.0222 - val_acc: 0.2159\n",
      "Epoch 261/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2385 - val_loss: 0.0222 - val_acc: 0.2138\n",
      "Epoch 262/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2364 - val_loss: 0.0222 - val_acc: 0.2188\n",
      "Epoch 263/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2279 - val_loss: 0.0222 - val_acc: 0.2167\n",
      "Epoch 264/500\n",
      "10240/10240 [==============================] - 296s 29ms/step - loss: 0.0373 - acc: 0.2289 - val_loss: 0.0222 - val_acc: 0.2164\n",
      "Epoch 265/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2499 - val_loss: 0.0222 - val_acc: 0.2168\n",
      "Epoch 266/500\n",
      "10240/10240 [==============================] - 295s 29ms/step - loss: 0.0373 - acc: 0.2307 - val_loss: 0.0222 - val_acc: 0.2169\n",
      "Epoch 267/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2366 - val_loss: 0.0222 - val_acc: 0.2142\n",
      "Epoch 268/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2335 - val_loss: 0.0222 - val_acc: 0.2161\n",
      "Epoch 269/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2410 - val_loss: 0.0222 - val_acc: 0.2154\n",
      "Epoch 270/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2294 - val_loss: 0.0222 - val_acc: 0.2178\n",
      "Epoch 271/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2472 - val_loss: 0.0222 - val_acc: 0.2229\n",
      "Epoch 272/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2398 - val_loss: 0.0222 - val_acc: 0.2171\n",
      "Epoch 273/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2348 - val_loss: 0.0222 - val_acc: 0.2167\n",
      "Epoch 274/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2360 - val_loss: 0.0222 - val_acc: 0.2194\n",
      "Epoch 275/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2313 - val_loss: 0.0222 - val_acc: 0.2175\n",
      "Epoch 276/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2314 - val_loss: 0.0222 - val_acc: 0.2167\n",
      "Epoch 277/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2515 - val_loss: 0.0222 - val_acc: 0.2200\n",
      "Epoch 278/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2355 - val_loss: 0.0222 - val_acc: 0.2206\n",
      "Epoch 279/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2371 - val_loss: 0.0222 - val_acc: 0.2188\n",
      "Epoch 280/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2387 - val_loss: 0.0222 - val_acc: 0.2198\n",
      "Epoch 281/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2321 - val_loss: 0.0222 - val_acc: 0.2155\n",
      "Epoch 282/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2364 - val_loss: 0.0222 - val_acc: 0.2195\n",
      "Epoch 283/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2417 - val_loss: 0.0222 - val_acc: 0.2181\n",
      "Epoch 284/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.2337 - val_loss: 0.0222 - val_acc: 0.2249\n",
      "Epoch 285/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2456 - val_loss: 0.0222 - val_acc: 0.2184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2365 - val_loss: 0.0222 - val_acc: 0.2189\n",
      "Epoch 287/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2347 - val_loss: 0.0222 - val_acc: 0.2175\n",
      "Epoch 288/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2400 - val_loss: 0.0222 - val_acc: 0.2188\n",
      "Epoch 289/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2451 - val_loss: 0.0222 - val_acc: 0.2255\n",
      "Epoch 290/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2388 - val_loss: 0.0222 - val_acc: 0.2188\n",
      "Epoch 291/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2367 - val_loss: 0.0222 - val_acc: 0.2176\n",
      "Epoch 292/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2350 - val_loss: 0.0222 - val_acc: 0.2164\n",
      "Epoch 293/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2343 - val_loss: 0.0222 - val_acc: 0.2190\n",
      "Epoch 294/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2416 - val_loss: 0.0222 - val_acc: 0.2174\n",
      "Epoch 295/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2485 - val_loss: 0.0222 - val_acc: 0.2171\n",
      "Epoch 296/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2350 - val_loss: 0.0222 - val_acc: 0.2203\n",
      "Epoch 297/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2360 - val_loss: 0.0222 - val_acc: 0.2196\n",
      "Epoch 298/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2324 - val_loss: 0.0222 - val_acc: 0.2218\n",
      "Epoch 299/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2366 - val_loss: 0.0222 - val_acc: 0.2192\n",
      "Epoch 300/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2578 - val_loss: 0.0222 - val_acc: 0.2217\n",
      "Epoch 301/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2397 - val_loss: 0.0222 - val_acc: 0.2171\n",
      "Epoch 302/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2316 - val_loss: 0.0222 - val_acc: 0.2229\n",
      "Epoch 303/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2343 - val_loss: 0.0222 - val_acc: 0.2181\n",
      "Epoch 304/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2324 - val_loss: 0.0222 - val_acc: 0.2178\n",
      "Epoch 305/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2348 - val_loss: 0.0222 - val_acc: 0.2212\n",
      "Epoch 306/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2431 - val_loss: 0.0222 - val_acc: 0.2228\n",
      "Epoch 307/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2400 - val_loss: 0.0222 - val_acc: 0.2221\n",
      "Epoch 308/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2326 - val_loss: 0.0222 - val_acc: 0.2280\n",
      "Epoch 309/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2408 - val_loss: 0.0222 - val_acc: 0.2209\n",
      "Epoch 310/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2384 - val_loss: 0.0222 - val_acc: 0.2228\n",
      "Epoch 311/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2335 - val_loss: 0.0222 - val_acc: 0.2213\n",
      "Epoch 312/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2410 - val_loss: 0.0222 - val_acc: 0.2180\n",
      "Epoch 313/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2380 - val_loss: 0.0222 - val_acc: 0.2199\n",
      "Epoch 314/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2408 - val_loss: 0.0222 - val_acc: 0.2223\n",
      "Epoch 315/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.2414 - val_loss: 0.0222 - val_acc: 0.2175\n",
      "Epoch 316/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2399 - val_loss: 0.0222 - val_acc: 0.2195\n",
      "Epoch 317/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2368 - val_loss: 0.0222 - val_acc: 0.2242\n",
      "Epoch 318/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2457 - val_loss: 0.0222 - val_acc: 0.2215\n",
      "Epoch 319/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2340 - val_loss: 0.0222 - val_acc: 0.2218\n",
      "Epoch 320/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2392 - val_loss: 0.0222 - val_acc: 0.2227\n",
      "Epoch 321/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2379 - val_loss: 0.0222 - val_acc: 0.2282\n",
      "Epoch 322/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2354 - val_loss: 0.0222 - val_acc: 0.2237\n",
      "Epoch 323/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2347 - val_loss: 0.0222 - val_acc: 0.2232\n",
      "Epoch 324/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2413 - val_loss: 0.0222 - val_acc: 0.2244\n",
      "Epoch 325/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2632 - val_loss: 0.0222 - val_acc: 0.2259\n",
      "Epoch 326/500\n",
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.2461 - val_loss: 0.0222 - val_acc: 0.2271\n",
      "Epoch 327/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2380 - val_loss: 0.0222 - val_acc: 0.2226\n",
      "Epoch 328/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2368 - val_loss: 0.0222 - val_acc: 0.2237\n",
      "Epoch 329/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2523 - val_loss: 0.0222 - val_acc: 0.2271\n",
      "Epoch 330/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2393 - val_loss: 0.0222 - val_acc: 0.2256\n",
      "Epoch 331/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2440 - val_loss: 0.0222 - val_acc: 0.2242\n",
      "Epoch 332/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2464 - val_loss: 0.0222 - val_acc: 0.2245\n",
      "Epoch 333/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2501 - val_loss: 0.0222 - val_acc: 0.2256\n",
      "Epoch 334/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2555 - val_loss: 0.0222 - val_acc: 0.2231\n",
      "Epoch 335/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2411 - val_loss: 0.0222 - val_acc: 0.2215\n",
      "Epoch 336/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2362 - val_loss: 0.0222 - val_acc: 0.2267\n",
      "Epoch 337/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2369 - val_loss: 0.0222 - val_acc: 0.2251\n",
      "Epoch 338/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2465 - val_loss: 0.0222 - val_acc: 0.2255\n",
      "Epoch 339/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2544 - val_loss: 0.0222 - val_acc: 0.2284\n",
      "Epoch 340/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2421 - val_loss: 0.0222 - val_acc: 0.2291\n",
      "Epoch 341/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2486 - val_loss: 0.0222 - val_acc: 0.2247\n",
      "Epoch 342/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2433 - val_loss: 0.0222 - val_acc: 0.2228\n",
      "Epoch 343/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240/10240 [==============================] - 306s 30ms/step - loss: 0.0373 - acc: 0.2357 - val_loss: 0.0222 - val_acc: 0.2221\n",
      "Epoch 344/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2501 - val_loss: 0.0222 - val_acc: 0.2258\n",
      "Epoch 345/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2396 - val_loss: 0.0222 - val_acc: 0.2258\n",
      "Epoch 346/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2478 - val_loss: 0.0222 - val_acc: 0.2259\n",
      "Epoch 347/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2464 - val_loss: 0.0222 - val_acc: 0.2254\n",
      "Epoch 348/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2378 - val_loss: 0.0222 - val_acc: 0.2267\n",
      "Epoch 349/500\n",
      "10240/10240 [==============================] - 296s 29ms/step - loss: 0.0373 - acc: 0.2411 - val_loss: 0.0222 - val_acc: 0.2257\n",
      "Epoch 350/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2432 - val_loss: 0.0222 - val_acc: 0.2247\n",
      "Epoch 351/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2468 - val_loss: 0.0222 - val_acc: 0.2260\n",
      "Epoch 352/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2391 - val_loss: 0.0222 - val_acc: 0.2236\n",
      "Epoch 353/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2439 - val_loss: 0.0222 - val_acc: 0.2261\n",
      "Epoch 354/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2415 - val_loss: 0.0222 - val_acc: 0.2246\n",
      "Epoch 355/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2492 - val_loss: 0.0222 - val_acc: 0.2295\n",
      "Epoch 356/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2474 - val_loss: 0.0222 - val_acc: 0.2275\n",
      "Epoch 357/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2373 - val_loss: 0.0222 - val_acc: 0.2278\n",
      "Epoch 358/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2440 - val_loss: 0.0222 - val_acc: 0.2269\n",
      "Epoch 359/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2385 - val_loss: 0.0222 - val_acc: 0.2258\n",
      "Epoch 360/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2429 - val_loss: 0.0222 - val_acc: 0.2274\n",
      "Epoch 361/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2431 - val_loss: 0.0222 - val_acc: 0.2289\n",
      "Epoch 362/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2512 - val_loss: 0.0222 - val_acc: 0.2283\n",
      "Epoch 363/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2398 - val_loss: 0.0222 - val_acc: 0.2264\n",
      "Epoch 364/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2419 - val_loss: 0.0222 - val_acc: 0.2251\n",
      "Epoch 365/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2395 - val_loss: 0.0222 - val_acc: 0.2235\n",
      "Epoch 366/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2391 - val_loss: 0.0222 - val_acc: 0.2311\n",
      "Epoch 367/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2425 - val_loss: 0.0222 - val_acc: 0.2268\n",
      "Epoch 368/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2379 - val_loss: 0.0222 - val_acc: 0.2247\n",
      "Epoch 369/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2474 - val_loss: 0.0222 - val_acc: 0.2299\n",
      "Epoch 370/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2483 - val_loss: 0.0222 - val_acc: 0.2295\n",
      "Epoch 371/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2405 - val_loss: 0.0222 - val_acc: 0.2279\n",
      "Epoch 372/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2422 - val_loss: 0.0222 - val_acc: 0.2275\n",
      "Epoch 373/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2488 - val_loss: 0.0222 - val_acc: 0.2255\n",
      "Epoch 374/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2460 - val_loss: 0.0222 - val_acc: 0.2290\n",
      "Epoch 375/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2383 - val_loss: 0.0222 - val_acc: 0.2281\n",
      "Epoch 376/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2449 - val_loss: 0.0222 - val_acc: 0.2266\n",
      "Epoch 377/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2498 - val_loss: 0.0222 - val_acc: 0.2290\n",
      "Epoch 378/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2436 - val_loss: 0.0222 - val_acc: 0.2584\n",
      "Epoch 379/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2483 - val_loss: 0.0222 - val_acc: 0.2285\n",
      "Epoch 380/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2669 - val_loss: 0.0222 - val_acc: 0.2292\n",
      "Epoch 381/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2619 - val_loss: 0.0222 - val_acc: 0.2263\n",
      "Epoch 382/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2409 - val_loss: 0.0222 - val_acc: 0.2292\n",
      "Epoch 383/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2418 - val_loss: 0.0222 - val_acc: 0.2299\n",
      "Epoch 384/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2433 - val_loss: 0.0222 - val_acc: 0.2286\n",
      "Epoch 385/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2455 - val_loss: 0.0222 - val_acc: 0.2288\n",
      "Epoch 386/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2422 - val_loss: 0.0222 - val_acc: 0.2276\n",
      "Epoch 387/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2512 - val_loss: 0.0222 - val_acc: 0.2284\n",
      "Epoch 388/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2414 - val_loss: 0.0222 - val_acc: 0.2531\n",
      "Epoch 389/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2444 - val_loss: 0.0222 - val_acc: 0.2245\n",
      "Epoch 390/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2441 - val_loss: 0.0222 - val_acc: 0.3030\n",
      "Epoch 391/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2483 - val_loss: 0.0222 - val_acc: 0.2307\n",
      "Epoch 392/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2555 - val_loss: 0.0222 - val_acc: 0.2255\n",
      "Epoch 393/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2433 - val_loss: 0.0222 - val_acc: 0.2285\n",
      "Epoch 394/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2429 - val_loss: 0.0222 - val_acc: 0.2306\n",
      "Epoch 395/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2431 - val_loss: 0.0222 - val_acc: 0.2320\n",
      "Epoch 396/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2418 - val_loss: 0.0222 - val_acc: 0.2286\n",
      "Epoch 397/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2447 - val_loss: 0.0222 - val_acc: 0.2313\n",
      "Epoch 398/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2434 - val_loss: 0.0222 - val_acc: 0.2287\n",
      "Epoch 399/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2450 - val_loss: 0.0222 - val_acc: 0.2261\n",
      "Epoch 400/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2512 - val_loss: 0.0222 - val_acc: 0.2275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2410 - val_loss: 0.0222 - val_acc: 0.2297\n",
      "Epoch 402/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2450 - val_loss: 0.0222 - val_acc: 0.2371\n",
      "Epoch 403/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2525 - val_loss: 0.0222 - val_acc: 0.2399\n",
      "Epoch 404/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2471 - val_loss: 0.0222 - val_acc: 0.2300\n",
      "Epoch 405/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2524 - val_loss: 0.0222 - val_acc: 0.2293\n",
      "Epoch 406/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2448 - val_loss: 0.0222 - val_acc: 0.2307\n",
      "Epoch 407/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2694 - val_loss: 0.0222 - val_acc: 0.2286\n",
      "Epoch 408/500\n",
      "10240/10240 [==============================] - 308s 30ms/step - loss: 0.0373 - acc: 0.2478 - val_loss: 0.0222 - val_acc: 0.2301\n",
      "Epoch 409/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2648 - val_loss: 0.0222 - val_acc: 0.2314\n",
      "Epoch 410/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2623 - val_loss: 0.0222 - val_acc: 0.2299\n",
      "Epoch 411/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2585 - val_loss: 0.0222 - val_acc: 0.2306\n",
      "Epoch 412/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2425 - val_loss: 0.0222 - val_acc: 0.2331\n",
      "Epoch 413/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2449 - val_loss: 0.0222 - val_acc: 0.2324\n",
      "Epoch 414/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2566 - val_loss: 0.0222 - val_acc: 0.2341\n",
      "Epoch 415/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2634 - val_loss: 0.0222 - val_acc: 0.2297\n",
      "Epoch 416/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2500 - val_loss: 0.0222 - val_acc: 0.2341\n",
      "Epoch 417/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2660 - val_loss: 0.0222 - val_acc: 0.2333\n",
      "Epoch 418/500\n",
      "10240/10240 [==============================] - 297s 29ms/step - loss: 0.0373 - acc: 0.2491 - val_loss: 0.0222 - val_acc: 0.2350\n",
      "Epoch 419/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2605 - val_loss: 0.0222 - val_acc: 0.2320\n",
      "Epoch 420/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2442 - val_loss: 0.0222 - val_acc: 0.2334\n",
      "Epoch 421/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2507 - val_loss: 0.0222 - val_acc: 0.2763\n",
      "Epoch 422/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2652 - val_loss: 0.0222 - val_acc: 0.2317\n",
      "Epoch 423/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2448 - val_loss: 0.0222 - val_acc: 0.2348\n",
      "Epoch 424/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2478 - val_loss: 0.0222 - val_acc: 0.2324\n",
      "Epoch 425/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2444 - val_loss: 0.0222 - val_acc: 0.2347\n",
      "Epoch 426/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2488 - val_loss: 0.0222 - val_acc: 0.2318\n",
      "Epoch 427/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2487 - val_loss: 0.0222 - val_acc: 0.2324\n",
      "Epoch 428/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2602 - val_loss: 0.0222 - val_acc: 0.2306\n",
      "Epoch 429/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2543 - val_loss: 0.0222 - val_acc: 0.2338\n",
      "Epoch 430/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2602 - val_loss: 0.0222 - val_acc: 0.2347\n",
      "Epoch 431/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2446 - val_loss: 0.0222 - val_acc: 0.2336\n",
      "Epoch 432/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2466 - val_loss: 0.0222 - val_acc: 0.2334\n",
      "Epoch 433/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2539 - val_loss: 0.0222 - val_acc: 0.2348\n",
      "Epoch 434/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2726 - val_loss: 0.0222 - val_acc: 0.2375\n",
      "Epoch 435/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2450 - val_loss: 0.0222 - val_acc: 0.2332\n",
      "Epoch 436/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2478 - val_loss: 0.0222 - val_acc: 0.2434\n",
      "Epoch 437/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2457 - val_loss: 0.0222 - val_acc: 0.2360\n",
      "Epoch 438/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2640 - val_loss: 0.0222 - val_acc: 0.2354\n",
      "Epoch 439/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2469 - val_loss: 0.0222 - val_acc: 0.2361\n",
      "Epoch 440/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2524 - val_loss: 0.0222 - val_acc: 0.2358\n",
      "Epoch 441/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2458 - val_loss: 0.0222 - val_acc: 0.2397\n",
      "Epoch 442/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2453 - val_loss: 0.0222 - val_acc: 0.2378\n",
      "Epoch 443/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2524 - val_loss: 0.0222 - val_acc: 0.2382\n",
      "Epoch 444/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2555 - val_loss: 0.0222 - val_acc: 0.2369\n",
      "Epoch 445/500\n",
      "10240/10240 [==============================] - 298s 29ms/step - loss: 0.0373 - acc: 0.2460 - val_loss: 0.0222 - val_acc: 0.2390\n",
      "Epoch 446/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2471 - val_loss: 0.0222 - val_acc: 0.2374\n",
      "Epoch 447/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2456 - val_loss: 0.0222 - val_acc: 0.2385\n",
      "Epoch 448/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2462 - val_loss: 0.0222 - val_acc: 0.2359\n",
      "Epoch 449/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2481 - val_loss: 0.0222 - val_acc: 0.2346\n",
      "Epoch 450/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2580 - val_loss: 0.0222 - val_acc: 0.2392\n",
      "Epoch 451/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2517 - val_loss: 0.0222 - val_acc: 0.2731\n",
      "Epoch 452/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2583 - val_loss: 0.0222 - val_acc: 0.2366\n",
      "Epoch 453/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2501 - val_loss: 0.0222 - val_acc: 0.2383\n",
      "Epoch 454/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2504 - val_loss: 0.0222 - val_acc: 0.2429\n",
      "Epoch 455/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2490 - val_loss: 0.0222 - val_acc: 0.2382\n",
      "Epoch 456/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2682 - val_loss: 0.0222 - val_acc: 0.2389\n",
      "Epoch 457/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2622 - val_loss: 0.0222 - val_acc: 0.2395\n",
      "Epoch 458/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2589 - val_loss: 0.0222 - val_acc: 0.2363\n",
      "Epoch 459/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2616 - val_loss: 0.0222 - val_acc: 0.2381\n",
      "Epoch 460/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2489 - val_loss: 0.0222 - val_acc: 0.2389\n",
      "Epoch 461/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2523 - val_loss: 0.0222 - val_acc: 0.2388\n",
      "Epoch 462/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2548 - val_loss: 0.0222 - val_acc: 0.2409\n",
      "Epoch 463/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2545 - val_loss: 0.0222 - val_acc: 0.2397\n",
      "Epoch 464/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2534 - val_loss: 0.0222 - val_acc: 0.2381\n",
      "Epoch 465/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2531 - val_loss: 0.0222 - val_acc: 0.2357\n",
      "Epoch 466/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2574 - val_loss: 0.0222 - val_acc: 0.2349\n",
      "Epoch 467/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2650 - val_loss: 0.0222 - val_acc: 0.2368\n",
      "Epoch 468/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2512 - val_loss: 0.0222 - val_acc: 0.2393\n",
      "Epoch 469/500\n",
      "10240/10240 [==============================] - 307s 30ms/step - loss: 0.0373 - acc: 0.2512 - val_loss: 0.0222 - val_acc: 0.2413\n",
      "Epoch 470/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2522 - val_loss: 0.0222 - val_acc: 0.2374\n",
      "Epoch 471/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2480 - val_loss: 0.0222 - val_acc: 0.2446\n",
      "Epoch 472/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2561 - val_loss: 0.0222 - val_acc: 0.2405\n",
      "Epoch 473/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2686 - val_loss: 0.0222 - val_acc: 0.2390\n",
      "Epoch 474/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2497 - val_loss: 0.0222 - val_acc: 0.2401\n",
      "Epoch 475/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2511 - val_loss: 0.0222 - val_acc: 0.2404\n",
      "Epoch 476/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2604 - val_loss: 0.0222 - val_acc: 0.2407\n",
      "Epoch 477/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2508 - val_loss: 0.0222 - val_acc: 0.2383\n",
      "Epoch 478/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2494 - val_loss: 0.0222 - val_acc: 0.2425\n",
      "Epoch 479/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2579 - val_loss: 0.0222 - val_acc: 0.2393\n",
      "Epoch 480/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2503 - val_loss: 0.0222 - val_acc: 0.2446\n",
      "Epoch 481/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2565 - val_loss: 0.0222 - val_acc: 0.2380\n",
      "Epoch 482/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2550 - val_loss: 0.0222 - val_acc: 0.2392\n",
      "Epoch 483/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2568 - val_loss: 0.0222 - val_acc: 0.2417\n",
      "Epoch 484/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2600 - val_loss: 0.0222 - val_acc: 0.7155\n",
      "Epoch 485/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2566 - val_loss: 0.0222 - val_acc: 0.2388\n",
      "Epoch 486/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2480 - val_loss: 0.0222 - val_acc: 0.2405\n",
      "Epoch 487/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2587 - val_loss: 0.0222 - val_acc: 0.2379\n",
      "Epoch 488/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2527 - val_loss: 0.0222 - val_acc: 0.2395\n",
      "Epoch 489/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2491 - val_loss: 0.0222 - val_acc: 0.2430\n",
      "Epoch 490/500\n",
      "10240/10240 [==============================] - 299s 29ms/step - loss: 0.0373 - acc: 0.2525 - val_loss: 0.0222 - val_acc: 0.2415\n",
      "Epoch 491/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2500 - val_loss: 0.0222 - val_acc: 0.2587\n",
      "Epoch 492/500\n",
      "10240/10240 [==============================] - 300s 29ms/step - loss: 0.0373 - acc: 0.2501 - val_loss: 0.0222 - val_acc: 0.2394\n",
      "Epoch 493/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2601 - val_loss: 0.0222 - val_acc: 0.2407\n",
      "Epoch 494/500\n",
      "10240/10240 [==============================] - 305s 30ms/step - loss: 0.0373 - acc: 0.2516 - val_loss: 0.0222 - val_acc: 0.2411\n",
      "Epoch 495/500\n",
      "10240/10240 [==============================] - 301s 29ms/step - loss: 0.0373 - acc: 0.2517 - val_loss: 0.0222 - val_acc: 0.2404\n",
      "Epoch 496/500\n",
      "10240/10240 [==============================] - 302s 30ms/step - loss: 0.0373 - acc: 0.2548 - val_loss: 0.0222 - val_acc: 0.2408\n",
      "Epoch 497/500\n",
      "10240/10240 [==============================] - 302s 29ms/step - loss: 0.0373 - acc: 0.2565 - val_loss: 0.0222 - val_acc: 0.2377\n",
      "Epoch 498/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2645 - val_loss: 0.0222 - val_acc: 0.2425\n",
      "Epoch 499/500\n",
      "10240/10240 [==============================] - 303s 30ms/step - loss: 0.0373 - acc: 0.2589 - val_loss: 0.0222 - val_acc: 0.2408\n",
      "Epoch 500/500\n",
      "10240/10240 [==============================] - 304s 30ms/step - loss: 0.0373 - acc: 0.2566 - val_loss: 0.0222 - val_acc: 0.2396\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1280, 64), return_sequences=True))\n",
    "model.summary()\n",
    "adam = Adam(lr=0.00005)\n",
    "model.compile(optimizer=adam, loss='mean_absolute_error', metrics=['acc'])\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=500,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5CEL7sYYk4f+Hzb78NE0gaumAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9064178208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4v3k3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9082c7c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 41s 13ms/step\n",
      "Test accuracy: 0.1716582030057907\n",
      "Test loss: 0.041925575754239615\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
